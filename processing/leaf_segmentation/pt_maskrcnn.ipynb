{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05f7d50d-c01a-4256-a71f-0b6ed43fd0d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import yaml\n",
    "import random\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.datasets import CocoDetection\n",
    "from torchvision.models.detection import maskrcnn_resnet50_fpn, maskrcnn_resnet50_fpn_v2\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms import functional as F \n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "from pycocotools.coco import COCO\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from coco_eval import CocoEvaluator\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "from PIL import Image, ImageDraw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef037898-65f4-455c-858a-f972e51d9723",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 100\n",
    "NUM_WORKERS = 2\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "OUTPUT_DIR = \"out/pt_maskrcnn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9af9e66d-de45-4c1f-a0a8-e5e0c2b07fc5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(f\"Using device {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26b2e822-286c-42ce-9e78-5c11fb57792a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_model_instance_segmentation(num_classes):\n",
    "    model = maskrcnn_resnet50_fpn_v2(pretrained=True)\n",
    "    # Replace the box classifier with the desired number of classes\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    \n",
    "    # Replace the mask predictor\n",
    "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "    hidden_layer = 256\n",
    "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask, hidden_layer, num_classes)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "669edd0e-03dc-46a9-841e-7c04687eaa06",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset_file, root, split, transforms=None):\n",
    "        self.dataset_file = dataset_file\n",
    "        self.root = root\n",
    "        with open(dataset_file) as f:\n",
    "            self.dataset = yaml.safe_load(f)\n",
    "        if split not in self.dataset:\n",
    "            raise Error(f\"Split not defined in {dataset_file}\")\n",
    "        self.split = split\n",
    "        self.transforms = transforms\n",
    "        self.data_dir = os.path.join(root, self.dataset[\"path\"], self.dataset[split])\n",
    "        if \"images\" in self.data_dir:\n",
    "            self.data_dir = Path(self.data_dir).parent\n",
    "        self.image_files = os.listdir(os.path.join(self.data_dir, \"images\"))\n",
    "        self.label_files = os.listdir(os.path.join(self.data_dir, \"labels\"))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = cv2.imread(os.path.join(self.data_dir, \"images\", self.image_files[index]))\n",
    "        target = self.load_annotations(os.path.join(self.data_dir, \"labels\", self.label_files[index]), img.shape[:2], index)\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "                  \n",
    "    def load_annotations(self, file, shape, idx):\n",
    "        with open(file) as f:\n",
    "            label_data = f.read().split(\"\\n\")\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        masks = []\n",
    "        image_ids = []\n",
    "        areas = []\n",
    "        iscrowds = []\n",
    "        for label_datum in label_data:\n",
    "            class_id = label_data[0]\n",
    "            bbox = np.array(label_data[1:5]).astype(np.float64)\n",
    "            poly = np.array(label_data[5:]).astype(np.float64)\n",
    "\n",
    "            #scale_mask = np.array([shape[0], shape[1]] * (len(poly) // 2 + 1))[:len(poly)]\n",
    "            poly_scaled = poly.copy()\n",
    "            poly_scaled[::2] *= shape[0]  # multiply every other element starting from index 0\n",
    "            poly_scaled[1::2] *= shape[1] # multiply every other element starting from index 1\n",
    "\n",
    "            poly_scaled = poly_scaled.reshape(-1, 2).reshape((-1, 1, 2)).astype(np.int32)\n",
    "\n",
    "            mask = np.zeros(shape)\n",
    "            cv2.fillPoly(mask, poly_scaled, 1)\n",
    "            pixels = cv2.countNonZero(mask)\n",
    "            image_area = shape[0] * shape[1]\n",
    "            area_ratio = (pixels / image_area)\n",
    "            mask = mask.astype(bool)\n",
    "            \n",
    "            labels.append(class_id)\n",
    "            boxes.append(bbox)\n",
    "            masks.append(mask)\n",
    "            image_ids.append(idx * random.randint(0,2543))\n",
    "            areas.append(area_ratio)\n",
    "            iscrowds.append(False)\n",
    "                                       \n",
    "        target = {\n",
    "            \"boxes\": torch.tensor(boxes, dtype=torch.float32),\n",
    "            \"labels\": torch.tensor(class_ids, dtype=torch.int64),\n",
    "            \"masks\": torch.tensor(masks, dtype=torch.uint8),  # Shape: (N, H, W)\n",
    "            \"image_id\": torch.tensor(image_ids),\n",
    "            \"area\": torch.tensor(areas, dtype=torch.float32),\n",
    "            \"iscrowd\": torch.tensor(iscrowds, dtype=torch.int64)\n",
    "        }\n",
    "                                       \n",
    "        return target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12d61f7f-0fee-4f7d-a5f8-bf5b2fd42cf1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CocoDataset(CocoDetection):\n",
    "    def __init__(self, root, annFile, transforms=None):\n",
    "        super(CocoDataset, self).__init__(root, annFile)\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img, targets = super(CocoDataset, self).__getitem__(idx)\n",
    "\n",
    "        # Extract image ID\n",
    "        image_id = self.ids[idx]\n",
    "\n",
    "        # Convert target information to a usable format\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        masks = []\n",
    "        \n",
    "        print(len(targets))\n",
    "\n",
    "        for target in targets:\n",
    "            # Each target is a tuple (mask, bbox, category_id)\n",
    "            masks.append(self.coco.annToMask(target[0]))  # Extract mask from the first item\n",
    "            boxes.append(target[1])  # Extract bounding box from the second item\n",
    "            labels.append(target[2])  # Extract label from the third item\n",
    "\n",
    "        # Convert to tensors\n",
    "        target_dict = {\n",
    "            \"boxes\": torch.tensor(boxes, dtype=torch.float32),\n",
    "            \"labels\": torch.tensor(labels, dtype=torch.int64),\n",
    "            \"masks\": torch.stack([torch.tensor(mask, dtype=torch.uint8) for mask in masks]),\n",
    "            \"image_id\": torch.tensor([image_id]),\n",
    "        }\n",
    "\n",
    "        # Apply transformations if provided\n",
    "        #if self.transforms:\n",
    "        #    img = self.transforms(img)\n",
    "\n",
    "        return img, target_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2195f688-11ec-4467-9914-00d7b8249b9a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CustomCocoDataset(CocoDetection):\n",
    "    def __init__(self, root, annFile, transform=None):\n",
    "        \"\"\"\n",
    "        Custom COCO dataset that loads images, bounding boxes, segmentation masks, \n",
    "        and category labels for each instance in an image.\n",
    "        \n",
    "        Args:\n",
    "            root (str): Directory with all the images.\n",
    "            annFile (str): Path to the COCO annotation file.\n",
    "            transform (callable, optional): Optional transform to be applied on an image.\n",
    "        \"\"\"\n",
    "        super().__init__(root, annFile)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Returns an image along with its target annotations.\n",
    "\n",
    "        Args:\n",
    "            index (int): Index of the sample.\n",
    "\n",
    "        Returns:\n",
    "            image (torch.Tensor): Transformed image tensor.\n",
    "            target (dict): Dictionary with keys:\n",
    "                - 'boxes': Tensor of bounding boxes (N, 4).\n",
    "                - 'labels': Tensor of category labels (N).\n",
    "                - 'masks': Tensor of segmentation masks (N, H, W).\n",
    "                - 'image_id': Tensor with a unique ID for the image.\n",
    "                - 'area': Tensor of areas of the bounding boxes.\n",
    "                - 'iscrowd': Tensor indicating if the instance is a crowd (1) or not (0).\n",
    "        \"\"\"\n",
    "        # Load image and annotations\n",
    "        img, annotations = super().__getitem__(index)\n",
    "        \n",
    "        # Convert image to tensor\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        else:\n",
    "            img = F.to_tensor(img)  # Convert image to tensor\n",
    "        \n",
    "        # Initialize lists to hold instance data\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        masks = []\n",
    "        area = []\n",
    "        iscrowd = []\n",
    "        ids = []\n",
    "        \n",
    "        if len(annotations) == 0:\n",
    "            return img, {\n",
    "                'boxes': torch.tensor([]),\n",
    "                'labels': torch.tensor([]),\n",
    "                'masks': torch.tensor([]),\n",
    "                'image_id': torch.tensor([]),\n",
    "                'area': torch.tensor([]),\n",
    "                'iscrowd': torch.tensor([]),\n",
    "                'ids': torch.tensor([])\n",
    "            }\n",
    "\n",
    "        for annotation in annotations:\n",
    "            # Bounding box in [x, y, width, height]\n",
    "            x, y, width, height = annotation['bbox']\n",
    "            boxes.append([x, y, x + width, y + height])\n",
    "            labels.append(annotation['category_id'])\n",
    "            area.append(annotation['area'])\n",
    "            iscrowd.append(annotation['iscrowd'])\n",
    "            ids.append(annotation['id'])\n",
    "\n",
    "            # Process segmentation\n",
    "            mask = self._create_segmentation_mask(annotation, img.size(1), img.size(2))\n",
    "            masks.append(mask)\n",
    "\n",
    "        # Convert lists to tensors\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "        if len(masks) == 0:\n",
    "            masks = torch.tensor([])\n",
    "        else:\n",
    "            masks = torch.stack(masks, dim=0)\n",
    "        area = torch.as_tensor(area, dtype=torch.float32)\n",
    "        iscrowd = torch.as_tensor(iscrowd, dtype=torch.int64)\n",
    "\n",
    "        # Image ID\n",
    "        image_id = torch.tensor([index])\n",
    "        ids = torch.as_tensor(ids, dtype=torch.int64)\n",
    "\n",
    "        # Create the target dictionary\n",
    "        target = {\n",
    "            'boxes': boxes,\n",
    "            'labels': labels,\n",
    "            'masks': masks,\n",
    "            'image_id': image_id,\n",
    "            'area': area,\n",
    "            'iscrowd': iscrowd,\n",
    "            'ids': ids\n",
    "        }\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def _create_segmentation_mask(self, annotation, height, width):\n",
    "        \"\"\"\n",
    "        Creates a binary mask for a given instance's segmentation data.\n",
    "\n",
    "        Args:\n",
    "            annotation (dict): COCO annotation dictionary containing segmentation data.\n",
    "            height (int): Height of the image.\n",
    "            width (int): Width of the image.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Binary mask tensor for the instance.\n",
    "        \"\"\"\n",
    "        mask = Image.new(\"L\", (width, height), 0)  # Create a blank mask\n",
    "        for segmentation in annotation['segmentation']:\n",
    "            # segmentation is a list of coordinates: [x1, y1, x2, y2, ..., xn, yn]\n",
    "            if isinstance(segmentation, list):  # Polygon format\n",
    "                poly = np.array(segmentation).reshape((-1, 2))\n",
    "                ImageDraw.Draw(mask).polygon(poly.flatten().tolist(), outline=1, fill=1)\n",
    "            # Additional processing may be added here for other segmentation formats\n",
    "        return F.to_tensor(mask)  # Convert mask to tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3dff16-053b-45ae-a1f8-9559db736e80",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n"
     ]
    }
   ],
   "source": [
    "# Create train and validation datasets and loaders\n",
    "#train_dataset = CustomDataset(root=\"_data/datasets\", dataset_file=\"_data/synthetic.yaml\", split=\"train\", transforms=F.to_tensor)\n",
    "#val_dataset = CustomDataset(root=\"_data/datasets\", dataset_file=\"_data/synthetic.yaml\", split=\"val\", transforms=F.to_tensor)\n",
    "input_transforms = transforms.Compose([\n",
    "    transforms.PILToTensor(),\n",
    "    transforms.ConvertImageDtype(torch.float32),\n",
    "])\n",
    "\n",
    "train_dataset = CustomCocoDataset(root=\"_data/datasets/synthetic_leaf_instances/train/images\", \n",
    "                              annFile=\"_data/coco_synthetic_train.json\", \n",
    "                              transform=input_transforms)\n",
    "val_dataset = CustomCocoDataset(root=\"_data/datasets/synthetic_leaf_instances/val/images\", \n",
    "                            annFile=\"_data/coco_synthetic_val.json\", \n",
    "                            transform=input_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a240d805-bd88-4caa-b870-ca6ee0ce8481",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_img, sample_annos = next(iter(train_dataset))\n",
    "print(sample_img.shape)\n",
    "print(sample_annos.keys())\n",
    "print(sample_annos)\n",
    "plt.imshow(sample_img.numpy().transpose((1,2,0)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b459882c-1df9-4b31-905b-d835b6df1785",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, \n",
    "                          batch_size=BATCH_SIZE, \n",
    "                          num_workers=NUM_WORKERS, \n",
    "                          shuffle=True,\n",
    "                          collate_fn=lambda batch: tuple(zip(*batch)))\n",
    "val_loader = DataLoader(val_dataset, \n",
    "                        batch_size=BATCH_SIZE,\n",
    "                        num_workers=NUM_WORKERS,\n",
    "                        shuffle=False,\n",
    "                        collate_fn=lambda batch: tuple(zip(*batch)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bd90a8-c399-4e24-917a-3af36ce55a9e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "loader_sample = next(iter(train_loader))\n",
    "print(loader_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eae8222-f717-44f6-9210-05ee6f84430a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "coco_test = COCO(\"_data/coco_synthetic_val.json\")\n",
    "len(coco_test.getAnnIds())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2e6ef6-5c57-45aa-b28c-2df021e0fc6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set up the model\n",
    "num_classes = 2  # background + 1 foreground class (or adjust as needed)\n",
    "model = get_model_instance_segmentation(num_classes)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0669dc7c-bd98-4669-a923-d0b9f0fed7fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set up optimizer and learning rate scheduler\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754da280-bd06-4c80-9639-9c784d3832d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def convert_to_coco_format(detections, image_ids, category_mapping):\n",
    "    \"\"\"\n",
    "    Converts model detections to COCO results format.\n",
    "    \n",
    "    Args:\n",
    "        detections (list of dict): List of detection results where each dict contains:\n",
    "            - 'boxes': Tensor of shape (N, 4) with bounding boxes in [x_min, y_min, x_max, y_max]\n",
    "            - 'labels': Tensor of shape (N,) with category labels\n",
    "            - 'scores': Tensor of shape (N,) with confidence scores\n",
    "        image_ids (list): List of image IDs corresponding to each detection.\n",
    "        category_mapping (dict): Mapping from class indices to COCO category IDs.\n",
    "        \n",
    "    Returns:\n",
    "        list: List of detections in COCO results format.\n",
    "    \"\"\"\n",
    "    coco_results = []\n",
    "    for i, det in enumerate(detections):\n",
    "        image_id = image_ids[i]\n",
    "        \n",
    "        boxes = det['boxes'].cpu().numpy()  # Bounding boxes\n",
    "        labels = det['labels'].cpu().numpy()  # Class labels\n",
    "        scores = det['scores'].cpu().numpy()  # Confidence scores\n",
    "        \n",
    "        for j in range(len(boxes)):\n",
    "            x_min, y_min, x_max, y_max = boxes[j]\n",
    "            width = x_max - x_min\n",
    "            height = y_max - y_min\n",
    "            coco_results.append({\n",
    "                'image_id': int(image_id),\n",
    "                'category_id': int(category_mapping[labels[j]]),\n",
    "                'bbox': [float(x_min), float(y_min), float(width), float(height)],\n",
    "                'score': float(scores[j]),\n",
    "                'mask': det['masks'].cpu().numpy()\n",
    "            })\n",
    "    \n",
    "    return coco_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6752dbe-8a00-4ef8-a78f-060d44606623",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Evaluation function to compute metrics\n",
    "coco_gt = COCO(\"_data/coco_synthetic_val.json\")  # Load COCO annotations if you have them\n",
    "def evaluate(model, data_loader, device):\n",
    "    model.eval()\n",
    "    coco_evaluator = None\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, targets in tqdm(data_loader):\n",
    "            corrupt = False\n",
    "            for target in targets:\n",
    "                if len(target[\"boxes\"]) == 0:\n",
    "                    corrupt = True\n",
    "                    break\n",
    "            if corrupt:\n",
    "                continue\n",
    "            images = list(img.to(device) for img in images)\n",
    "            outputs = model(images)\n",
    "            \n",
    "            # Collect outputs and ground-truth for coco-style evaluation\n",
    "            if coco_evaluator is None:\n",
    "                coco_evaluator = CocoEvaluator(coco_gt, iou_types=[\"bbox\", \"segm\"])\n",
    "            \n",
    "            coco_evaluator.update(convert_to_coco_format(outputs, image_ids=[target['image_id'] for target in targets], category_mapping={1:1}))\n",
    "        \n",
    "        # Gather evaluation results\n",
    "        coco_evaluator.synchronize_between_processes()\n",
    "        coco_evaluator.accumulate()\n",
    "        coco_evaluator.summarize()\n",
    "        \n",
    "        return coco_evaluator.coco_eval[\"segm\"].stats  # return segmentation stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3c749a-512b-4f27-baee-c519c4fbb6fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def coco_eval_to_dict(coco_eval):\n",
    "    \"\"\"\n",
    "    Converts a COCOeval object to a dictionary format.\n",
    "\n",
    "    Args:\n",
    "        coco_eval (COCOeval): COCOeval object after evaluation.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary with COCO evaluation metrics.\n",
    "    \"\"\"\n",
    "    stats = coco_eval\n",
    "    \n",
    "    eval_metrics = {\n",
    "        'Average Precision (AP) @[ IoU=0.50:0.95 | area=all | maxDets=100 ]': stats[0],\n",
    "        'Average Precision (AP) @[ IoU=0.50      | area=all | maxDets=100 ]': stats[1],\n",
    "        'Average Precision (AP) @[ IoU=0.75      | area=all | maxDets=100 ]': stats[2],\n",
    "        'Average Precision (AP) @[ IoU=0.50:0.95 | area=small | maxDets=100 ]': stats[3],\n",
    "        'Average Precision (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ]': stats[4],\n",
    "        'Average Precision (AP) @[ IoU=0.50:0.95 | area=large | maxDets=100 ]': stats[5],\n",
    "        'Average Recall (AR) @[ IoU=0.50:0.95    | area=all | maxDets=1 ]': stats[6],\n",
    "        'Average Recall (AR) @[ IoU=0.50:0.95    | area=all | maxDets=10 ]': stats[7],\n",
    "        'Average Recall (AR) @[ IoU=0.50:0.95    | area=all | maxDets=100 ]': stats[8],\n",
    "        'Average Recall (AR) @[ IoU=0.50:0.95    | area=small | maxDets=100 ]': stats[9],\n",
    "        'Average Recall (AR) @[ IoU=0.50:0.95    | area=medium | maxDets=100 ]': stats[10],\n",
    "        'Average Recall (AR) @[ IoU=0.50:0.95    | area=large | maxDets=100 ]': stats[11],\n",
    "    }\n",
    "    \n",
    "    return eval_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd584f65-e13b-4ea9-acb4-3bbccba9d7a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Training loop\n",
    "metrics_df = pd.DataFrame()\n",
    "training_df = pd.DataFrame()\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    p_bar = tqdm(enumerate(train_loader), desc=f\"Training Epoch {epoch+1}\", total=len(train_loader))\n",
    "    total_corrupt = 0\n",
    "    for i, data in p_bar:\n",
    "        images, targets = data\n",
    "        corrupt = False\n",
    "        for target in targets:\n",
    "            if len(target[\"boxes\"]) == 0:\n",
    "                corrupt = True\n",
    "                break\n",
    "        if corrupt:\n",
    "            total_corrupt += 1\n",
    "            continue\n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        \n",
    "        loss_dict = model(images, targets)\n",
    "        if i % 100:\n",
    "            training_df = pd.concat([training_df, pd.DataFrame({key: loss.item() for key, loss in loss_dict.items()}, index=[epoch * len(train_loader) + i])])\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        epoch_loss += losses.item()\n",
    "    \n",
    "        p_bar.set_description(f\"Training Epoch {epoch+1}, loss: {0 if not epoch_loss > 0 else (epoch_loss/(i+1)):.4f}, corrupted: {total_corrupt}\")\n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Step the scheduler\n",
    "    lr_scheduler.step()\n",
    "    \n",
    "    # Print training loss\n",
    "    print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}], Loss: {epoch_loss/len(train_loader):.4f}\")\n",
    "    \n",
    "    # Evaluate on validation set and log metrics\n",
    "    segm_stats = evaluate(model, val_loader, device)\n",
    "    segm_stats_dict = coco_eval_to_dict(segm_stats)\n",
    "#    segm_stats_dict['epoch'] = epoch\n",
    "    metrics_df = pd.concat([metrics_df, pd.DataFrame(segm_stats_dict, index=[epoch])])\n",
    "    print(f\"Segmentation metrics (AP, AR) at epoch {epoch+1}: {segm_stats}\")\n",
    "    metrics_df.to_csv(os.path.join(OUTPUT_DIR, \"metrics_eval.csv\"))\n",
    "    training_df.to_csv(os.path.join(OUTPUT_DIR, \"metrics_train.csv\"))\n",
    "    \n",
    "\n",
    "print(\"Finished training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85891751-d614-4326-aeee-63bf58b909f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch w/ CUDA 12.4",
   "language": "python",
   "name": "pytorch_cuda_12.4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
