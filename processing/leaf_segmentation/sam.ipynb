{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! pip install kaggle &> /dev/null\n",
    "! pip install torch torchvision &> /dev/null\n",
    "! pip install opencv-python pycocotools matplotlib onnxruntime onnx &> /dev/null\n",
    "! pip install git+https://github.com/facebookresearch/segment-anything.git &> /dev/null\n",
    "! wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth &> /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MASK_DIR = \"_data/combined/train/leaf_instances\"\n",
    "RGB_DIR = \"_data/combined/train/images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_type = 'vit_b'\n",
    "checkpoint = 'sam_vit_b_01ec64.pth'\n",
    "device = 'cuda:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from segment_anything import SamPredictor, sam_model_registry, SamAutomaticMaskGenerator\n",
    "sam_model = sam_model_registry[model_type]()\n",
    "sam_model.to(device)\n",
    "amg = SamAutomaticMaskGenerator(sam_model)\n",
    "sam_model.train();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "# Preprocess the images\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "\n",
    "from segment_anything.utils.transforms import ResizeLongestSide\n",
    "import os\n",
    "\n",
    "transform = ResizeLongestSide(sam_model.image_encoder.img_size)\n",
    "\n",
    "class LeafInstanceDataset(Dataset):\n",
    "    def __init__(self, path):\n",
    "        self.path = path\n",
    "        self.files = os.listdir(os.path.join(path, \"images\"))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.path, \"images\", self.files[idx])\n",
    "        mask_path =  os.path.join(self.path, \"leaf_instances\", self.files[idx])\n",
    "        mask_im = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "        mask = np.array(mask_im)\n",
    "\n",
    "        unique_categories = np.unique(mask)\n",
    "        unique_categories = unique_categories[unique_categories > 0]  # Exclude background (assumed to be 0)\n",
    "        \n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        input_image = transform.apply_image(image)\n",
    "        input_image_torch = torch.as_tensor(input_image, device=device)\n",
    "        transformed_image = input_image_torch.permute(2, 0, 1).contiguous()[None, :, :, :]\n",
    "\n",
    "        input_image = sam_model.preprocess(transformed_image)\n",
    "        original_image_size = image.shape[:2]\n",
    "        input_size = tuple(transformed_image.shape[-2:])\n",
    "        \n",
    "        data = [{}] * len(unique_categories)\n",
    "        \n",
    "        for category_index, category_id in enumerate(unique_categories):\n",
    "            y, x = np.nonzero(mask)\n",
    "            x_min = np.min(x)\n",
    "            y_min = np.min(y)\n",
    "            x_max = np.max(x)\n",
    "            y_max = np.max(y)\n",
    "            data[category_index][\"bbox\"] = np.array([x_min, y_min, x_max, y_max])\n",
    "            data[category_index][\"bbox_transformed\"] = transform.apply_boxes(np.array([x_min, y_min, x_max, y_max]), original_image_size)\n",
    "            data[category_index][\"mask\"] = (mask == category_id).squeeze()\n",
    "            data[category_index][\"image\"] = input_image.squeeze()\n",
    "            data[category_index][\"input_size\"] = input_size\n",
    "            data[category_index]['original_image_size'] = original_image_size\n",
    "            \n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set up the optimizer, hyperparameter tuning will improve performance here\n",
    "lr = 1e-4\n",
    "wd = 0\n",
    "optimizer = torch.optim.Adam(sam_model.mask_decoder.parameters(), lr=lr, weight_decay=wd)\n",
    "\n",
    "#loss_fn = torch.nn.MSELoss()\n",
    "loss_fn = torch.nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 73.42185974121094:   0%|          | 39/10010 [00:19<1:17:59,  2.13it/s]"
     ]
    }
   ],
   "source": [
    "from statistics import mean\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torch.nn.functional import threshold, normalize\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "num_epochs = 100\n",
    "losses = []\n",
    "data_loader = DataLoader(LeafInstanceDataset(\"_data/combined/train\"))\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_losses = []\n",
    "    p_bar = tqdm(data_loader)\n",
    "    for data in p_bar:\n",
    "        for i in range(len(data)):\n",
    "            input_image = data[i]['image'].to(device)\n",
    "            input_size = data[i]['input_size']\n",
    "            original_image_size = data[i]['original_image_size']\n",
    "\n",
    "            # No grad here as we don't want to optimise the encoders\n",
    "            with torch.no_grad():\n",
    "              image_embedding = sam_model.image_encoder(input_image)\n",
    "\n",
    "              box = data[i][\"bbox_transformed\"]\n",
    "              box_torch = torch.as_tensor(box, dtype=torch.float, device=device)\n",
    "              box_torch = box_torch[None, :]\n",
    "\n",
    "              sparse_embeddings, dense_embeddings = sam_model.prompt_encoder(\n",
    "                  points=None,\n",
    "                  boxes=box_torch,\n",
    "                  masks=None,\n",
    "              )\n",
    "            low_res_masks, iou_predictions = sam_model.mask_decoder(\n",
    "              image_embeddings=image_embedding,\n",
    "              image_pe=sam_model.prompt_encoder.get_dense_pe(),\n",
    "              sparse_prompt_embeddings=sparse_embeddings,\n",
    "              dense_prompt_embeddings=dense_embeddings,\n",
    "              multimask_output=False,\n",
    "            )\n",
    "\n",
    "            upscaled_masks = sam_model.postprocess_masks(low_res_masks, input_size, original_image_size).to(device)\n",
    "            binary_mask = normalize(threshold(upscaled_masks, 0.0, 0))\n",
    "\n",
    "            data[i][\"mask\"] = data[i][\"mask\"].squeeze()\n",
    "            gt_mask_resized = torch.from_numpy(np.resize(data[i][\"mask\"], (1, 1, data[i][\"mask\"].shape[0], data[i][\"mask\"].shape[1]))).to(device)\n",
    "            gt_binary_mask = torch.as_tensor(gt_mask_resized > 0, dtype=torch.float32)\n",
    "\n",
    "            loss = loss_fn(binary_mask, gt_binary_mask)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_losses.append(loss.item())\n",
    "            p_bar.set_description(f\"Loss: {loss.item()}\")\n",
    "    losses.append(epoch_losses)\n",
    "    print(f'EPOCH: {epoch}')\n",
    "    print(f'Mean loss: {mean(epoch_losses)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_loss(pred, target, smooth=1.):\n",
    "    \"\"\"This definition generalize to real valued pred and target vector.\n",
    "This should be differentiable.\n",
    "    pred: tensor with first dimension as batch\n",
    "    target: tensor with first dimension as batch\n",
    "    \"\"\"\n",
    "\n",
    "    # have to use contiguous since they may from a torch.view op\n",
    "    iflat = pred.contiguous().view(-1)\n",
    "    tflat = target.contiguous().view(-1)\n",
    "    intersection = (iflat * tflat).sum()\n",
    "\n",
    "    A_sum = torch.sum(tflat * iflat)\n",
    "    B_sum = torch.sum(tflat * tflat)\n",
    "    \n",
    "    return 1 - ((2. * intersection + smooth) / (A_sum + B_sum + smooth) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torch.nn.functional import threshold, normalize\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "num_epochs = 100\n",
    "losses = []\n",
    "data_loader = DataLoader(LeafInstanceDataset(\"_data/combined/train\"))\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_losses = []\n",
    "    p_bar = tqdm(data_loader)\n",
    "    for data in p_bar:\n",
    "          input_image = data[0]['image'].to(device)\n",
    "          input_size = data[0]['input_size']\n",
    "          original_image_size = data[0]['original_image_size']\n",
    "\n",
    "          # No grad here as we don't want to optimise the encoders\n",
    "          with torch.no_grad():\n",
    "            predictions = amg.generate(data[i][\"image\"])\n",
    "\n",
    "          target_mask = torch.zeros(input_size)\n",
    "          for i, d in enumerate(data):\n",
    "            target_mask[d[\"mask\"] > 0] = i\n",
    "          \n",
    "          pred_mask = torch.zeros(input_size)\n",
    "\n",
    "          for i, p in enumerate(predictions):\n",
    "            target_mask[p[\"segmentation\"] > 0] = i\n",
    "\n",
    "          loss = dice_loss(pred_mask, target_mask)\n",
    "          optimizer.zero_grad()\n",
    "          loss.backward()\n",
    "          optimizer.step()\n",
    "          epoch_losses.append(loss.item())\n",
    "          p_bar.set_description(f\"Loss: {loss.item()}\")\n",
    "    losses.append(epoch_losses)\n",
    "    print(f'EPOCH: {epoch}')\n",
    "    print(f'Mean loss: {mean(epoch_losses)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_losses = [mean(x) for x in losses]\n",
    "mean_losses\n",
    "\n",
    "plt.plot(list(range(len(mean_losses))), mean_losses)\n",
    "plt.title('Mean epoch loss')\n",
    "plt.xlabel('Epoch Number')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up predictors for both tuned and original models\n",
    "from segment_anything import sam_model_registry, SamPredictor\n",
    "predictor_tuned = SamPredictor(sam_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "image_file = random.sample(os.listdir(\"_data/combined/test\"))\n",
    "image_file = os.path.join(\"_data/combined/test\", image_file)\n",
    "image = cv2.imread(image_file)\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_tuned.set_image(image)\n",
    "\n",
    "masks_tuned, _, _ = predictor_tuned.predict(\n",
    "    point_coords=None,\n",
    "    box=None,\n",
    "    multimask_output=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions provided in https://github.com/facebookresearch/segment-anything/blob/9e8f1309c94f1128a6e5c047a10fdcb02fc8d651/notebooks/predictor_example.ipynb\n",
    "def show_mask(mask, ax, random_color=False):\n",
    "    if random_color:\n",
    "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
    "    else:\n",
    "        color = np.array([30/255, 144/255, 255/255, 0.6])\n",
    "    h, w = mask.shape[-2:]\n",
    "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "    ax.imshow(mask_image)\n",
    "\n",
    "def show_box(box, ax):\n",
    "    x0, y0 = box[0], box[1]\n",
    "    w, h = box[2] - box[0], box[3] - box[1]\n",
    "    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0,0,0,0), lw=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axs[0].imshow(image)\n",
    "show_mask(masks_tuned, axs[0])\n",
    "show_box(input_bbox, axs[0])\n",
    "axs[0].set_title('Mask with Tuned Model', fontsize=26)\n",
    "axs[0].axis('off')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch w/ CUDA 12.4",
   "language": "python",
   "name": "pytorch_cuda_12.4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
