{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install kaggle &> /dev/null\n",
    "! pip install torch torchvision &> /dev/null\n",
    "! pip install opencv-python pycocotools matplotlib onnxruntime onnx &> /dev/null\n",
    "! pip install git+https://github.com/facebookresearch/segment-anything.git &> /dev/null\n",
    "! wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth &> /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MASK_DIR = \"_data/combined/train/leaf_instances\"\n",
    "RGB_DIR = \"_data/combined/train/images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = 'vit_b'\n",
    "checkpoint = 'sam_vit_b_01ec64.pth'\n",
    "device = 'cuda:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "bbox_coords = {}\n",
    "ground_truth_masks = {}\n",
    "for file in os.listdir(MASK_DIR):\n",
    "    if os.path.isdir(file):\n",
    "        continue\n",
    "    im = cv2.imread(os.path.join(MASK_DIR, file), cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "    mask = np.array(im)\n",
    "    \n",
    "    unique_categories = np.unique(mask)\n",
    "    unique_categories = unique_categories[unique_categories > 0]  # Exclude background (assumed to be 0)\n",
    "\n",
    "    for category_id in unique_categories:\n",
    "        y, x = np.nonzero(mask)\n",
    "        x_min = np.min(x)\n",
    "        y_min = np.min(y)\n",
    "        x_max = np.max(x)\n",
    "        y_max = np.max(y)\n",
    "        bbox_coords[f\"{file}_{category_id}\"] = np.array([x_min, y_min, x_max, y_max])\n",
    "        ground_truth_masks[f\"{file}_{category_id}\"] = (mask == category_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the optimizer, hyperparameter tuning will improve performance here\n",
    "lr = 1e-4\n",
    "wd = 0\n",
    "optimizer = torch.optim.Adam(sam_model.mask_decoder.parameters(), lr=lr, weight_decay=wd)\n",
    "\n",
    "#loss_fn = torch.nn.MSELoss()\n",
    "loss_fn = torch.nn.BCELoss()\n",
    "keys = list(bbox_coords.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from segment_anything import SamPredictor, sam_model_registry\n",
    "sam_model = sam_model_registry[model_type]()\n",
    "sam_model.to(device)\n",
    "sam_model.train();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the images\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "\n",
    "from segment_anything.utils.transforms import ResizeLongestSide\n",
    "\n",
    "transformed_data = defaultdict(dict)\n",
    "for k in bbox_coords.keys():\n",
    "  filename = k \n",
    "  image = cv2.imread(os.path.join(RGB_DIR, k))\n",
    "  image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "  transform = ResizeLongestSide(sam_model.image_encoder.img_size)\n",
    "  input_image = transform.apply_image(image)\n",
    "  input_image_torch = torch.as_tensor(input_image, device=device)\n",
    "  transformed_image = input_image_torch.permute(2, 0, 1).contiguous()[None, :, :, :]\n",
    "\n",
    "  input_image = sam_model.preprocess(transformed_image)\n",
    "  original_image_size = image.shape[:2]\n",
    "  input_size = tuple(transformed_image.shape[-2:])\n",
    "\n",
    "  transformed_data[k]['image'] = input_image\n",
    "  transformed_data[k]['input_size'] = input_size\n",
    "  transformed_data[k]['original_image_size'] = original_image_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torch.nn.functional import threshold, normalize\n",
    "\n",
    "num_epochs = 100\n",
    "losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "  epoch_losses = []\n",
    "  # Just train on the first 20 examples\n",
    "  for k in keys:\n",
    "    input_image = transformed_data[k]['image'].to(device)\n",
    "    input_size = transformed_data[k]['input_size']\n",
    "    original_image_size = transformed_data[k]['original_image_size']\n",
    "\n",
    "    # No grad here as we don't want to optimise the encoders\n",
    "    with torch.no_grad():\n",
    "      image_embedding = sam_model.image_encoder(input_image)\n",
    "\n",
    "      prompt_box = bbox_coords[k]\n",
    "      box = transform.apply_boxes(prompt_box, original_image_size)\n",
    "      box_torch = torch.as_tensor(box, dtype=torch.float, device=device)\n",
    "      box_torch = box_torch[None, :]\n",
    "\n",
    "      sparse_embeddings, dense_embeddings = sam_model.prompt_encoder(\n",
    "          points=None,\n",
    "          boxes=box_torch,\n",
    "          masks=None,\n",
    "      )\n",
    "    low_res_masks, iou_predictions = sam_model.mask_decoder(\n",
    "      image_embeddings=image_embedding,\n",
    "      image_pe=sam_model.prompt_encoder.get_dense_pe(),\n",
    "      sparse_prompt_embeddings=sparse_embeddings,\n",
    "      dense_prompt_embeddings=dense_embeddings,\n",
    "      multimask_output=False,\n",
    "    )\n",
    "\n",
    "    upscaled_masks = sam_model.postprocess_masks(low_res_masks, input_size, original_image_size).to(device)\n",
    "    binary_mask = normalize(threshold(upscaled_masks, 0.0, 0))\n",
    "\n",
    "    gt_mask_resized = torch.from_numpy(np.resize(ground_truth_masks[k], (1, 1, ground_truth_masks[k].shape[0], ground_truth_masks[k].shape[1]))).to(device)\n",
    "    gt_binary_mask = torch.as_tensor(gt_mask_resized > 0, dtype=torch.float32)\n",
    "\n",
    "    loss = loss_fn(binary_mask, gt_binary_mask)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    epoch_losses.append(loss.item())\n",
    "  losses.append(epoch_losses)\n",
    "  print(f'EPOCH: {epoch}')\n",
    "  print(f'Mean loss: {mean(epoch_losses)}')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
