{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! pip install kaggle &> /dev/null\n",
    "! pip install torch torchvision &> /dev/null\n",
    "! pip install opencv-python pycocotools matplotlib onnxruntime onnx &> /dev/null\n",
    "! pip install git+https://github.com/facebookresearch/segment-anything.git &> /dev/null\n",
    "! wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth &> /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MASK_DIR = \"_data/combined/train/leaf_instances\"\n",
    "RGB_DIR = \"_data/combined/train/images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_type = 'vit_b'\n",
    "checkpoint = 'sam_vit_b_01ec64.pth'\n",
    "device = 'cuda:0'"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "tags": []
   },
   "source": [
    "import os\n",
    "bbox_coords = {}\n",
    "ground_truth_masks = {}\n",
    "for file in os.listdir(MASK_DIR):\n",
    "    if os.path.isdir(file):\n",
    "        continue\n",
    "    im = cv2.imread(os.path.join(MASK_DIR, file), cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "    mask = np.array(im)\n",
    "    \n",
    "    unique_categories = np.unique(mask)\n",
    "    unique_categories = unique_categories[unique_categories > 0]  # Exclude background (assumed to be 0)\n",
    "\n",
    "    for category_id in unique_categories:\n",
    "        y, x = np.nonzero(mask)\n",
    "        x_min = np.min(x)\n",
    "        y_min = np.min(y)\n",
    "        x_max = np.max(x)\n",
    "        y_max = np.max(y)\n",
    "        bbox_coords[f\"{file}_{category_id}\"] = np.array([x_min, y_min, x_max, y_max])\n",
    "        ground_truth_masks[f\"{file}_{category_id}\"] = (mask == category_id)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "tags": []
   },
   "source": [
    "len(bbox_coords.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from segment_anything import SamPredictor, sam_model_registry\n",
    "sam_model = sam_model_registry[model_type]()\n",
    "sam_model.to(device)\n",
    "sam_model.train();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "# Preprocess the images\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "\n",
    "from segment_anything.utils.transforms import ResizeLongestSide\n",
    "import os\n",
    "\n",
    "transform = ResizeLongestSide(sam_model.image_encoder.img_size)\n",
    "\n",
    "class LeafInstanceDataset(Dataset):\n",
    "    def __init__(self, path):\n",
    "        self.path = path\n",
    "        self.files = os.listdir(os.path.join(path, \"images\"))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.path, \"images\", self.files[idx])\n",
    "        mask_path =  os.path.join(self.path, \"leaf_instances\", self.files[idx])\n",
    "        mask_im = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "        mask = np.array(mask_im)\n",
    "\n",
    "        unique_categories = np.unique(mask)\n",
    "        unique_categories = unique_categories[unique_categories > 0]  # Exclude background (assumed to be 0)\n",
    "        \n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        input_image = transform.apply_image(image)\n",
    "        input_image_torch = torch.as_tensor(input_image, device=device)\n",
    "        transformed_image = input_image_torch.permute(2, 0, 1).contiguous()[None, :, :, :]\n",
    "\n",
    "        input_image = sam_model.preprocess(transformed_image)\n",
    "        original_image_size = image.shape[:2]\n",
    "        input_size = tuple(transformed_image.shape[-2:])\n",
    "        \n",
    "        data = [{}] * len(unique_categories)\n",
    "        \n",
    "        for category_index, category_id in enumerate(unique_categories):\n",
    "            y, x = np.nonzero(mask)\n",
    "            x_min = np.min(x)\n",
    "            y_min = np.min(y)\n",
    "            x_max = np.max(x)\n",
    "            y_max = np.max(y)\n",
    "            data[category_index][\"bbox\"] = np.array([x_min, y_min, x_max, y_max])\n",
    "            data[category_index][\"bbox_transformed\"] = transform.apply_boxes(np.array([x_min, y_min, x_max, y_max]), original_image_size)\n",
    "            data[category_index][\"mask\"] = (mask == category_id).squeeze()\n",
    "            data[category_index][\"image\"] = input_image.squeeze()\n",
    "            data[category_index][\"input_size\"] = input_size\n",
    "            data[category_index]['original_image_size'] = original_image_size\n",
    "            \n",
    "        return data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Preprocess the images\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "\n",
    "from segment_anything.utils.transforms import ResizeLongestSide\n",
    "\n",
    "\n",
    "transformed_data = defaultdict(dict)\n",
    "for k in bbox_coords.keys():\n",
    "    filename = k.rsplit(\"_\", 1)[0]\n",
    "    image = cv2.imread(os.path.join(RGB_DIR, filename))\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    input_image = transform.apply_image(image)\n",
    "    input_image_torch = torch.as_tensor(input_image)\n",
    "    transformed_image = input_image_torch.permute(2, 0, 1).contiguous()[None, :, :, :]\n",
    "\n",
    "    input_image = sam_model.preprocess(transformed_image)\n",
    "    original_image_size = image.shape[:2]\n",
    "    input_size = tuple(transformed_image.shape[-2:])\n",
    "\n",
    "    transformed_data[k]['image'] = input_image\n",
    "    transformed_data[k]['input_size'] = input_size\n",
    "    transformed_data[k]['original_image_size'] = original_image_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set up the optimizer, hyperparameter tuning will improve performance here\n",
    "lr = 1e-4\n",
    "wd = 0\n",
    "optimizer = torch.optim.Adam(sam_model.mask_decoder.parameters(), lr=lr, weight_decay=wd)\n",
    "\n",
    "#loss_fn = torch.nn.MSELoss()\n",
    "loss_fn = torch.nn.BCELoss()\n",
    "#keys = list(bbox_coords.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from statistics import mean\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torch.nn.functional import threshold, normalize\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "num_epochs = 100\n",
    "losses = []\n",
    "data_loader = DataLoader(LeafInstanceDataset(\"_data/combined/train\"))\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "  epoch_losses = []\n",
    "  # Just train on the first 20 examples\n",
    "  for data in data_loader:\n",
    "    for i in range(len(data)):\n",
    "        input_image = data[i]['image'].to(device)\n",
    "        input_size = data[i]['input_size']\n",
    "        original_image_size = data[i]['original_image_size']\n",
    "\n",
    "        # No grad here as we don't want to optimise the encoders\n",
    "        with torch.no_grad():\n",
    "          image_embedding = sam_model.image_encoder(input_image)\n",
    "\n",
    "          box = data[i][\"bbox_transformed\"]\n",
    "          box_torch = torch.as_tensor(box, dtype=torch.float, device=device)\n",
    "          box_torch = box_torch[None, :]\n",
    "\n",
    "          sparse_embeddings, dense_embeddings = sam_model.prompt_encoder(\n",
    "              points=None,\n",
    "              boxes=box_torch,\n",
    "              masks=None,\n",
    "          )\n",
    "        low_res_masks, iou_predictions = sam_model.mask_decoder(\n",
    "          image_embeddings=image_embedding,\n",
    "          image_pe=sam_model.prompt_encoder.get_dense_pe(),\n",
    "          sparse_prompt_embeddings=sparse_embeddings,\n",
    "          dense_prompt_embeddings=dense_embeddings,\n",
    "          multimask_output=False,\n",
    "        )\n",
    "\n",
    "        upscaled_masks = sam_model.postprocess_masks(low_res_masks, input_size, original_image_size).to(device)\n",
    "        binary_mask = normalize(threshold(upscaled_masks, 0.0, 0))\n",
    "\n",
    "        data[i][\"mask\"] = data[i][\"mask\"].squeeze()\n",
    "        gt_mask_resized = torch.from_numpy(np.resize(data[i][\"mask\"], (1, 1, data[i][\"mask\"].shape[0], data[i][\"mask\"].shape[1]))).to(device)\n",
    "        gt_binary_mask = torch.as_tensor(gt_mask_resized > 0, dtype=torch.float32)\n",
    "\n",
    "        loss = loss_fn(binary_mask, gt_binary_mask)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_losses.append(loss.item())\n",
    "  losses.append(epoch_losses)\n",
    "  print(f'EPOCH: {epoch}')\n",
    "  print(f'Mean loss: {mean(epoch_losses)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'(' was never closed (852363337.py, line 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[10], line 10\u001b[0;36m\u001b[0m\n\u001b[0;31m    data_loader = DataLoader(LeafInstanceDataset(\"_data/combined\")\u001b[0m\n\u001b[0m                            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m '(' was never closed\n"
     ]
    }
   ],
   "source": [
    "from statistics import mean\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torch.nn.functional import threshold, normalize\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "num_epochs = 100\n",
    "losses = []\n",
    "data_loader = DataLoader(LeafInstanceDataset(\"_data/combined\"))\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "  epoch_losses = []\n",
    "  # Just train on the first 20 examples\n",
    "  for k in keys:\n",
    "    input_image = transformed_data[k]['image'].to(device)\n",
    "    input_size = transformed_data[k]['input_size']\n",
    "    original_image_size = transformed_data[k]['original_image_size']\n",
    "\n",
    "    # No grad here as we don't want to optimise the encoders\n",
    "    with torch.no_grad():\n",
    "      image_embedding = sam_model.image_encoder(input_image)\n",
    "\n",
    "      prompt_box = bbox_coords[k]\n",
    "      box = transform.apply_boxes(prompt_box, original_image_size)\n",
    "      box_torch = torch.as_tensor(box, dtype=torch.float, device=device)\n",
    "      box_torch = box_torch[None, :]\n",
    "\n",
    "      sparse_embeddings, dense_embeddings = sam_model.prompt_encoder(\n",
    "          points=None,\n",
    "          boxes=box_torch,\n",
    "          masks=None,\n",
    "      )\n",
    "    low_res_masks, iou_predictions = sam_model.mask_decoder(\n",
    "      image_embeddings=image_embedding,\n",
    "      image_pe=sam_model.prompt_encoder.get_dense_pe(),\n",
    "      sparse_prompt_embeddings=sparse_embeddings,\n",
    "      dense_prompt_embeddings=dense_embeddings,\n",
    "      multimask_output=False,\n",
    "    )\n",
    "\n",
    "    upscaled_masks = sam_model.postprocess_masks(low_res_masks, input_size, original_image_size).to(device)\n",
    "    binary_mask = normalize(threshold(upscaled_masks, 0.0, 0))\n",
    "\n",
    "    gt_mask_resized = torch.from_numpy(np.resize(ground_truth_masks[k], (1, 1, ground_truth_masks[k].shape[0], ground_truth_masks[k].shape[1]))).to(device)\n",
    "    gt_binary_mask = torch.as_tensor(gt_mask_resized > 0, dtype=torch.float32)\n",
    "\n",
    "    loss = loss_fn(binary_mask, gt_binary_mask)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    epoch_losses.append(loss.item())\n",
    "  losses.append(epoch_losses)\n",
    "  print(f'EPOCH: {epoch}')\n",
    "  print(f'Mean loss: {mean(epoch_losses)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch w/ CUDA 12.4",
   "language": "python",
   "name": "pytorch_cuda_12.4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
