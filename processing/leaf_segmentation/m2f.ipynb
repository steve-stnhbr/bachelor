{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stefan.steinheber/.conda/envs/pytorch/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import lib.Mask2Former as m2f\n",
    "import lib.Mask2Former.mask2former as mask2former\n",
    "import os\n",
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from detectron2.engine import (launch)\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.projects.deeplab import add_deeplab_config\n",
    "from detectron2.data import build_detection_train_loader\n",
    "from lib.Mask2Former.train_net import Trainer\n",
    "import numpy as np\n",
    "from detectron2.structures import Boxes, Instances, BitMasks, BoxMode\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from detectron2.evaluation import DatasetEvaluator, DatasetEvaluators\n",
    "from detectron2.data import build_detection_test_loader\n",
    "from detectron2.utils import comm\n",
    "from detectron2.structures import BoxMode, pairwise_iou\n",
    "import copy\n",
    "import random\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATA_SOURCE = \"combined\"\n",
    "DATA_LOCATION = \"_data\"\n",
    "DATA_DIR = \"coco\"\n",
    "os.environ[\"DETECTRON2_DATASETS\"] = os.path.join(DATA_LOCATION, DATA_DIR)\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert the dataset to COCO format\n",
    "The following commands convert the existing PNG mask-based dataset to the coco annotations required for training Mask2Former"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "tags": []
   },
   "source": [
    "!cd {DATA_LOCATION} && python mask_to_coco.py --images {DATA_SOURCE}/val/images/ --masks {DATA_SOURCE}/val/leaf_instances/ --output {DATA_DIR}/annotations/instances_val2017.json --fixed-category-id 58 --fixed-category-name \"potted plant\" --default-categories\n",
    "!cd {DATA_LOCATION} && python mask_to_coco.py --images {DATA_SOURCE}/train/images/ --masks {DATA_SOURCE}/train/leaf_instances/ --output {DATA_DIR}/annotations/instances_train2017.json --fixed-category-id 58 --fixed-category-name \"potted plant\" --default-categories"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "tags": []
   },
   "source": [
    "!cd {DATA_LOCATION} && mkdir {DATA_DIR}/train2017\n",
    "!cd {DATA_LOCATION} && cp {DATA_SOURCE}/train/images/* {DATA_DIR}/train2017\n",
    "!cd {DATA_LOCATION} && mkdir {DATA_DIR}/val2017\n",
    "!cd {DATA_LOCATION} && cp {DATA_SOURCE}/val/images/* {DATA_DIR}/val2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#CONFIG = \"lib/Mask2Former/configs/coco/instance-segmentation/swin/maskformer2_swin_base_IN21k_384_bs16_50ep.yaml\"\n",
    "CONFIG = \"configs/gen_mask2former.yaml\"\n",
    "NUM_GPUS = 1\n",
    "BATCH_SIZE = 8\n",
    "LEARNING_RATE = 0.001\n",
    "DATASET_DIR = \"_data/urban_street_combined\"\n",
    "DATASET_DIR_VAL = \"_data/combined/val\"\n",
    "IMAGES_DIR_NAME = \"images\"\n",
    "IMAGE_DIR = os.path.join(DATASET_DIR, IMAGES_DIR_NAME)\n",
    "INSTANCES_DIR_NAME = \"leaf_instances\"\n",
    "INSTANCES_DIR = os.path.join(DATASET_DIR, INSTANCES_DIR_NAME)\n",
    "IMAGE_DIR_VAL = os.path.join(DATASET_DIR_VAL, IMAGES_DIR_NAME)\n",
    "INSTANCES_DIR_VAL = os.path.join(DATASET_DIR_VAL, INSTANCES_DIR_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LeavesDataset(Dataset):\n",
    "    def __init__(self, image_dir, label_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.transform = transform\n",
    "        self.image_files = os.listdir(image_dir)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_path = os.path.join(self.image_dir, self.image_files[index])\n",
    "        label_path = os.path.join(self.label_dir, self.image_files[index])\n",
    "\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        label = Image.open(label_path).convert(\"L\")\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            #label = self.transform(label).squeeze()\n",
    "\n",
    "        # Convert label to tensor\n",
    "        label = torch.from_numpy(np.array(label))\n",
    "\n",
    "        # Create instances\n",
    "        instances = Instances(image.shape[1:])\n",
    "\n",
    "        # Create gt_boxes\n",
    "        boxes = []\n",
    "        gt_classes = []\n",
    "        gt_masks = []\n",
    "        unique_labels = torch.unique(label)\n",
    "        if len(unique_labels) > 1:\n",
    "            if 255 in unique_labels: \n",
    "                print(\"Invalid label in file\", image_path)\n",
    "            for obj_class in unique_labels:\n",
    "                if obj_class > 0:\n",
    "                    mask = label == obj_class\n",
    "                    coords = torch.nonzero(mask)\n",
    "                    xmin, ymin = coords.min(dim=0).values\n",
    "                    xmax, ymax = coords.max(dim=0).values\n",
    "                    box = [xmin, ymin, xmax, ymax]\n",
    "                    boxes.append(box)\n",
    "                    gt_classes.append(obj_class.item())\n",
    "                    gt_masks.append(mask)\n",
    "\n",
    "            instances.gt_boxes = Boxes(torch.tensor(boxes))\n",
    "            instances.gt_classes = torch.tensor(gt_classes, dtype=torch.long)\n",
    "\n",
    "            # Resize masks to match the image size\n",
    "            resized_masks = []\n",
    "            for mask in gt_masks:\n",
    "                resized_mask = F.interpolate(mask.unsqueeze(0).unsqueeze(0).float(), size=image.shape[1:], mode='nearest').squeeze().to(torch.bool)\n",
    "                resized_masks.append(resized_mask)\n",
    "\n",
    "            if len(resized_masks) > 0:\n",
    "                instances.gt_masks = torch.stack(resized_masks)\n",
    "            else:\n",
    "                print(\"Masks empty, class lenght is\", len(gt_classes))\n",
    "                instances.gt_masks = torch.Tensor()\n",
    "\n",
    "            return {\n",
    "                \"image\": image,\n",
    "                \"height\": image.shape[1],\n",
    "                \"width\": image.shape[2],\n",
    "                \"instances\": instances,\n",
    "            }\n",
    "        \n",
    "        return {\n",
    "            \"image\": image,\n",
    "            \"height\": image.shape[1],\n",
    "            \"width\": image.shape[2]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(imgs):\n",
    "    if not isinstance(imgs, list):\n",
    "        imgs = [imgs]\n",
    "    fix, axs = plt.subplots(ncols=len(imgs), squeeze=False)\n",
    "    for i, img in enumerate(imgs):\n",
    "        img = img.detach()\n",
    "        img = F.to_pil_image(img)\n",
    "        axs[0, i].imshow(np.asarray(img))\n",
    "        axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "test_ds = LeavesDataset(IMAGE_DIR, INSTANCES_DIR)\n",
    "\n",
    "sample = next(test_ds)\n",
    "result = draw_bounding_boxes(sample[\"image\"], [instance[\"gt_boxes\"] for instance in sample[\"instances\"]], width=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LeavesEvaluator(DatasetEvaluator):\n",
    "    def __init__(self, dataset_name):\n",
    "        self.dataset_name = dataset_name\n",
    "        self._cpu_device = torch.device(\"cpu\")\n",
    "\n",
    "    def reset(self):\n",
    "        self._predictions = []\n",
    "        self._targets = []\n",
    "\n",
    "    def process(self, inputs, outputs):\n",
    "        # sample single random instance\n",
    "#        idx = random.randrange(len(inputs))\n",
    "#        self._predictions.append(outputs[idx][\"instances\"].to(self._cpu_device))\n",
    "#        self._targets.append(inputs[idx][\"instances\"].to(self._cpu_device))\n",
    "        for input, output in zip(inputs, outputs):\n",
    "            self._predictions.append(output[\"instances\"].to(self._cpu_device))\n",
    "            self._targets.append(input[\"instances\"].to(self._cpu_device))\n",
    "\n",
    "    def evaluate(self):\n",
    "        if comm.is_main_process():\n",
    "            self._evaluate()\n",
    "\n",
    "        if comm.is_main_process():\n",
    "            return copy.deepcopy(self._results)\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def _evaluate(self):\n",
    "        self._results = {}\n",
    "        iou_thresholds = [0.5, 0.75]\n",
    "        for iou_threshold in iou_thresholds:\n",
    "            self._results[f\"IoU_{iou_threshold}\"] = self._compute_iou(iou_threshold)\n",
    "        #self._results[\"mask_mse_loss\"] = self._compute_mask_mse_loss()\n",
    "        print(self._results)\n",
    "        print(\"First Target:\", self._targets[0])\n",
    "        print(\"First Prediction\", self._predictions[0])\n",
    "\n",
    "    def _compute_iou(self, iou_threshold):\n",
    "        print(\"Computing IoU\")\n",
    "        num_instances = len(self._predictions)\n",
    "        iou_sum = 0.0\n",
    "\n",
    "        for pred, target in zip(self._predictions, self._targets):\n",
    "            pred_boxes = pred.pred_boxes.tensor\n",
    "            target_boxes = target.gt_boxes.tensor\n",
    "\n",
    "            if len(pred_boxes) == 0 or len(target_boxes) == 0:\n",
    "                continue\n",
    "\n",
    "            # Convert the boxes to the format expected by the pairwise_iou function\n",
    "            pred_boxes = BoxMode.convert(pred_boxes, BoxMode.XYXY_ABS, BoxMode.XYXY_ABS)\n",
    "            target_boxes = BoxMode.convert(target_boxes, BoxMode.XYXY_ABS, BoxMode.XYXY_ABS)\n",
    "\n",
    "            # Compute IoU between predicted and target boxes\n",
    "            iou_matrix = pairwise_iou(Boxes(pred_boxes), Boxes(target_boxes))\n",
    "            max_iou, _ = iou_matrix.max(dim=1)\n",
    "\n",
    "            # Count the number of predicted boxes with IoU above the threshold\n",
    "            num_above_threshold = (max_iou > iou_threshold).sum().item()\n",
    "            iou_sum += num_above_threshold\n",
    "\n",
    "        avg_iou = iou_sum / num_instances\n",
    "        return avg_iou\n",
    "    \n",
    "    def _compute_mask_mse_loss(self):\n",
    "        print(\"Computing Mask MSE Loss\")\n",
    "        loss = 0\n",
    "        for pred, target in zip(self._predictions, self._targets):\n",
    "            for pred_mask, target_mask in zip(pred.pred_masks, target.gt_masks):\n",
    "                print(\"Single MSE\")\n",
    "                target_mask = target_mask.float()\n",
    "                diff2 = (torch.flatten(pred_mask) - torch.flatten(target_mask)) ** 2.0\n",
    "                sum2 = 0.0\n",
    "                num = 0\n",
    "\n",
    "                flat_mask = torch.flatten(target_mask)\n",
    "                assert(len(flat_mask) == len(diff2))\n",
    "                for i in range(len(diff2)):\n",
    "                    if flat_mask[i] == 1:\n",
    "                        sum2 += diff2[i]\n",
    "                        num += 1\n",
    "\n",
    "                loss += sum2 / num\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    images = []\n",
    "    instances = []\n",
    "    extras = {}\n",
    "\n",
    "    for item in batch:\n",
    "        images.append(item[\"image\"])\n",
    "        \n",
    "        item_instances = item[\"instances\"]\n",
    "        item_instances[\"gt_boxes\"] = torch.tensor(item_instances[\"gt_boxes\"])\n",
    "        item_instances[\"gt_classes\"] = torch.tensor(item_instances[\"gt_classes\"], dtype=torch.long)\n",
    "        item_instances[\"gt_masks\"] = torch.tensor(item_instances[\"gt_masks\"])\n",
    "        instances.append(item_instances)\n",
    "        \n",
    "        extras[\"height\"] = item[\"height\"]\n",
    "        extras[\"width\"] = item[\"width\"]\n",
    "\n",
    "    batched_inputs = [\n",
    "        {\"image\": image, \"instances\": instance, **extras}\n",
    "        for image, instance in zip(images, instances)\n",
    "    ]\n",
    "\n",
    "    return batched_inputs\n",
    "\n",
    "class LeavesTrainer(Trainer):\n",
    "    @classmethod\n",
    "    def build_train_loader(cls, _):\n",
    "        # Define your data transforms\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((800, 800)),\n",
    "            transforms.ToTensor(),\n",
    "            #transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "\n",
    "        # Create the dataset\n",
    "        dataset = LeavesDataset(IMAGE_DIR, INSTANCES_DIR, transform=transform, )\n",
    "        \n",
    "        # Create the DataLoader\n",
    "        dataloader = build_detection_train_loader(dataset, mapper=None, total_batch_size=8, num_workers=2)\n",
    "        return dataloader\n",
    "    \n",
    "    @classmethod\n",
    "    def build_test_loader(cls, cfg, dataset_name):\n",
    "        # Define your data transforms\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((800, 800)),\n",
    "            transforms.ToTensor(),\n",
    "            #transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "\n",
    "        # Create the dataset\n",
    "        dataset = LeavesDataset(IMAGE_DIR_VAL, INSTANCES_DIR_VAL, transform=transform, )\n",
    "        \n",
    "        # Create the DataLoader\n",
    "        dataloader = build_detection_test_loader(dataset, mapper=None, num_workers=2)\n",
    "        return dataloader\n",
    "        \n",
    "    @classmethod\n",
    "    def build_evaluator(cls, cfg, dataset_name, output_folder=None):\n",
    "        return LeavesEvaluator(dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_trainer(cfg):\n",
    "    trainer = LeavesTrainer(cfg)\n",
    "    #trainer.resume_or_load(resume=args.resume)\n",
    "    return trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stefan.steinheber/.conda/envs/pytorch/lib/python3.12/site-packages/torch/functional.py:512: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /opt/conda/conda-bld/pytorch_1716905971214/work/aten/src/ATen/native/TensorShape.cpp:3587.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[07/17 14:45:21 d2.engine.defaults]: \u001b[0mModel:\n",
      "MaskFormer(\n",
      "  (backbone): D2SwinTransformer(\n",
      "    (patch_embed): PatchEmbed(\n",
      "      (proj): Conv2d(3, 128, kernel_size=(4, 4), stride=(4, 4))\n",
      "      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (pos_drop): Dropout(p=0.0, inplace=False)\n",
      "    (layers): ModuleList(\n",
      "      (0): BasicLayer(\n",
      "        (blocks): ModuleList(\n",
      "          (0): SwinTransformerBlock(\n",
      "            (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): Identity()\n",
      "            (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (1): SwinTransformerBlock(\n",
      "            (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.013)\n",
      "            (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (downsample): PatchMerging(\n",
      "          (reduction): Linear(in_features=512, out_features=256, bias=False)\n",
      "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicLayer(\n",
      "        (blocks): ModuleList(\n",
      "          (0): SwinTransformerBlock(\n",
      "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.026)\n",
      "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (1): SwinTransformerBlock(\n",
      "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.039)\n",
      "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (downsample): PatchMerging(\n",
      "          (reduction): Linear(in_features=1024, out_features=512, bias=False)\n",
      "          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (2): BasicLayer(\n",
      "        (blocks): ModuleList(\n",
      "          (0): SwinTransformerBlock(\n",
      "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.052)\n",
      "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (1): SwinTransformerBlock(\n",
      "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.065)\n",
      "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (2): SwinTransformerBlock(\n",
      "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.078)\n",
      "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (3): SwinTransformerBlock(\n",
      "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.091)\n",
      "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (4): SwinTransformerBlock(\n",
      "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.104)\n",
      "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (5): SwinTransformerBlock(\n",
      "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.117)\n",
      "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (6): SwinTransformerBlock(\n",
      "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.130)\n",
      "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (7): SwinTransformerBlock(\n",
      "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.143)\n",
      "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (8): SwinTransformerBlock(\n",
      "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.157)\n",
      "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (9): SwinTransformerBlock(\n",
      "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.170)\n",
      "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (10): SwinTransformerBlock(\n",
      "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.183)\n",
      "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (11): SwinTransformerBlock(\n",
      "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.196)\n",
      "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (12): SwinTransformerBlock(\n",
      "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.209)\n",
      "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (13): SwinTransformerBlock(\n",
      "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.222)\n",
      "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (14): SwinTransformerBlock(\n",
      "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.235)\n",
      "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (15): SwinTransformerBlock(\n",
      "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.248)\n",
      "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (16): SwinTransformerBlock(\n",
      "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.261)\n",
      "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (17): SwinTransformerBlock(\n",
      "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.274)\n",
      "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (downsample): PatchMerging(\n",
      "          (reduction): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "          (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (3): BasicLayer(\n",
      "        (blocks): ModuleList(\n",
      "          (0): SwinTransformerBlock(\n",
      "            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.287)\n",
      "            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (1): SwinTransformerBlock(\n",
      "            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.300)\n",
      "            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (norm0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    (norm3): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (sem_seg_head): MaskFormerHead(\n",
      "    (pixel_decoder): MSDeformAttnPixelDecoder(\n",
      "      (input_proj): ModuleList(\n",
      "        (0): Sequential(\n",
      "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "        )\n",
      "        (1): Sequential(\n",
      "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "        )\n",
      "        (2): Sequential(\n",
      "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "        )\n",
      "      )\n",
      "      (transformer): MSDeformAttnTransformerEncoderOnly(\n",
      "        (encoder): MSDeformAttnTransformerEncoder(\n",
      "          (layers): ModuleList(\n",
      "            (0-5): 6 x MSDeformAttnTransformerEncoderLayer(\n",
      "              (self_attn): MSDeformAttn(\n",
      "                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)\n",
      "                (attention_weights): Linear(in_features=256, out_features=96, bias=True)\n",
      "                (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "                (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "              )\n",
      "              (dropout1): Dropout(p=0.0, inplace=False)\n",
      "              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "              (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "              (dropout2): Dropout(p=0.0, inplace=False)\n",
      "              (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "              (dropout3): Dropout(p=0.0, inplace=False)\n",
      "              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (pe_layer): Positional encoding PositionEmbeddingSine\n",
      "          num_pos_feats: 128\n",
      "          temperature: 10000\n",
      "          normalize: True\n",
      "          scale: 6.283185307179586\n",
      "      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (adapter_1): Conv2d(\n",
      "        128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "      )\n",
      "      (layer_1): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "      )\n",
      "    )\n",
      "    (predictor): MultiScaleMaskedTransformerDecoder(\n",
      "      (pe_layer): Positional encoding PositionEmbeddingSine\n",
      "          num_pos_feats: 128\n",
      "          temperature: 10000\n",
      "          normalize: True\n",
      "          scale: 6.283185307179586\n",
      "      (transformer_self_attention_layers): ModuleList(\n",
      "        (0-8): 9 x SelfAttentionLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (transformer_cross_attention_layers): ModuleList(\n",
      "        (0-8): 9 x CrossAttentionLayer(\n",
      "          (multihead_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (transformer_ffn_layers): ModuleList(\n",
      "        (0-8): 9 x FFNLayer(\n",
      "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (query_feat): Embedding(100, 256)\n",
      "      (query_embed): Embedding(100, 256)\n",
      "      (level_embed): Embedding(3, 256)\n",
      "      (input_proj): ModuleList(\n",
      "        (0-2): 3 x Sequential()\n",
      "      )\n",
      "      (class_embed): Linear(in_features=256, out_features=81, bias=True)\n",
      "      (mask_embed): MLP(\n",
      "        (layers): ModuleList(\n",
      "          (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (criterion): Criterion SetCriterion\n",
      "      matcher: Matcher HungarianMatcher\n",
      "          cost_class: 2.0\n",
      "          cost_mask: 5.0\n",
      "          cost_dice: 5.0\n",
      "      losses: ['labels', 'masks']\n",
      "      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}\n",
      "      num_classes: 80\n",
      "      eos_coef: 0.1\n",
      "      num_points: 12544\n",
      "      oversample_ratio: 3.0\n",
      "      importance_sample_ratio: 0.75\n",
      ")\n",
      "relative_position_bias_table\n",
      "relative_position_bias_table\n",
      "relative_position_bias_table\n",
      "relative_position_bias_table\n",
      "relative_position_bias_table\n",
      "relative_position_bias_table\n",
      "relative_position_bias_table\n",
      "relative_position_bias_table\n",
      "relative_position_bias_table\n",
      "relative_position_bias_table\n",
      "relative_position_bias_table\n",
      "relative_position_bias_table\n",
      "relative_position_bias_table\n",
      "relative_position_bias_table\n",
      "relative_position_bias_table\n",
      "relative_position_bias_table\n",
      "relative_position_bias_table\n",
      "relative_position_bias_table\n",
      "relative_position_bias_table\n",
      "relative_position_bias_table\n",
      "relative_position_bias_table\n",
      "relative_position_bias_table\n",
      "relative_position_bias_table\n",
      "relative_position_bias_table\n",
      "\u001b[32m[07/17 14:45:21 d2.data.build]: \u001b[0mMaking batched data loader with batch_size=8\n",
      "\u001b[32m[07/17 14:45:21 d2.engine.train_loop]: \u001b[0mStarting training from iteration 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stefan.steinheber/.conda/envs/pytorch/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[07/17 14:45:55 d2.utils.events]: \u001b[0m eta: 6 days, 21:50:38  iter: 19  total_loss: 111.3  loss_ce: 4.548  loss_mask: 1.918  loss_dice: 4.556  loss_ce_0: 8.953  loss_mask_0: 1.828  loss_dice_0: 4.281  loss_ce_1: 5.033  loss_mask_1: 1.814  loss_dice_1: 4.304  loss_ce_2: 4.605  loss_mask_2: 1.658  loss_dice_2: 4.414  loss_ce_3: 4.407  loss_mask_3: 1.78  loss_dice_3: 4.492  loss_ce_4: 4.416  loss_mask_4: 1.741  loss_dice_4: 4.513  loss_ce_5: 4.405  loss_mask_5: 1.855  loss_dice_5: 4.518  loss_ce_6: 4.406  loss_mask_6: 1.853  loss_dice_6: 4.555  loss_ce_7: 4.624  loss_mask_7: 1.966  loss_dice_7: 4.578  loss_ce_8: 4.553  loss_mask_8: 1.994  loss_dice_8: 4.566    time: 1.6151  last_time: 1.6164  data_time: 0.1548  last_data_time: 0.1250   lr: 1e-05  max_mem: 39896M\n",
      "\u001b[32m[07/17 14:46:27 d2.utils.events]: \u001b[0m eta: 6 days, 21:41:28  iter: 39  total_loss: 102.1  loss_ce: 3.568  loss_mask: 1.66  loss_dice: 4.62  loss_ce_0: 8.933  loss_mask_0: 1.622  loss_dice_0: 4.302  loss_ce_1: 3.557  loss_mask_1: 1.559  loss_dice_1: 4.406  loss_ce_2: 3.507  loss_mask_2: 1.534  loss_dice_2: 4.42  loss_ce_3: 3.503  loss_mask_3: 1.529  loss_dice_3: 4.483  loss_ce_4: 3.512  loss_mask_4: 1.55  loss_dice_4: 4.514  loss_ce_5: 3.546  loss_mask_5: 1.548  loss_dice_5: 4.563  loss_ce_6: 3.543  loss_mask_6: 1.608  loss_dice_6: 4.579  loss_ce_7: 3.592  loss_mask_7: 1.632  loss_dice_7: 4.605  loss_ce_8: 3.567  loss_mask_8: 1.646  loss_dice_8: 4.603    time: 1.6138  last_time: 1.6120  data_time: 0.1205  last_data_time: 0.1088   lr: 1e-05  max_mem: 39896M\n",
      "\u001b[32m[07/17 14:47:00 d2.utils.events]: \u001b[0m eta: 6 days, 21:49:33  iter: 59  total_loss: 98.8  loss_ce: 3.363  loss_mask: 1.818  loss_dice: 4.549  loss_ce_0: 8.856  loss_mask_0: 1.625  loss_dice_0: 4.348  loss_ce_1: 3.27  loss_mask_1: 1.662  loss_dice_1: 4.32  loss_ce_2: 3.239  loss_mask_2: 1.613  loss_dice_2: 4.366  loss_ce_3: 3.169  loss_mask_3: 1.515  loss_dice_3: 4.374  loss_ce_4: 3.286  loss_mask_4: 1.539  loss_dice_4: 4.422  loss_ce_5: 3.3  loss_mask_5: 1.494  loss_dice_5: 4.447  loss_ce_6: 3.306  loss_mask_6: 1.507  loss_dice_6: 4.471  loss_ce_7: 3.32  loss_mask_7: 1.673  loss_dice_7: 4.485  loss_ce_8: 3.362  loss_mask_8: 1.795  loss_dice_8: 4.497    time: 1.6151  last_time: 1.6019  data_time: 0.1171  last_data_time: 0.1021   lr: 1e-05  max_mem: 39896M\n",
      "\u001b[32m[07/17 14:47:32 d2.utils.events]: \u001b[0m eta: 6 days, 21:46:56  iter: 79  total_loss: 98.29  loss_ce: 3.387  loss_mask: 1.471  loss_dice: 4.463  loss_ce_0: 8.821  loss_mask_0: 1.733  loss_dice_0: 4.195  loss_ce_1: 3.402  loss_mask_1: 1.751  loss_dice_1: 4.203  loss_ce_2: 3.278  loss_mask_2: 1.684  loss_dice_2: 4.177  loss_ce_3: 3.299  loss_mask_3: 1.707  loss_dice_3: 4.184  loss_ce_4: 3.367  loss_mask_4: 1.641  loss_dice_4: 4.234  loss_ce_5: 3.388  loss_mask_5: 1.659  loss_dice_5: 4.325  loss_ce_6: 3.392  loss_mask_6: 1.547  loss_dice_6: 4.373  loss_ce_7: 3.384  loss_mask_7: 1.498  loss_dice_7: 4.457  loss_ce_8: 3.404  loss_mask_8: 1.573  loss_dice_8: 4.414    time: 1.6183  last_time: 1.6008  data_time: 0.1280  last_data_time: 0.1068   lr: 1e-05  max_mem: 39899M\n",
      "\u001b[32m[07/17 14:48:05 d2.utils.events]: \u001b[0m eta: 6 days, 21:46:35  iter: 99  total_loss: 97.71  loss_ce: 3.45  loss_mask: 1.819  loss_dice: 4.187  loss_ce_0: 8.763  loss_mask_0: 2.005  loss_dice_0: 3.768  loss_ce_1: 3.458  loss_mask_1: 1.996  loss_dice_1: 3.714  loss_ce_2: 3.411  loss_mask_2: 1.972  loss_dice_2: 3.781  loss_ce_3: 3.345  loss_mask_3: 1.952  loss_dice_3: 3.762  loss_ce_4: 3.34  loss_mask_4: 1.983  loss_dice_4: 3.8  loss_ce_5: 3.369  loss_mask_5: 1.961  loss_dice_5: 3.844  loss_ce_6: 3.4  loss_mask_6: 1.942  loss_dice_6: 3.99  loss_ce_7: 3.447  loss_mask_7: 1.911  loss_dice_7: 4.001  loss_ce_8: 3.4  loss_mask_8: 1.941  loss_dice_8: 4.027    time: 1.6188  last_time: 1.5976  data_time: 0.1216  last_data_time: 0.1258   lr: 1e-05  max_mem: 39902M\n",
      "\u001b[32m[07/17 14:48:37 d2.utils.events]: \u001b[0m eta: 6 days, 21:30:44  iter: 119  total_loss: 94.38  loss_ce: 3.432  loss_mask: 2.047  loss_dice: 3.733  loss_ce_0: 8.748  loss_mask_0: 2.013  loss_dice_0: 3.429  loss_ce_1: 3.41  loss_mask_1: 1.979  loss_dice_1: 3.38  loss_ce_2: 3.382  loss_mask_2: 2.037  loss_dice_2: 3.411  loss_ce_3: 3.561  loss_mask_3: 2.041  loss_dice_3: 3.355  loss_ce_4: 3.553  loss_mask_4: 1.999  loss_dice_4: 3.396  loss_ce_5: 3.442  loss_mask_5: 2.038  loss_dice_5: 3.455  loss_ce_6: 3.415  loss_mask_6: 2.046  loss_dice_6: 3.491  loss_ce_7: 3.394  loss_mask_7: 2.005  loss_dice_7: 3.538  loss_ce_8: 3.373  loss_mask_8: 2.085  loss_dice_8: 3.61    time: 1.6181  last_time: 1.6572  data_time: 0.1251  last_data_time: 0.1393   lr: 1e-05  max_mem: 39902M\n",
      "\u001b[32m[07/17 14:49:09 d2.utils.events]: \u001b[0m eta: 6 days, 21:40:38  iter: 139  total_loss: 90.7  loss_ce: 3.36  loss_mask: 1.932  loss_dice: 3.344  loss_ce_0: 8.734  loss_mask_0: 1.919  loss_dice_0: 3.367  loss_ce_1: 3.289  loss_mask_1: 1.932  loss_dice_1: 3.228  loss_ce_2: 3.223  loss_mask_2: 1.962  loss_dice_2: 3.153  loss_ce_3: 3.273  loss_mask_3: 1.931  loss_dice_3: 3.156  loss_ce_4: 3.381  loss_mask_4: 1.971  loss_dice_4: 3.143  loss_ce_5: 3.369  loss_mask_5: 1.961  loss_dice_5: 3.102  loss_ce_6: 3.364  loss_mask_6: 1.99  loss_dice_6: 3.176  loss_ce_7: 3.329  loss_mask_7: 1.984  loss_dice_7: 3.234  loss_ce_8: 3.381  loss_mask_8: 1.967  loss_dice_8: 3.207    time: 1.6185  last_time: 1.5904  data_time: 0.1220  last_data_time: 0.1207   lr: 1e-05  max_mem: 39902M\n",
      "\u001b[32m[07/17 14:49:42 d2.utils.events]: \u001b[0m eta: 6 days, 21:33:27  iter: 159  total_loss: 91.06  loss_ce: 3.512  loss_mask: 1.887  loss_dice: 3.324  loss_ce_0: 8.691  loss_mask_0: 1.84  loss_dice_0: 3.374  loss_ce_1: 3.47  loss_mask_1: 1.869  loss_dice_1: 3.221  loss_ce_2: 3.463  loss_mask_2: 1.912  loss_dice_2: 3.194  loss_ce_3: 3.463  loss_mask_3: 1.944  loss_dice_3: 3.172  loss_ce_4: 3.549  loss_mask_4: 1.884  loss_dice_4: 3.12  loss_ce_5: 3.558  loss_mask_5: 1.892  loss_dice_5: 3.176  loss_ce_6: 3.573  loss_mask_6: 1.939  loss_dice_6: 3.237  loss_ce_7: 3.559  loss_mask_7: 1.96  loss_dice_7: 3.306  loss_ce_8: 3.549  loss_mask_8: 1.922  loss_dice_8: 3.248    time: 1.6177  last_time: 1.6120  data_time: 0.1194  last_data_time: 0.1188   lr: 1e-05  max_mem: 39902M\n",
      "\u001b[32m[07/17 14:50:14 d2.utils.events]: \u001b[0m eta: 6 days, 21:31:33  iter: 179  total_loss: 89.83  loss_ce: 3.453  loss_mask: 1.972  loss_dice: 3.11  loss_ce_0: 8.645  loss_mask_0: 1.871  loss_dice_0: 3.21  loss_ce_1: 3.377  loss_mask_1: 1.86  loss_dice_1: 3.118  loss_ce_2: 3.418  loss_mask_2: 1.943  loss_dice_2: 3.068  loss_ce_3: 3.459  loss_mask_3: 1.913  loss_dice_3: 3.029  loss_ce_4: 3.456  loss_mask_4: 1.965  loss_dice_4: 3.075  loss_ce_5: 3.462  loss_mask_5: 1.923  loss_dice_5: 3.024  loss_ce_6: 3.41  loss_mask_6: 1.883  loss_dice_6: 3.049  loss_ce_7: 3.427  loss_mask_7: 1.943  loss_dice_7: 3.093  loss_ce_8: 3.47  loss_mask_8: 1.972  loss_dice_8: 3.052    time: 1.6178  last_time: 1.6175  data_time: 0.1209  last_data_time: 0.1113   lr: 1e-05  max_mem: 39902M\n",
      "\u001b[32m[07/17 14:50:46 d2.evaluation.evaluator]: \u001b[0mStart inference on 69 batches\n",
      "\u001b[32m[07/17 14:50:50 d2.evaluation.evaluator]: \u001b[0mInference done 11/69. Dataloading: 0.0054 s/iter. Inference: 0.2249 s/iter. Eval: 0.0349 s/iter. Total: 0.2652 s/iter. ETA=0:00:15\n",
      "\u001b[32m[07/17 14:50:55 d2.evaluation.evaluator]: \u001b[0mInference done 32/69. Dataloading: 0.0073 s/iter. Inference: 0.2033 s/iter. Eval: 0.0389 s/iter. Total: 0.2497 s/iter. ETA=0:00:09\n",
      "\u001b[32m[07/17 14:51:00 d2.evaluation.evaluator]: \u001b[0mInference done 53/69. Dataloading: 0.0079 s/iter. Inference: 0.2016 s/iter. Eval: 0.0381 s/iter. Total: 0.2477 s/iter. ETA=0:00:03\n",
      "\u001b[32m[07/17 14:51:04 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:15.761271 (0.246270 s / iter per device, on 1 devices)\n",
      "\u001b[32m[07/17 14:51:04 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:12 (0.199640 s / iter per device, on 1 devices)\n",
      "Computing IoU\n",
      "Computing IoU\n",
      "{'IoU_0.5': 0.0, 'IoU_0.75': 0.0}\n",
      "First Target: Instances(num_instances=28, image_height=800, image_width=800, fields=[gt_boxes: Boxes(tensor([[  7., 661.,  33., 695.],\n",
      "        [  0., 678.,  12., 710.],\n",
      "        [  0., 720.,  21., 732.],\n",
      "        [  0., 726.,  12., 754.],\n",
      "        [855., 188., 872., 223.],\n",
      "        [865., 222., 884., 231.],\n",
      "        [853., 227., 872., 248.],\n",
      "        [839., 223., 857., 229.],\n",
      "        [816., 750., 876., 779.],\n",
      "        [777., 747., 795., 772.],\n",
      "        [793., 740., 819., 778.],\n",
      "        [799., 779., 827., 803.],\n",
      "        [767., 784., 801., 822.],\n",
      "        [793., 773., 804., 784.],\n",
      "        [641., 753., 653., 759.],\n",
      "        [621., 742., 648., 760.],\n",
      "        [600., 707., 632., 747.],\n",
      "        [600., 748., 616., 758.],\n",
      "        [611., 757., 637., 794.],\n",
      "        [420., 725., 467., 755.],\n",
      "        [419., 761., 465., 810.],\n",
      "        [413., 749., 426., 761.],\n",
      "        [391., 679., 425., 739.],\n",
      "        [362., 740., 411., 772.],\n",
      "        [198., 728., 241., 755.],\n",
      "        [188., 714., 207., 735.],\n",
      "        [182., 737., 196., 763.],\n",
      "        [151., 724., 185., 747.]])), gt_classes: tensor([ 25,  26,  27,  28, 139, 140, 141, 142, 165, 166, 167, 168, 169, 170,\n",
      "        171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184]), gt_masks: tensor([[[False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         ...,\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False]],\n",
      "\n",
      "        [[False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         ...,\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False]],\n",
      "\n",
      "        [[False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         ...,\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         ...,\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False]],\n",
      "\n",
      "        [[False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         ...,\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False]],\n",
      "\n",
      "        [[False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         ...,\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False]]])])\n",
      "First Prediction Instances(num_instances=100, image_height=800, image_width=800, fields=[pred_masks: tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]]), pred_boxes: Boxes(tensor([[0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.]])), scores: tensor([0.0269, 0.0252, 0.0253, 0.0264, 0.0254, 0.0229, 0.0250, 0.0232, 0.0297,\n",
      "        0.0250, 0.0363, 0.0241, 0.0000, 0.0280, 0.0255, 0.0260, 0.0241, 0.0212,\n",
      "        0.0202, 0.0310, 0.0326, 0.0246, 0.0267, 0.0202, 0.0226, 0.0226, 0.0217,\n",
      "        0.0182, 0.0265, 0.0197, 0.0214, 0.0248, 0.0000, 0.0000, 0.0000, 0.0273,\n",
      "        0.0256, 0.0255, 0.0311, 0.0236, 0.0251, 0.0259, 0.0276, 0.0235, 0.0210,\n",
      "        0.0196, 0.0245, 0.0197, 0.0276, 0.0230, 0.0249, 0.0214, 0.0269, 0.0222,\n",
      "        0.0323, 0.0247, 0.0243, 0.0314, 0.0315, 0.0255, 0.0198, 0.0285, 0.0295,\n",
      "        0.0288, 0.0266, 0.0000, 0.0000, 0.0000, 0.0282, 0.0273, 0.0245, 0.0233,\n",
      "        0.0233, 0.0238, 0.0238, 0.0233, 0.0224, 0.0285, 0.0305, 0.0221, 0.0271,\n",
      "        0.0301, 0.0241, 0.0346, 0.0266, 0.0264, 0.0187, 0.0180, 0.0172, 0.0202,\n",
      "        0.0226, 0.0310, 0.0217, 0.0276, 0.0249, 0.0196, 0.0239, 0.0256, 0.0259,\n",
      "        0.0235]), pred_classes: tensor([ 2,  4,  1,  4,  6,  7,  8,  9,  4,  7,  8,  9,  5,  4,  8,  4,  8,  5,\n",
      "         6,  4,  8,  9,  2,  4,  8,  4,  6,  7,  8,  9,  2,  4,  1,  6,  7,  4,\n",
      "         8,  1,  2,  4,  7, 12,  4,  8,  1,  3,  4,  5,  6,  7,  8,  9,  4,  6,\n",
      "         8,  9,  8,  4,  8,  9,  5,  5,  6,  8,  9,  5,  6,  8,  4,  8,  4,  8,\n",
      "         4,  5,  6,  7,  2,  7,  9, 11,  4,  6,  7,  8,  9,  4,  5,  6,  8,  3,\n",
      "         4,  6,  7,  8,  9,  5,  5,  2,  4,  7])])\n",
      "\u001b[32m[07/17 14:51:04 d2.engine.defaults]: \u001b[0mEvaluation results for coco_2017_val in csv format:\n",
      "\u001b[32m[07/17 14:51:04 d2.evaluation.testing]: \u001b[0mcopypaste: IoU_0.5=0.0\n",
      "\u001b[32m[07/17 14:51:04 d2.evaluation.testing]: \u001b[0mcopypaste: IoU_0.75=0.0\n",
      "\u001b[32m[07/17 14:51:04 d2.utils.events]: \u001b[0m eta: 6 days, 21:27:20  iter: 199  total_loss: 87.98  loss_ce: 3.438  loss_mask: 1.925  loss_dice: 3.052  loss_ce_0: 8.594  loss_mask_0: 1.874  loss_dice_0: 3.178  loss_ce_1: 3.356  loss_mask_1: 1.898  loss_dice_1: 2.983  loss_ce_2: 3.375  loss_mask_2: 1.937  loss_dice_2: 2.938  loss_ce_3: 3.399  loss_mask_3: 1.924  loss_dice_3: 2.909  loss_ce_4: 3.414  loss_mask_4: 1.951  loss_dice_4: 2.893  loss_ce_5: 3.462  loss_mask_5: 1.939  loss_dice_5: 2.921  loss_ce_6: 3.445  loss_mask_6: 1.973  loss_dice_6: 2.935  loss_ce_7: 3.428  loss_mask_7: 1.943  loss_dice_7: 2.988  loss_ce_8: 3.412  loss_mask_8: 1.974  loss_dice_8: 2.97    time: 1.6172  last_time: 1.6092  data_time: 0.1184  last_data_time: 0.1057   lr: 1e-05  max_mem: 39902M\n",
      "\u001b[32m[07/17 14:51:37 d2.utils.events]: \u001b[0m eta: 6 days, 21:24:57  iter: 219  total_loss: 88.82  loss_ce: 3.268  loss_mask: 2.001  loss_dice: 3.15  loss_ce_0: 8.574  loss_mask_0: 1.841  loss_dice_0: 3.208  loss_ce_1: 3.271  loss_mask_1: 1.856  loss_dice_1: 2.992  loss_ce_2: 3.352  loss_mask_2: 1.955  loss_dice_2: 3.004  loss_ce_3: 3.289  loss_mask_3: 1.901  loss_dice_3: 2.946  loss_ce_4: 3.309  loss_mask_4: 1.896  loss_dice_4: 2.922  loss_ce_5: 3.363  loss_mask_5: 1.91  loss_dice_5: 2.934  loss_ce_6: 3.307  loss_mask_6: 1.949  loss_dice_6: 2.899  loss_ce_7: 3.292  loss_mask_7: 1.996  loss_dice_7: 2.986  loss_ce_8: 3.305  loss_mask_8: 1.972  loss_dice_8: 3.004    time: 1.6166  last_time: 1.5952  data_time: 0.1208  last_data_time: 0.1159   lr: 1e-05  max_mem: 39902M\n",
      "\u001b[32m[07/17 14:52:09 d2.utils.events]: \u001b[0m eta: 6 days, 21:16:09  iter: 239  total_loss: 87.56  loss_ce: 3.418  loss_mask: 1.906  loss_dice: 3.039  loss_ce_0: 8.522  loss_mask_0: 1.809  loss_dice_0: 3.178  loss_ce_1: 3.354  loss_mask_1: 1.777  loss_dice_1: 3.004  loss_ce_2: 3.439  loss_mask_2: 1.818  loss_dice_2: 2.951  loss_ce_3: 3.464  loss_mask_3: 1.825  loss_dice_3: 2.835  loss_ce_4: 3.451  loss_mask_4: 1.863  loss_dice_4: 2.896  loss_ce_5: 3.499  loss_mask_5: 1.879  loss_dice_5: 2.874  loss_ce_6: 3.511  loss_mask_6: 1.827  loss_dice_6: 2.953  loss_ce_7: 3.522  loss_mask_7: 1.887  loss_dice_7: 2.989  loss_ce_8: 3.471  loss_mask_8: 1.931  loss_dice_8: 2.971    time: 1.6163  last_time: 1.6344  data_time: 0.1216  last_data_time: 0.1201   lr: 1e-05  max_mem: 39902M\n",
      "\u001b[32m[07/17 14:52:41 d2.utils.events]: \u001b[0m eta: 6 days, 21:09:40  iter: 259  total_loss: 87.2  loss_ce: 3.262  loss_mask: 1.938  loss_dice: 2.97  loss_ce_0: 8.464  loss_mask_0: 1.876  loss_dice_0: 3.083  loss_ce_1: 3.24  loss_mask_1: 1.929  loss_dice_1: 2.97  loss_ce_2: 3.261  loss_mask_2: 1.885  loss_dice_2: 2.877  loss_ce_3: 3.311  loss_mask_3: 1.914  loss_dice_3: 2.797  loss_ce_4: 3.275  loss_mask_4: 1.945  loss_dice_4: 2.851  loss_ce_5: 3.246  loss_mask_5: 2.008  loss_dice_5: 2.833  loss_ce_6: 3.261  loss_mask_6: 1.945  loss_dice_6: 2.795  loss_ce_7: 3.319  loss_mask_7: 1.899  loss_dice_7: 2.907  loss_ce_8: 3.257  loss_mask_8: 1.917  loss_dice_8: 2.879    time: 1.6153  last_time: 1.6253  data_time: 0.1176  last_data_time: 0.1252   lr: 1e-05  max_mem: 39902M\n",
      "\u001b[32m[07/17 14:53:13 d2.utils.events]: \u001b[0m eta: 6 days, 21:09:07  iter: 279  total_loss: 86.59  loss_ce: 3.229  loss_mask: 1.987  loss_dice: 2.944  loss_ce_0: 8.466  loss_mask_0: 1.894  loss_dice_0: 3.09  loss_ce_1: 3.271  loss_mask_1: 1.938  loss_dice_1: 2.867  loss_ce_2: 3.275  loss_mask_2: 1.94  loss_dice_2: 2.812  loss_ce_3: 3.293  loss_mask_3: 2.017  loss_dice_3: 2.824  loss_ce_4: 3.287  loss_mask_4: 2.013  loss_dice_4: 2.832  loss_ce_5: 3.264  loss_mask_5: 1.951  loss_dice_5: 2.778  loss_ce_6: 3.269  loss_mask_6: 1.956  loss_dice_6: 2.782  loss_ce_7: 3.251  loss_mask_7: 1.983  loss_dice_7: 2.822  loss_ce_8: 3.277  loss_mask_8: 2.059  loss_dice_8: 2.831    time: 1.6155  last_time: 1.6631  data_time: 0.1185  last_data_time: 0.1378   lr: 1e-05  max_mem: 39902M\n",
      "\u001b[32m[07/17 14:53:46 d2.utils.events]: \u001b[0m eta: 6 days, 21:13:23  iter: 299  total_loss: 87.01  loss_ce: 3.463  loss_mask: 1.953  loss_dice: 2.894  loss_ce_0: 8.39  loss_mask_0: 1.84  loss_dice_0: 3.108  loss_ce_1: 3.356  loss_mask_1: 1.896  loss_dice_1: 2.868  loss_ce_2: 3.381  loss_mask_2: 1.9  loss_dice_2: 2.833  loss_ce_3: 3.432  loss_mask_3: 1.883  loss_dice_3: 2.764  loss_ce_4: 3.478  loss_mask_4: 1.89  loss_dice_4: 2.812  loss_ce_5: 3.372  loss_mask_5: 1.919  loss_dice_5: 2.852  loss_ce_6: 3.435  loss_mask_6: 1.862  loss_dice_6: 2.806  loss_ce_7: 3.422  loss_mask_7: 1.887  loss_dice_7: 2.831  loss_ce_8: 3.481  loss_mask_8: 1.918  loss_dice_8: 2.895    time: 1.6165  last_time: 1.6471  data_time: 0.1237  last_data_time: 0.1224   lr: 1e-05  max_mem: 39902M\n",
      "\u001b[32m[07/17 14:54:18 d2.utils.events]: \u001b[0m eta: 6 days, 21:21:28  iter: 319  total_loss: 85.61  loss_ce: 3.409  loss_mask: 1.982  loss_dice: 2.921  loss_ce_0: 8.332  loss_mask_0: 1.9  loss_dice_0: 3.052  loss_ce_1: 3.284  loss_mask_1: 1.869  loss_dice_1: 2.834  loss_ce_2: 3.346  loss_mask_2: 1.873  loss_dice_2: 2.751  loss_ce_3: 3.329  loss_mask_3: 1.914  loss_dice_3: 2.832  loss_ce_4: 3.394  loss_mask_4: 1.894  loss_dice_4: 2.777  loss_ce_5: 3.441  loss_mask_5: 1.837  loss_dice_5: 2.733  loss_ce_6: 3.361  loss_mask_6: 1.927  loss_dice_6: 2.823  loss_ce_7: 3.393  loss_mask_7: 1.969  loss_dice_7: 2.807  loss_ce_8: 3.465  loss_mask_8: 1.933  loss_dice_8: 2.838    time: 1.6171  last_time: 1.6229  data_time: 0.1205  last_data_time: 0.1096   lr: 1e-05  max_mem: 39902M\n",
      "\u001b[32m[07/17 14:54:51 d2.utils.events]: \u001b[0m eta: 6 days, 21:20:56  iter: 339  total_loss: 85.58  loss_ce: 3.289  loss_mask: 1.896  loss_dice: 2.778  loss_ce_0: 8.284  loss_mask_0: 1.89  loss_dice_0: 3.066  loss_ce_1: 3.264  loss_mask_1: 1.933  loss_dice_1: 2.852  loss_ce_2: 3.264  loss_mask_2: 1.836  loss_dice_2: 2.727  loss_ce_3: 3.272  loss_mask_3: 1.92  loss_dice_3: 2.694  loss_ce_4: 3.265  loss_mask_4: 1.924  loss_dice_4: 2.761  loss_ce_5: 3.331  loss_mask_5: 1.929  loss_dice_5: 2.735  loss_ce_6: 3.31  loss_mask_6: 1.943  loss_dice_6: 2.723  loss_ce_7: 3.289  loss_mask_7: 1.962  loss_dice_7: 2.777  loss_ce_8: 3.27  loss_mask_8: 1.971  loss_dice_8: 2.822    time: 1.6173  last_time: 1.6229  data_time: 0.1188  last_data_time: 0.1350   lr: 1e-05  max_mem: 39902M\n",
      "\u001b[32m[07/17 14:55:23 d2.utils.events]: \u001b[0m eta: 6 days, 21:21:29  iter: 359  total_loss: 84.91  loss_ce: 3.273  loss_mask: 1.862  loss_dice: 2.885  loss_ce_0: 8.174  loss_mask_0: 1.858  loss_dice_0: 3.026  loss_ce_1: 3.25  loss_mask_1: 1.894  loss_dice_1: 2.814  loss_ce_2: 3.388  loss_mask_2: 1.834  loss_dice_2: 2.756  loss_ce_3: 3.284  loss_mask_3: 1.856  loss_dice_3: 2.751  loss_ce_4: 3.328  loss_mask_4: 1.887  loss_dice_4: 2.769  loss_ce_5: 3.239  loss_mask_5: 1.918  loss_dice_5: 2.785  loss_ce_6: 3.264  loss_mask_6: 1.875  loss_dice_6: 2.769  loss_ce_7: 3.23  loss_mask_7: 1.986  loss_dice_7: 2.837  loss_ce_8: 3.287  loss_mask_8: 1.877  loss_dice_8: 2.828    time: 1.6175  last_time: 1.6295  data_time: 0.1156  last_data_time: 0.1174   lr: 1e-05  max_mem: 39902M\n",
      "\u001b[32m[07/17 14:55:56 d2.utils.events]: \u001b[0m eta: 6 days, 21:19:07  iter: 379  total_loss: 83.71  loss_ce: 3.311  loss_mask: 1.881  loss_dice: 2.734  loss_ce_0: 8.194  loss_mask_0: 1.836  loss_dice_0: 2.973  loss_ce_1: 3.227  loss_mask_1: 1.905  loss_dice_1: 2.747  loss_ce_2: 3.275  loss_mask_2: 1.877  loss_dice_2: 2.767  loss_ce_3: 3.357  loss_mask_3: 1.88  loss_dice_3: 2.649  loss_ce_4: 3.315  loss_mask_4: 1.853  loss_dice_4: 2.634  loss_ce_5: 3.321  loss_mask_5: 1.897  loss_dice_5: 2.617  loss_ce_6: 3.323  loss_mask_6: 1.911  loss_dice_6: 2.673  loss_ce_7: 3.273  loss_mask_7: 1.897  loss_dice_7: 2.707  loss_ce_8: 3.289  loss_mask_8: 1.866  loss_dice_8: 2.695    time: 1.6171  last_time: 1.6296  data_time: 0.1186  last_data_time: 0.1221   lr: 1e-05  max_mem: 39902M\n",
      "\u001b[32m[07/17 14:56:28 d2.evaluation.evaluator]: \u001b[0mStart inference on 69 batches\n",
      "\u001b[32m[07/17 14:56:32 d2.evaluation.evaluator]: \u001b[0mInference done 11/69. Dataloading: 0.0076 s/iter. Inference: 0.2082 s/iter. Eval: 0.0348 s/iter. Total: 0.2506 s/iter. ETA=0:00:14\n",
      "\u001b[32m[07/17 14:56:37 d2.evaluation.evaluator]: \u001b[0mInference done 32/69. Dataloading: 0.0070 s/iter. Inference: 0.2007 s/iter. Eval: 0.0371 s/iter. Total: 0.2450 s/iter. ETA=0:00:09\n",
      "\u001b[32m[07/17 14:56:42 d2.evaluation.evaluator]: \u001b[0mInference done 52/69. Dataloading: 0.0074 s/iter. Inference: 0.2027 s/iter. Eval: 0.0372 s/iter. Total: 0.2474 s/iter. ETA=0:00:04\n",
      "\u001b[32m[07/17 14:56:46 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:15.592114 (0.243627 s / iter per device, on 1 devices)\n",
      "\u001b[32m[07/17 14:56:46 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:12 (0.198618 s / iter per device, on 1 devices)\n",
      "Computing IoU\n",
      "Computing IoU\n",
      "{'IoU_0.5': 0.0, 'IoU_0.75': 0.0}\n",
      "First Target: Instances(num_instances=28, image_height=800, image_width=800, fields=[gt_boxes: Boxes(tensor([[  7., 661.,  33., 695.],\n",
      "        [  0., 678.,  12., 710.],\n",
      "        [  0., 720.,  21., 732.],\n",
      "        [  0., 726.,  12., 754.],\n",
      "        [855., 188., 872., 223.],\n",
      "        [865., 222., 884., 231.],\n",
      "        [853., 227., 872., 248.],\n",
      "        [839., 223., 857., 229.],\n",
      "        [816., 750., 876., 779.],\n",
      "        [777., 747., 795., 772.],\n",
      "        [793., 740., 819., 778.],\n",
      "        [799., 779., 827., 803.],\n",
      "        [767., 784., 801., 822.],\n",
      "        [793., 773., 804., 784.],\n",
      "        [641., 753., 653., 759.],\n",
      "        [621., 742., 648., 760.],\n",
      "        [600., 707., 632., 747.],\n",
      "        [600., 748., 616., 758.],\n",
      "        [611., 757., 637., 794.],\n",
      "        [420., 725., 467., 755.],\n",
      "        [419., 761., 465., 810.],\n",
      "        [413., 749., 426., 761.],\n",
      "        [391., 679., 425., 739.],\n",
      "        [362., 740., 411., 772.],\n",
      "        [198., 728., 241., 755.],\n",
      "        [188., 714., 207., 735.],\n",
      "        [182., 737., 196., 763.],\n",
      "        [151., 724., 185., 747.]])), gt_classes: tensor([ 25,  26,  27,  28, 139, 140, 141, 142, 165, 166, 167, 168, 169, 170,\n",
      "        171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184]), gt_masks: tensor([[[False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         ...,\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False]],\n",
      "\n",
      "        [[False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         ...,\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False]],\n",
      "\n",
      "        [[False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         ...,\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         ...,\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False]],\n",
      "\n",
      "        [[False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         ...,\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False]],\n",
      "\n",
      "        [[False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         ...,\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False]]])])\n",
      "First Prediction Instances(num_instances=100, image_height=800, image_width=800, fields=[pred_masks: tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]]), pred_boxes: Boxes(tensor([[0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.]])), scores: tensor([0.0259, 0.0251, 0.0252, 0.0241, 0.0264, 0.0251, 0.0340, 0.0335, 0.0256,\n",
      "        0.0235, 0.0259, 0.0325, 0.0256, 0.0260, 0.0286, 0.0297, 0.0274, 0.0243,\n",
      "        0.0257, 0.0283, 0.0283, 0.0278, 0.0274, 0.0260, 0.0299, 0.0300, 0.0308,\n",
      "        0.0327, 0.0303, 0.0282, 0.0337, 0.0253, 0.0294, 0.0291, 0.0287, 0.0274,\n",
      "        0.0299, 0.0348, 0.0369, 0.0261, 0.0214, 0.0221, 0.0214, 0.0277, 0.0239,\n",
      "        0.0258, 0.0252, 0.0212, 0.0254, 0.0306, 0.0243, 0.0272, 0.0268, 0.0254,\n",
      "        0.0223, 0.0203, 0.0209, 0.0246, 0.0257, 0.0273, 0.0293, 0.0304, 0.0264,\n",
      "        0.0273, 0.0278, 0.0376, 0.0374, 0.0281, 0.0292, 0.0295, 0.0295, 0.0351,\n",
      "        0.0386, 0.0288, 0.0273, 0.0260, 0.0292, 0.0318, 0.0272, 0.0390, 0.0297,\n",
      "        0.0303, 0.0244, 0.0170, 0.0216, 0.0220, 0.0234, 0.0238, 0.0229, 0.0272,\n",
      "        0.0247, 0.0264, 0.0326, 0.0264, 0.0253, 0.0240, 0.0228, 0.0264, 0.0214,\n",
      "        0.0192]), pred_classes: tensor([ 2,  7,  8,  9,  2,  6,  7,  9, 10, 12,  1,  2,  5,  6,  7,  9, 10, 11,\n",
      "         8,  2,  6,  2,  7,  8,  2,  6,  7,  8,  9,  7,  9, 10,  2,  4,  5,  6,\n",
      "         7,  8,  9, 10,  2,  4,  5,  6,  7,  8,  9, 13,  1,  2,  7,  9, 10, 11,\n",
      "         2,  7,  9,  2,  4,  6,  7,  9,  1,  2,  6,  7,  9, 10, 12,  6,  7,  8,\n",
      "         9,  2,  4,  5,  6,  7,  8,  9, 10, 12, 13,  2,  2,  4,  6,  8,  9,  2,\n",
      "         5,  6,  8,  9,  9,  2,  7,  9, 10,  6])])\n",
      "\u001b[32m[07/17 14:56:46 d2.engine.defaults]: \u001b[0mEvaluation results for coco_2017_val in csv format:\n",
      "\u001b[32m[07/17 14:56:46 d2.evaluation.testing]: \u001b[0mcopypaste: IoU_0.5=0.0\n",
      "\u001b[32m[07/17 14:56:46 d2.evaluation.testing]: \u001b[0mcopypaste: IoU_0.75=0.0\n",
      "\u001b[32m[07/17 14:56:46 d2.utils.events]: \u001b[0m eta: 6 days, 21:12:43  iter: 399  total_loss: 83.95  loss_ce: 3.214  loss_mask: 1.941  loss_dice: 2.799  loss_ce_0: 8.092  loss_mask_0: 1.867  loss_dice_0: 3.035  loss_ce_1: 3.166  loss_mask_1: 1.841  loss_dice_1: 2.852  loss_ce_2: 3.173  loss_mask_2: 1.856  loss_dice_2: 2.73  loss_ce_3: 3.21  loss_mask_3: 1.874  loss_dice_3: 2.736  loss_ce_4: 3.23  loss_mask_4: 1.852  loss_dice_4: 2.798  loss_ce_5: 3.224  loss_mask_5: 1.929  loss_dice_5: 2.771  loss_ce_6: 3.26  loss_mask_6: 1.963  loss_dice_6: 2.801  loss_ce_7: 3.203  loss_mask_7: 1.958  loss_dice_7: 2.858  loss_ce_8: 3.19  loss_mask_8: 1.994  loss_dice_8: 2.89    time: 1.6166  last_time: 1.5981  data_time: 0.1231  last_data_time: 0.1085   lr: 1e-05  max_mem: 39902M\n",
      "\u001b[32m[07/17 14:57:18 d2.utils.events]: \u001b[0m eta: 6 days, 21:11:19  iter: 419  total_loss: 83.03  loss_ce: 3.228  loss_mask: 1.924  loss_dice: 2.746  loss_ce_0: 8.041  loss_mask_0: 1.823  loss_dice_0: 2.97  loss_ce_1: 3.182  loss_mask_1: 1.808  loss_dice_1: 2.752  loss_ce_2: 3.281  loss_mask_2: 1.793  loss_dice_2: 2.65  loss_ce_3: 3.298  loss_mask_3: 1.802  loss_dice_3: 2.702  loss_ce_4: 3.294  loss_mask_4: 1.793  loss_dice_4: 2.679  loss_ce_5: 3.249  loss_mask_5: 1.768  loss_dice_5: 2.686  loss_ce_6: 3.255  loss_mask_6: 1.825  loss_dice_6: 2.685  loss_ce_7: 3.258  loss_mask_7: 1.836  loss_dice_7: 2.673  loss_ce_8: 3.244  loss_mask_8: 1.927  loss_dice_8: 2.752    time: 1.6165  last_time: 1.5882  data_time: 0.1180  last_data_time: 0.1139   lr: 1e-05  max_mem: 39902M\n",
      "\u001b[32m[07/17 14:57:50 d2.utils.events]: \u001b[0m eta: 6 days, 21:08:50  iter: 439  total_loss: 82.89  loss_ce: 3.285  loss_mask: 1.835  loss_dice: 2.75  loss_ce_0: 7.978  loss_mask_0: 1.833  loss_dice_0: 2.918  loss_ce_1: 3.144  loss_mask_1: 1.827  loss_dice_1: 2.729  loss_ce_2: 3.163  loss_mask_2: 1.839  loss_dice_2: 2.646  loss_ce_3: 3.205  loss_mask_3: 1.878  loss_dice_3: 2.68  loss_ce_4: 3.207  loss_mask_4: 1.813  loss_dice_4: 2.648  loss_ce_5: 3.199  loss_mask_5: 1.823  loss_dice_5: 2.648  loss_ce_6: 3.24  loss_mask_6: 1.839  loss_dice_6: 2.726  loss_ce_7: 3.22  loss_mask_7: 1.846  loss_dice_7: 2.71  loss_ce_8: 3.28  loss_mask_8: 1.906  loss_dice_8: 2.664    time: 1.6159  last_time: 1.5968  data_time: 0.1153  last_data_time: 0.1180   lr: 1e-05  max_mem: 39902M\n",
      "\u001b[32m[07/17 14:58:23 d2.utils.events]: \u001b[0m eta: 6 days, 21:06:08  iter: 459  total_loss: 84.83  loss_ce: 3.354  loss_mask: 1.886  loss_dice: 2.748  loss_ce_0: 7.969  loss_mask_0: 1.824  loss_dice_0: 3.023  loss_ce_1: 3.331  loss_mask_1: 1.768  loss_dice_1: 2.797  loss_ce_2: 3.356  loss_mask_2: 1.798  loss_dice_2: 2.716  loss_ce_3: 3.422  loss_mask_3: 1.838  loss_dice_3: 2.708  loss_ce_4: 3.431  loss_mask_4: 1.839  loss_dice_4: 2.718  loss_ce_5: 3.431  loss_mask_5: 1.84  loss_dice_5: 2.715  loss_ce_6: 3.433  loss_mask_6: 1.833  loss_dice_6: 2.736  loss_ce_7: 3.44  loss_mask_7: 1.825  loss_dice_7: 2.713  loss_ce_8: 3.393  loss_mask_8: 1.859  loss_dice_8: 2.738    time: 1.6156  last_time: 1.5717  data_time: 0.1226  last_data_time: 0.1021   lr: 1e-05  max_mem: 39902M\n",
      "\u001b[32m[07/17 14:58:55 d2.utils.events]: \u001b[0m eta: 6 days, 21:07:45  iter: 479  total_loss: 83.35  loss_ce: 3.309  loss_mask: 1.894  loss_dice: 2.759  loss_ce_0: 7.882  loss_mask_0: 1.849  loss_dice_0: 2.931  loss_ce_1: 3.232  loss_mask_1: 1.832  loss_dice_1: 2.729  loss_ce_2: 3.319  loss_mask_2: 1.828  loss_dice_2: 2.683  loss_ce_3: 3.32  loss_mask_3: 1.797  loss_dice_3: 2.669  loss_ce_4: 3.358  loss_mask_4: 1.796  loss_dice_4: 2.683  loss_ce_5: 3.328  loss_mask_5: 1.787  loss_dice_5: 2.728  loss_ce_6: 3.383  loss_mask_6: 1.782  loss_dice_6: 2.666  loss_ce_7: 3.339  loss_mask_7: 1.821  loss_dice_7: 2.708  loss_ce_8: 3.299  loss_mask_8: 1.933  loss_dice_8: 2.758    time: 1.6160  last_time: 1.5908  data_time: 0.1247  last_data_time: 0.1191   lr: 1e-05  max_mem: 39902M\n",
      "\u001b[32m[07/17 14:59:27 d2.utils.events]: \u001b[0m eta: 6 days, 21:05:04  iter: 499  total_loss: 84.29  loss_ce: 3.436  loss_mask: 1.767  loss_dice: 2.676  loss_ce_0: 7.831  loss_mask_0: 1.801  loss_dice_0: 2.875  loss_ce_1: 3.363  loss_mask_1: 1.771  loss_dice_1: 2.742  loss_ce_2: 3.366  loss_mask_2: 1.834  loss_dice_2: 2.666  loss_ce_3: 3.405  loss_mask_3: 1.784  loss_dice_3: 2.626  loss_ce_4: 3.468  loss_mask_4: 1.735  loss_dice_4: 2.55  loss_ce_5: 3.439  loss_mask_5: 1.766  loss_dice_5: 2.586  loss_ce_6: 3.443  loss_mask_6: 1.813  loss_dice_6: 2.583  loss_ce_7: 3.435  loss_mask_7: 1.757  loss_dice_7: 2.663  loss_ce_8: 3.419  loss_mask_8: 1.801  loss_dice_8: 2.687    time: 1.6159  last_time: 1.5961  data_time: 0.1196  last_data_time: 0.1070   lr: 1e-05  max_mem: 39902M\n",
      "\u001b[32m[07/17 15:00:00 d2.utils.events]: \u001b[0m eta: 6 days, 21:03:30  iter: 519  total_loss: 82.31  loss_ce: 3.245  loss_mask: 1.875  loss_dice: 2.793  loss_ce_0: 7.736  loss_mask_0: 1.787  loss_dice_0: 2.962  loss_ce_1: 3.119  loss_mask_1: 1.799  loss_dice_1: 2.785  loss_ce_2: 3.199  loss_mask_2: 1.787  loss_dice_2: 2.699  loss_ce_3: 3.278  loss_mask_3: 1.804  loss_dice_3: 2.711  loss_ce_4: 3.26  loss_mask_4: 1.81  loss_dice_4: 2.745  loss_ce_5: 3.292  loss_mask_5: 1.795  loss_dice_5: 2.738  loss_ce_6: 3.295  loss_mask_6: 1.781  loss_dice_6: 2.741  loss_ce_7: 3.27  loss_mask_7: 1.819  loss_dice_7: 2.756  loss_ce_8: 3.242  loss_mask_8: 1.835  loss_dice_8: 2.818    time: 1.6156  last_time: 1.6130  data_time: 0.1232  last_data_time: 0.1091   lr: 1e-05  max_mem: 39902M\n",
      "\u001b[32m[07/17 15:00:32 d2.utils.events]: \u001b[0m eta: 6 days, 21:02:58  iter: 539  total_loss: 81.9  loss_ce: 3.281  loss_mask: 1.864  loss_dice: 2.712  loss_ce_0: 7.673  loss_mask_0: 1.797  loss_dice_0: 2.932  loss_ce_1: 3.234  loss_mask_1: 1.747  loss_dice_1: 2.773  loss_ce_2: 3.186  loss_mask_2: 1.878  loss_dice_2: 2.697  loss_ce_3: 3.205  loss_mask_3: 1.825  loss_dice_3: 2.635  loss_ce_4: 3.223  loss_mask_4: 1.812  loss_dice_4: 2.715  loss_ce_5: 3.223  loss_mask_5: 1.825  loss_dice_5: 2.679  loss_ce_6: 3.235  loss_mask_6: 1.851  loss_dice_6: 2.706  loss_ce_7: 3.194  loss_mask_7: 1.791  loss_dice_7: 2.69  loss_ce_8: 3.231  loss_mask_8: 1.816  loss_dice_8: 2.702    time: 1.6157  last_time: 1.6348  data_time: 0.1215  last_data_time: 0.1268   lr: 1e-05  max_mem: 39902M\n",
      "\u001b[32m[07/17 15:01:04 d2.utils.events]: \u001b[0m eta: 6 days, 21:03:07  iter: 559  total_loss: 82.8  loss_ce: 3.381  loss_mask: 1.807  loss_dice: 2.695  loss_ce_0: 7.656  loss_mask_0: 1.782  loss_dice_0: 2.865  loss_ce_1: 3.283  loss_mask_1: 1.791  loss_dice_1: 2.701  loss_ce_2: 3.377  loss_mask_2: 1.722  loss_dice_2: 2.552  loss_ce_3: 3.353  loss_mask_3: 1.781  loss_dice_3: 2.548  loss_ce_4: 3.367  loss_mask_4: 1.829  loss_dice_4: 2.55  loss_ce_5: 3.375  loss_mask_5: 1.77  loss_dice_5: 2.589  loss_ce_6: 3.383  loss_mask_6: 1.753  loss_dice_6: 2.641  loss_ce_7: 3.361  loss_mask_7: 1.757  loss_dice_7: 2.597  loss_ce_8: 3.375  loss_mask_8: 1.781  loss_dice_8: 2.607    time: 1.6157  last_time: 1.5914  data_time: 0.1216  last_data_time: 0.1057   lr: 1e-05  max_mem: 39902M\n",
      "\u001b[32m[07/17 15:01:37 d2.utils.events]: \u001b[0m eta: 6 days, 21:02:54  iter: 579  total_loss: 82.75  loss_ce: 3.468  loss_mask: 1.746  loss_dice: 2.692  loss_ce_0: 7.552  loss_mask_0: 1.832  loss_dice_0: 2.906  loss_ce_1: 3.358  loss_mask_1: 1.779  loss_dice_1: 2.705  loss_ce_2: 3.385  loss_mask_2: 1.778  loss_dice_2: 2.672  loss_ce_3: 3.462  loss_mask_3: 1.763  loss_dice_3: 2.582  loss_ce_4: 3.484  loss_mask_4: 1.757  loss_dice_4: 2.616  loss_ce_5: 3.487  loss_mask_5: 1.701  loss_dice_5: 2.634  loss_ce_6: 3.464  loss_mask_6: 1.731  loss_dice_6: 2.593  loss_ce_7: 3.455  loss_mask_7: 1.711  loss_dice_7: 2.614  loss_ce_8: 3.44  loss_mask_8: 1.803  loss_dice_8: 2.693    time: 1.6157  last_time: 1.6304  data_time: 0.1179  last_data_time: 0.1246   lr: 1e-05  max_mem: 39902M\n",
      "\u001b[32m[07/17 15:02:09 d2.evaluation.evaluator]: \u001b[0mStart inference on 69 batches\n",
      "\u001b[32m[07/17 15:02:13 d2.evaluation.evaluator]: \u001b[0mInference done 11/69. Dataloading: 0.0105 s/iter. Inference: 0.1947 s/iter. Eval: 0.0436 s/iter. Total: 0.2488 s/iter. ETA=0:00:14\n",
      "\u001b[32m[07/17 15:02:18 d2.evaluation.evaluator]: \u001b[0mInference done 32/69. Dataloading: 0.0080 s/iter. Inference: 0.1987 s/iter. Eval: 0.0381 s/iter. Total: 0.2449 s/iter. ETA=0:00:09\n",
      "\u001b[32m[07/17 15:02:23 d2.evaluation.evaluator]: \u001b[0mInference done 53/69. Dataloading: 0.0092 s/iter. Inference: 0.1962 s/iter. Eval: 0.0386 s/iter. Total: 0.2442 s/iter. ETA=0:00:03\n",
      "\u001b[32m[07/17 15:02:27 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:15.542847 (0.242857 s / iter per device, on 1 devices)\n",
      "\u001b[32m[07/17 15:02:27 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:12 (0.195815 s / iter per device, on 1 devices)\n",
      "Computing IoU\n",
      "Computing IoU\n",
      "{'IoU_0.5': 0.0, 'IoU_0.75': 0.0}\n",
      "First Target: Instances(num_instances=28, image_height=800, image_width=800, fields=[gt_boxes: Boxes(tensor([[  7., 661.,  33., 695.],\n",
      "        [  0., 678.,  12., 710.],\n",
      "        [  0., 720.,  21., 732.],\n",
      "        [  0., 726.,  12., 754.],\n",
      "        [855., 188., 872., 223.],\n",
      "        [865., 222., 884., 231.],\n",
      "        [853., 227., 872., 248.],\n",
      "        [839., 223., 857., 229.],\n",
      "        [816., 750., 876., 779.],\n",
      "        [777., 747., 795., 772.],\n",
      "        [793., 740., 819., 778.],\n",
      "        [799., 779., 827., 803.],\n",
      "        [767., 784., 801., 822.],\n",
      "        [793., 773., 804., 784.],\n",
      "        [641., 753., 653., 759.],\n",
      "        [621., 742., 648., 760.],\n",
      "        [600., 707., 632., 747.],\n",
      "        [600., 748., 616., 758.],\n",
      "        [611., 757., 637., 794.],\n",
      "        [420., 725., 467., 755.],\n",
      "        [419., 761., 465., 810.],\n",
      "        [413., 749., 426., 761.],\n",
      "        [391., 679., 425., 739.],\n",
      "        [362., 740., 411., 772.],\n",
      "        [198., 728., 241., 755.],\n",
      "        [188., 714., 207., 735.],\n",
      "        [182., 737., 196., 763.],\n",
      "        [151., 724., 185., 747.]])), gt_classes: tensor([ 25,  26,  27,  28, 139, 140, 141, 142, 165, 166, 167, 168, 169, 170,\n",
      "        171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184]), gt_masks: tensor([[[False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         ...,\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False]],\n",
      "\n",
      "        [[False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         ...,\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False]],\n",
      "\n",
      "        [[False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         ...,\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         ...,\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False]],\n",
      "\n",
      "        [[False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         ...,\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False]],\n",
      "\n",
      "        [[False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         ...,\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False]]])])\n",
      "First Prediction Instances(num_instances=100, image_height=800, image_width=800, fields=[pred_masks: tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 1., 1., 1.],\n",
      "         [0., 0., 0.,  ..., 1., 1., 1.],\n",
      "         [0., 0., 0.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 1., 1., 1.],\n",
      "         [0., 0., 0.,  ..., 1., 1., 1.],\n",
      "         [0., 0., 0.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]]), pred_boxes: Boxes(tensor([[0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.]])), scores: tensor([0.0289, 0.0248, 0.0392, 0.0304, 0.0323, 0.0283, 0.0279, 0.0261, 0.0261,\n",
      "        0.0370, 0.0278, 0.0336, 0.0321, 0.0284, 0.0361, 0.0363, 0.0295, 0.0276,\n",
      "        0.0257, 0.0395, 0.0270, 0.0337, 0.0303, 0.0297, 0.0191, 0.0191, 0.0237,\n",
      "        0.0257, 0.0250, 0.0235, 0.0235, 0.0357, 0.0294, 0.0338, 0.0289, 0.0300,\n",
      "        0.0231, 0.0345, 0.0260, 0.0292, 0.0299, 0.0302, 0.0241, 0.0307, 0.0276,\n",
      "        0.0314, 0.0307, 0.0436, 0.0351, 0.0184, 0.0261, 0.0338, 0.0338, 0.0255,\n",
      "        0.0000, 0.0320, 0.0283, 0.0237, 0.0264, 0.0276, 0.0315, 0.0241, 0.0242,\n",
      "        0.0250, 0.0288, 0.0272, 0.0301, 0.0360, 0.0326, 0.0277, 0.0330, 0.0273,\n",
      "        0.0274, 0.0290, 0.0305, 0.0448, 0.0423, 0.0292, 0.0272, 0.0281, 0.0171,\n",
      "        0.0179, 0.0205, 0.0241, 0.0241, 0.0230, 0.0276, 0.0248, 0.0265, 0.0259,\n",
      "        0.0303, 0.0346, 0.0347, 0.0288, 0.0337, 0.0307, 0.0288, 0.0386, 0.0311,\n",
      "        0.0235]), pred_classes: tensor([ 2,  5,  2,  3,  5,  6,  9,  2,  5,  2,  3,  5,  6,  7,  9, 10, 11,  2,\n",
      "         5,  2,  3,  5,  9, 10,  2,  5,  2,  5,  6,  9, 11,  2,  3,  5,  6,  9,\n",
      "         5,  2,  5,  7,  9, 10,  9,  2,  5,  6,  7,  9, 10,  2,  6,  9, 10, 11,\n",
      "         2,  2, 10,  5,  6,  7,  9,  2,  9, 10,  2,  6,  7,  9, 10, 12,  2,  5,\n",
      "         2,  6,  7,  9, 10, 11, 12,  2,  2,  5,  2,  2,  5,  2,  9, 10,  2, 10,\n",
      "         2,  5,  6,  7,  9, 10, 11,  2,  5,  7])])\n",
      "\u001b[32m[07/17 15:02:27 d2.engine.defaults]: \u001b[0mEvaluation results for coco_2017_val in csv format:\n",
      "\u001b[32m[07/17 15:02:27 d2.evaluation.testing]: \u001b[0mcopypaste: IoU_0.5=0.0\n",
      "\u001b[32m[07/17 15:02:27 d2.evaluation.testing]: \u001b[0mcopypaste: IoU_0.75=0.0\n",
      "\u001b[32m[07/17 15:02:27 d2.utils.events]: \u001b[0m eta: 6 days, 21:02:02  iter: 599  total_loss: 82.95  loss_ce: 3.32  loss_mask: 1.901  loss_dice: 2.658  loss_ce_0: 7.497  loss_mask_0: 1.844  loss_dice_0: 2.844  loss_ce_1: 3.245  loss_mask_1: 1.845  loss_dice_1: 2.673  loss_ce_2: 3.308  loss_mask_2: 1.867  loss_dice_2: 2.591  loss_ce_3: 3.324  loss_mask_3: 1.845  loss_dice_3: 2.611  loss_ce_4: 3.351  loss_mask_4: 1.89  loss_dice_4: 2.601  loss_ce_5: 3.365  loss_mask_5: 1.869  loss_dice_5: 2.562  loss_ce_6: 3.366  loss_mask_6: 1.867  loss_dice_6: 2.569  loss_ce_7: 3.341  loss_mask_7: 1.89  loss_dice_7: 2.602  loss_ce_8: 3.34  loss_mask_8: 1.908  loss_dice_8: 2.647    time: 1.6155  last_time: 1.5909  data_time: 0.1230  last_data_time: 0.1298   lr: 1e-05  max_mem: 39902M\n",
      "\u001b[32m[07/17 15:03:00 d2.utils.events]: \u001b[0m eta: 6 days, 21:03:27  iter: 619  total_loss: 82.67  loss_ce: 3.497  loss_mask: 1.865  loss_dice: 2.686  loss_ce_0: 7.469  loss_mask_0: 1.828  loss_dice_0: 2.81  loss_ce_1: 3.427  loss_mask_1: 1.796  loss_dice_1: 2.595  loss_ce_2: 3.505  loss_mask_2: 1.739  loss_dice_2: 2.528  loss_ce_3: 3.552  loss_mask_3: 1.744  loss_dice_3: 2.545  loss_ce_4: 3.576  loss_mask_4: 1.754  loss_dice_4: 2.522  loss_ce_5: 3.531  loss_mask_5: 1.779  loss_dice_5: 2.582  loss_ce_6: 3.568  loss_mask_6: 1.797  loss_dice_6: 2.533  loss_ce_7: 3.535  loss_mask_7: 1.81  loss_dice_7: 2.581  loss_ce_8: 3.521  loss_mask_8: 1.802  loss_dice_8: 2.591    time: 1.6160  last_time: 1.6012  data_time: 0.1262  last_data_time: 0.1067   lr: 1e-05  max_mem: 39902M\n"
     ]
    }
   ],
   "source": [
    "cfg = get_cfg()\n",
    "add_deeplab_config(cfg)\n",
    "mask2former.add_maskformer2_config(cfg)\n",
    "cfg.merge_from_file(CONFIG)\n",
    "\n",
    "launch(get_trainer, 1, args=(cfg,))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
