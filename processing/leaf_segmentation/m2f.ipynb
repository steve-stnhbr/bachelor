{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stefan.steinheber/.conda/envs/pytorch/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import lib.Mask2Former as m2f\n",
    "import lib.Mask2Former.mask2former as mask2former\n",
    "import os\n",
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from detectron2.engine import (launch)\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.projects.deeplab import add_deeplab_config\n",
    "from detectron2.data import build_detection_train_loader\n",
    "from lib.Mask2Former.train_net import Trainer\n",
    "import numpy as np\n",
    "from detectron2.structures import Boxes, Instances, BitMasks\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from detectron2.evaluation import DatasetEvaluator, DatasetEvaluators\n",
    "from detectron2.data import build_detection_test_loader\n",
    "from detectron2.utils import comm\n",
    "from detectron2.structures import BoxMode, pairwise_iou\n",
    "import copy\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATA_SOURCE = \"combined\"\n",
    "DATA_LOCATION = \"_data\"\n",
    "DATA_DIR = \"coco\"\n",
    "os.environ[\"DETECTRON2_DATASETS\"] = os.path.join(DATA_LOCATION, DATA_DIR)\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert the dataset to COCO format\n",
    "The following commands convert the existing PNG mask-based dataset to the coco annotations required for training Mask2Former"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "tags": []
   },
   "source": [
    "!cd {DATA_LOCATION} && python mask_to_coco.py --images {DATA_SOURCE}/val/images/ --masks {DATA_SOURCE}/val/leaf_instances/ --output {DATA_DIR}/annotations/instances_val2017.json --fixed-category-id 58 --fixed-category-name \"potted plant\" --default-categories\n",
    "!cd {DATA_LOCATION} && python mask_to_coco.py --images {DATA_SOURCE}/train/images/ --masks {DATA_SOURCE}/train/leaf_instances/ --output {DATA_DIR}/annotations/instances_train2017.json --fixed-category-id 58 --fixed-category-name \"potted plant\" --default-categories"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "tags": []
   },
   "source": [
    "!cd {DATA_LOCATION} && mkdir {DATA_DIR}/train2017\n",
    "!cd {DATA_LOCATION} && cp {DATA_SOURCE}/train/images/* {DATA_DIR}/train2017\n",
    "!cd {DATA_LOCATION} && mkdir {DATA_DIR}/val2017\n",
    "!cd {DATA_LOCATION} && cp {DATA_SOURCE}/val/images/* {DATA_DIR}/val2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "CONFIG = \"lib/Mask2Former/configs/coco/instance-segmentation/swin/maskformer2_swin_base_IN21k_384_bs16_50ep.yaml\"\n",
    "#CONFIG = \"configs/mask2former.yaml\"\n",
    "NUM_GPUS = 1\n",
    "BATCH_SIZE = 8\n",
    "LEARNING_RATE = 0.001\n",
    "DATASET_DIR = \"_data/urban_street_combined\"\n",
    "DATASET_DIR_VAL = \"_data/combined/val\"\n",
    "IMAGES_DIR_NAME = \"images\"\n",
    "IMAGE_DIR = os.path.join(DATASET_DIR, IMAGES_DIR_NAME)\n",
    "INSTANCES_DIR_NAME = \"leaf_instances\"\n",
    "INSTANCES_DIR = os.path.join(DATASET_DIR, INSTANCES_DIR_NAME)\n",
    "IMAGE_DIR_VAL = os.path.join(DATASET_DIR_VAL, IMAGES_DIR_NAME)\n",
    "INSTANCES_DIR_VAL = os.path.join(DATASET_DIR_VAL, INSTANCES_DIR_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LeavesDataset(Dataset):\n",
    "    def __init__(self, image_dir, label_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.transform = transform\n",
    "        self.image_files = os.listdir(image_dir)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_path = os.path.join(self.image_dir, self.image_files[index])\n",
    "        label_path = os.path.join(self.label_dir, self.image_files[index])\n",
    "\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        label = Image.open(label_path).convert(\"L\")\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            #label = self.transform(label).squeeze()\n",
    "\n",
    "        # Convert label to tensor\n",
    "        label = torch.from_numpy(np.array(label))\n",
    "\n",
    "        # Create instances\n",
    "        instances = Instances(image.shape[1:])\n",
    "\n",
    "        # Create gt_boxes\n",
    "        boxes = []\n",
    "        gt_classes = []\n",
    "        gt_masks = []\n",
    "        unique_labels = torch.unique(label)\n",
    "        if len(unique_labels) > 1:\n",
    "            if 255 in unique_labels: \n",
    "                print(\"Invalid label in file\", image_path)\n",
    "            for obj_class in unique_labels:\n",
    "                if obj_class > 0:\n",
    "                    mask = label == obj_class\n",
    "                    coords = torch.nonzero(mask)\n",
    "                    xmin, ymin = coords.min(dim=0).values\n",
    "                    xmax, ymax = coords.max(dim=0).values\n",
    "                    boxes.append([xmin, ymin, xmax, ymax])\n",
    "                    gt_classes.append(obj_class.item())\n",
    "                    gt_masks.append(mask)\n",
    "\n",
    "            instances.gt_boxes = Boxes(torch.tensor(boxes))\n",
    "            instances.gt_classes = torch.tensor(gt_classes, dtype=torch.long)\n",
    "\n",
    "            # Resize masks to match the image size\n",
    "            resized_masks = []\n",
    "            for mask in gt_masks:\n",
    "                resized_mask = F.interpolate(mask.unsqueeze(0).unsqueeze(0).float(), size=image.shape[1:], mode='nearest').squeeze().to(torch.bool)\n",
    "                resized_masks.append(resized_mask)\n",
    "\n",
    "            if len(resized_masks) > 0:\n",
    "                instances.gt_masks = torch.stack(resized_masks)\n",
    "            else:\n",
    "                print(\"Masks empty, class lenght is\", len(gt_classes))\n",
    "                instances.gt_masks = torch.Tensor()\n",
    "\n",
    "            return {\n",
    "                \"image\": image,\n",
    "                \"height\": image.shape[1],\n",
    "                \"width\": image.shape[2],\n",
    "                \"instances\": instances,\n",
    "            }\n",
    "        \n",
    "        return {\n",
    "            \"image\": image,\n",
    "            \"height\": image.shape[1],\n",
    "            \"width\": image.shape[2]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LeavesEvaluator(DatasetEvaluator):\n",
    "    def __init__(self, dataset_name):\n",
    "        self.dataset_name = dataset_name\n",
    "        self._cpu_device = torch.device(\"cpu\")\n",
    "\n",
    "    def reset(self):\n",
    "        self._predictions = []\n",
    "        self._targets = []\n",
    "\n",
    "    def process(self, inputs, outputs):\n",
    "        # sample single random instance\n",
    "#        idx = random.randrange(len(inputs))\n",
    "#        self._predictions.append(outputs[idx][\"instances\"].to(self._cpu_device))\n",
    "#        self._targets.append(inputs[idx][\"instances\"].to(self._cpu_device))\n",
    "        for input, output in zip(inputs, outputs):\n",
    "            self._predictions.append(output[\"instances\"].to(self._cpu_device))\n",
    "            self._targets.append(input[\"instances\"].to(self._cpu_device))\n",
    "\n",
    "    def evaluate(self):\n",
    "        if comm.is_main_process():\n",
    "            self._evaluate()\n",
    "\n",
    "        if comm.is_main_process():\n",
    "            return copy.deepcopy(self._results)\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def _evaluate(self):\n",
    "        self._results = {}\n",
    "        iou_thresholds = [0.5, 0.75]\n",
    "        for iou_threshold in iou_thresholds:\n",
    "            self._results[f\"IoU_{iou_threshold}\"] = self._compute_iou(iou_threshold)\n",
    "        #self._results[\"mask_mse_loss\"] = self._compute_mask_mse_loss()\n",
    "        print(self._results)\n",
    "\n",
    "    def _compute_iou(self, iou_threshold):\n",
    "        print(\"Computing IoU\")\n",
    "        num_instances = len(self._predictions)\n",
    "        iou_sum = 0.0\n",
    "\n",
    "        for pred, target in zip(self._predictions, self._targets):\n",
    "            pred_boxes = pred.pred_boxes.tensor\n",
    "            target_boxes = target.gt_boxes.tensor\n",
    "\n",
    "            if len(pred_boxes) == 0 or len(target_boxes) == 0:\n",
    "                continue\n",
    "\n",
    "            # Convert the boxes to the format expected by the pairwise_iou function\n",
    "            pred_boxes = BoxMode.convert(pred_boxes, BoxMode.XYXY_ABS, BoxMode.XYXY_ABS)\n",
    "            target_boxes = BoxMode.convert(target_boxes, BoxMode.XYXY_ABS, BoxMode.XYXY_ABS)\n",
    "\n",
    "            # Compute IoU between predicted and target boxes\n",
    "            iou_matrix = pairwise_iou(Boxes(pred_boxes), Boxes(target_boxes))\n",
    "            max_iou, _ = iou_matrix.max(dim=1)\n",
    "\n",
    "            # Count the number of predicted boxes with IoU above the threshold\n",
    "            num_above_threshold = (max_iou > iou_threshold).sum().item()\n",
    "            iou_sum += num_above_threshold\n",
    "\n",
    "        avg_iou = iou_sum / num_instances\n",
    "        return avg_iou\n",
    "    \n",
    "    def _compute_mask_mse_loss(self):\n",
    "        print(\"Computing Mask MSE Loss\")\n",
    "        loss = 0\n",
    "        for pred, target in zip(self._predictions, self._targets):\n",
    "            for pred_mask, target_mask in zip(pred.pred_masks, target.gt_masks):\n",
    "                print(\"Single MSE\")\n",
    "                target_mask = target_mask.float()\n",
    "                diff2 = (torch.flatten(pred_mask) - torch.flatten(target_mask)) ** 2.0\n",
    "                sum2 = 0.0\n",
    "                num = 0\n",
    "\n",
    "                flat_mask = torch.flatten(target_mask)\n",
    "                assert(len(flat_mask) == len(diff2))\n",
    "                for i in range(len(diff2)):\n",
    "                    if flat_mask[i] == 1:\n",
    "                        sum2 += diff2[i]\n",
    "                        num += 1\n",
    "\n",
    "                loss += sum2 / num\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    images = []\n",
    "    instances = []\n",
    "    extras = {}\n",
    "\n",
    "    for item in batch:\n",
    "        images.append(item[\"image\"])\n",
    "        \n",
    "        item_instances = item[\"instances\"]\n",
    "        item_instances[\"gt_boxes\"] = torch.tensor(item_instances[\"gt_boxes\"])\n",
    "        item_instances[\"gt_classes\"] = torch.tensor(item_instances[\"gt_classes\"], dtype=torch.long)\n",
    "        item_instances[\"gt_masks\"] = torch.tensor(item_instances[\"gt_masks\"])\n",
    "        instances.append(item_instances)\n",
    "        \n",
    "        extras[\"height\"] = item[\"height\"]\n",
    "        extras[\"width\"] = item[\"width\"]\n",
    "\n",
    "    batched_inputs = [\n",
    "        {\"image\": image, \"instances\": instance, **extras}\n",
    "        for image, instance in zip(images, instances)\n",
    "    ]\n",
    "\n",
    "    return batched_inputs\n",
    "\n",
    "class LeavesTrainer(Trainer):\n",
    "    @classmethod\n",
    "    def build_train_loader(cls, _):\n",
    "        # Define your data transforms\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((800, 800)),\n",
    "            transforms.ToTensor(),\n",
    "            #transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "\n",
    "        # Create the dataset\n",
    "        dataset = LeavesDataset(IMAGE_DIR, INSTANCES_DIR, transform=transform, )\n",
    "        \n",
    "        # Create the DataLoader\n",
    "        dataloader = build_detection_train_loader(dataset, mapper=None, total_batch_size=1)\n",
    "        return dataloader\n",
    "    \n",
    "    @classmethod\n",
    "    def build_test_loader(cls, cfg, dataset_name):\n",
    "        # Define your data transforms\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((800, 800)),\n",
    "            transforms.ToTensor(),\n",
    "            #transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "\n",
    "        # Create the dataset\n",
    "        dataset = LeavesDataset(IMAGE_DIR_VAL, INSTANCES_DIR_VAL, transform=transform, )\n",
    "        \n",
    "        # Create the DataLoader\n",
    "        dataloader = build_detection_test_loader(dataset, mapper=None)\n",
    "        return dataloader\n",
    "        \n",
    "    @classmethod\n",
    "    def build_evaluator(cls, cfg, dataset_name, output_folder=None):\n",
    "        return LeavesEvaluator(dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_trainer(cfg):\n",
    "    trainer = LeavesTrainer(cfg)\n",
    "    #trainer.resume_or_load(resume=args.resume)\n",
    "    return trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stefan.steinheber/.conda/envs/pytorch/lib/python3.12/site-packages/torch/functional.py:512: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /opt/conda/conda-bld/pytorch_1716905971214/work/aten/src/ATen/native/TensorShape.cpp:3587.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[07/17 13:17:57 d2.engine.defaults]: \u001b[0mModel:\n",
      "MaskFormer(\n",
      "  (backbone): D2SwinTransformer(\n",
      "    (patch_embed): PatchEmbed(\n",
      "      (proj): Conv2d(3, 128, kernel_size=(4, 4), stride=(4, 4))\n",
      "      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (pos_drop): Dropout(p=0.0, inplace=False)\n",
      "    (layers): ModuleList(\n",
      "      (0): BasicLayer(\n",
      "        (blocks): ModuleList(\n",
      "          (0): SwinTransformerBlock(\n",
      "            (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): Identity()\n",
      "            (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (1): SwinTransformerBlock(\n",
      "            (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.013)\n",
      "            (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (downsample): PatchMerging(\n",
      "          (reduction): Linear(in_features=512, out_features=256, bias=False)\n",
      "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicLayer(\n",
      "        (blocks): ModuleList(\n",
      "          (0): SwinTransformerBlock(\n",
      "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.026)\n",
      "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (1): SwinTransformerBlock(\n",
      "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.039)\n",
      "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (downsample): PatchMerging(\n",
      "          (reduction): Linear(in_features=1024, out_features=512, bias=False)\n",
      "          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (2): BasicLayer(\n",
      "        (blocks): ModuleList(\n",
      "          (0): SwinTransformerBlock(\n",
      "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.052)\n",
      "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (1): SwinTransformerBlock(\n",
      "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.065)\n",
      "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (2): SwinTransformerBlock(\n",
      "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.078)\n",
      "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (3): SwinTransformerBlock(\n",
      "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.091)\n",
      "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (4): SwinTransformerBlock(\n",
      "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.104)\n",
      "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (5): SwinTransformerBlock(\n",
      "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.117)\n",
      "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (6): SwinTransformerBlock(\n",
      "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.130)\n",
      "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (7): SwinTransformerBlock(\n",
      "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.143)\n",
      "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (8): SwinTransformerBlock(\n",
      "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.157)\n",
      "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (9): SwinTransformerBlock(\n",
      "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.170)\n",
      "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (10): SwinTransformerBlock(\n",
      "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.183)\n",
      "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (11): SwinTransformerBlock(\n",
      "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.196)\n",
      "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (12): SwinTransformerBlock(\n",
      "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.209)\n",
      "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (13): SwinTransformerBlock(\n",
      "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.222)\n",
      "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (14): SwinTransformerBlock(\n",
      "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.235)\n",
      "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (15): SwinTransformerBlock(\n",
      "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.248)\n",
      "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (16): SwinTransformerBlock(\n",
      "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.261)\n",
      "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (17): SwinTransformerBlock(\n",
      "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.274)\n",
      "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (downsample): PatchMerging(\n",
      "          (reduction): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "          (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (3): BasicLayer(\n",
      "        (blocks): ModuleList(\n",
      "          (0): SwinTransformerBlock(\n",
      "            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.287)\n",
      "            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (1): SwinTransformerBlock(\n",
      "            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.300)\n",
      "            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (norm0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    (norm3): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (sem_seg_head): MaskFormerHead(\n",
      "    (pixel_decoder): MSDeformAttnPixelDecoder(\n",
      "      (input_proj): ModuleList(\n",
      "        (0): Sequential(\n",
      "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "        )\n",
      "        (1): Sequential(\n",
      "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "        )\n",
      "        (2): Sequential(\n",
      "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "        )\n",
      "      )\n",
      "      (transformer): MSDeformAttnTransformerEncoderOnly(\n",
      "        (encoder): MSDeformAttnTransformerEncoder(\n",
      "          (layers): ModuleList(\n",
      "            (0-5): 6 x MSDeformAttnTransformerEncoderLayer(\n",
      "              (self_attn): MSDeformAttn(\n",
      "                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)\n",
      "                (attention_weights): Linear(in_features=256, out_features=96, bias=True)\n",
      "                (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "                (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "              )\n",
      "              (dropout1): Dropout(p=0.0, inplace=False)\n",
      "              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "              (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "              (dropout2): Dropout(p=0.0, inplace=False)\n",
      "              (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "              (dropout3): Dropout(p=0.0, inplace=False)\n",
      "              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (pe_layer): Positional encoding PositionEmbeddingSine\n",
      "          num_pos_feats: 128\n",
      "          temperature: 10000\n",
      "          normalize: True\n",
      "          scale: 6.283185307179586\n",
      "      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (adapter_1): Conv2d(\n",
      "        128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "      )\n",
      "      (layer_1): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "      )\n",
      "    )\n",
      "    (predictor): MultiScaleMaskedTransformerDecoder(\n",
      "      (pe_layer): Positional encoding PositionEmbeddingSine\n",
      "          num_pos_feats: 128\n",
      "          temperature: 10000\n",
      "          normalize: True\n",
      "          scale: 6.283185307179586\n",
      "      (transformer_self_attention_layers): ModuleList(\n",
      "        (0-8): 9 x SelfAttentionLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (transformer_cross_attention_layers): ModuleList(\n",
      "        (0-8): 9 x CrossAttentionLayer(\n",
      "          (multihead_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (transformer_ffn_layers): ModuleList(\n",
      "        (0-8): 9 x FFNLayer(\n",
      "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (query_feat): Embedding(100, 256)\n",
      "      (query_embed): Embedding(100, 256)\n",
      "      (level_embed): Embedding(3, 256)\n",
      "      (input_proj): ModuleList(\n",
      "        (0-2): 3 x Sequential()\n",
      "      )\n",
      "      (class_embed): Linear(in_features=256, out_features=81, bias=True)\n",
      "      (mask_embed): MLP(\n",
      "        (layers): ModuleList(\n",
      "          (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (criterion): Criterion SetCriterion\n",
      "      matcher: Matcher HungarianMatcher\n",
      "          cost_class: 2.0\n",
      "          cost_mask: 5.0\n",
      "          cost_dice: 5.0\n",
      "      losses: ['labels', 'masks']\n",
      "      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}\n",
      "      num_classes: 80\n",
      "      eos_coef: 0.1\n",
      "      num_points: 12544\n",
      "      oversample_ratio: 3.0\n",
      "      importance_sample_ratio: 0.75\n",
      ")\n",
      "relative_position_bias_table\n",
      "relative_position_bias_table\n",
      "relative_position_bias_table\n",
      "relative_position_bias_table\n",
      "relative_position_bias_table\n",
      "relative_position_bias_table\n",
      "relative_position_bias_table\n",
      "relative_position_bias_table\n",
      "relative_position_bias_table\n",
      "relative_position_bias_table\n",
      "relative_position_bias_table\n",
      "relative_position_bias_table\n",
      "relative_position_bias_table\n",
      "relative_position_bias_table\n",
      "relative_position_bias_table\n",
      "relative_position_bias_table\n",
      "relative_position_bias_table\n",
      "relative_position_bias_table\n",
      "relative_position_bias_table\n",
      "relative_position_bias_table\n",
      "relative_position_bias_table\n",
      "relative_position_bias_table\n",
      "relative_position_bias_table\n",
      "relative_position_bias_table\n",
      "\u001b[32m[07/17 13:17:57 d2.data.build]: \u001b[0mMaking batched data loader with batch_size=1\n",
      "\u001b[32m[07/17 13:17:57 d2.engine.train_loop]: \u001b[0mStarting training from iteration 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stefan.steinheber/.conda/envs/pytorch/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[07/17 13:18:07 d2.utils.events]: \u001b[0m eta: 1 day, 22:30:23  iter: 19  total_loss: 123.8  loss_ce: 5.645  loss_mask: 3.125  loss_dice: 4.55  loss_ce_0: 9.081  loss_mask_0: 2.052  loss_dice_0: 4.103  loss_ce_1: 6.476  loss_mask_1: 2.465  loss_dice_1: 4.167  loss_ce_2: 5.377  loss_mask_2: 2.474  loss_dice_2: 4.455  loss_ce_3: 5.071  loss_mask_3: 2.617  loss_dice_3: 4.43  loss_ce_4: 5.116  loss_mask_4: 2.751  loss_dice_4: 4.404  loss_ce_5: 4.81  loss_mask_5: 2.609  loss_dice_5: 4.458  loss_ce_6: 4.862  loss_mask_6: 3.194  loss_dice_6: 4.504  loss_ce_7: 4.975  loss_mask_7: 2.961  loss_dice_7: 4.411  loss_ce_8: 5.389  loss_mask_8: 2.617  loss_dice_8: 4.519    time: 0.4626  last_time: 0.4405  data_time: 0.0263  last_data_time: 0.0194   lr: 1e-05  max_mem: 6709M\n",
      "\u001b[32m[07/17 13:18:15 d2.utils.events]: \u001b[0m eta: 1 day, 21:09:39  iter: 39  total_loss: 106.9  loss_ce: 3.962  loss_mask: 1.695  loss_dice: 4.584  loss_ce_0: 8.836  loss_mask_0: 1.597  loss_dice_0: 4.377  loss_ce_1: 3.741  loss_mask_1: 1.454  loss_dice_1: 4.415  loss_ce_2: 3.952  loss_mask_2: 1.48  loss_dice_2: 4.475  loss_ce_3: 3.974  loss_mask_3: 1.457  loss_dice_3: 4.538  loss_ce_4: 3.989  loss_mask_4: 1.609  loss_dice_4: 4.609  loss_ce_5: 3.933  loss_mask_5: 1.609  loss_dice_5: 4.644  loss_ce_6: 3.951  loss_mask_6: 1.448  loss_dice_6: 4.665  loss_ce_7: 3.945  loss_mask_7: 1.494  loss_dice_7: 4.646  loss_ce_8: 4.037  loss_mask_8: 1.693  loss_dice_8: 4.606    time: 0.4514  last_time: 0.4336  data_time: 0.0234  last_data_time: 0.0202   lr: 1e-05  max_mem: 6709M\n",
      "\u001b[32m[07/17 13:18:24 d2.utils.events]: \u001b[0m eta: 1 day, 20:56:48  iter: 59  total_loss: 101.5  loss_ce: 3.21  loss_mask: 1.953  loss_dice: 4.498  loss_ce_0: 9.076  loss_mask_0: 1.722  loss_dice_0: 4.382  loss_ce_1: 3.026  loss_mask_1: 1.893  loss_dice_1: 4.419  loss_ce_2: 2.983  loss_mask_2: 1.818  loss_dice_2: 4.308  loss_ce_3: 2.945  loss_mask_3: 1.811  loss_dice_3: 4.403  loss_ce_4: 3.139  loss_mask_4: 1.934  loss_dice_4: 4.371  loss_ce_5: 3.186  loss_mask_5: 1.963  loss_dice_5: 4.428  loss_ce_6: 3.227  loss_mask_6: 2.044  loss_dice_6: 4.534  loss_ce_7: 3.256  loss_mask_7: 2.105  loss_dice_7: 4.441  loss_ce_8: 3.268  loss_mask_8: 2.144  loss_dice_8: 4.464    time: 0.4478  last_time: 0.4384  data_time: 0.0244  last_data_time: 0.0256   lr: 1e-05  max_mem: 6709M\n",
      "\u001b[32m[07/17 13:18:33 d2.utils.events]: \u001b[0m eta: 1 day, 20:55:50  iter: 79  total_loss: 101.6  loss_ce: 3.211  loss_mask: 1.832  loss_dice: 4.643  loss_ce_0: 8.942  loss_mask_0: 1.818  loss_dice_0: 4.338  loss_ce_1: 2.927  loss_mask_1: 1.74  loss_dice_1: 4.412  loss_ce_2: 2.772  loss_mask_2: 1.58  loss_dice_2: 4.444  loss_ce_3: 2.8  loss_mask_3: 1.632  loss_dice_3: 4.495  loss_ce_4: 2.91  loss_mask_4: 1.703  loss_dice_4: 4.471  loss_ce_5: 3.066  loss_mask_5: 1.704  loss_dice_5: 4.49  loss_ce_6: 3.216  loss_mask_6: 1.677  loss_dice_6: 4.558  loss_ce_7: 3.16  loss_mask_7: 1.868  loss_dice_7: 4.649  loss_ce_8: 3.217  loss_mask_8: 1.747  loss_dice_8: 4.595    time: 0.4452  last_time: 0.4312  data_time: 0.0228  last_data_time: 0.0196   lr: 1e-05  max_mem: 6709M\n",
      "\u001b[32m[07/17 13:18:42 d2.utils.events]: \u001b[0m eta: 1 day, 20:51:54  iter: 99  total_loss: 100.6  loss_ce: 3.586  loss_mask: 1.845  loss_dice: 4.582  loss_ce_0: 8.894  loss_mask_0: 1.515  loss_dice_0: 4.494  loss_ce_1: 3.297  loss_mask_1: 1.658  loss_dice_1: 4.389  loss_ce_2: 3.098  loss_mask_2: 1.629  loss_dice_2: 4.435  loss_ce_3: 3.248  loss_mask_3: 1.736  loss_dice_3: 4.423  loss_ce_4: 3.248  loss_mask_4: 1.584  loss_dice_4: 4.438  loss_ce_5: 3.263  loss_mask_5: 1.61  loss_dice_5: 4.472  loss_ce_6: 3.412  loss_mask_6: 1.704  loss_dice_6: 4.478  loss_ce_7: 3.54  loss_mask_7: 1.778  loss_dice_7: 4.568  loss_ce_8: 3.586  loss_mask_8: 1.798  loss_dice_8: 4.558    time: 0.4440  last_time: 0.4330  data_time: 0.0233  last_data_time: 0.0204   lr: 1e-05  max_mem: 6709M\n",
      "\u001b[32m[07/17 13:18:51 d2.utils.events]: \u001b[0m eta: 1 day, 20:50:03  iter: 119  total_loss: 97.54  loss_ce: 3.248  loss_mask: 1.432  loss_dice: 4.649  loss_ce_0: 8.801  loss_mask_0: 1.529  loss_dice_0: 4.442  loss_ce_1: 2.826  loss_mask_1: 1.438  loss_dice_1: 4.506  loss_ce_2: 2.755  loss_mask_2: 1.362  loss_dice_2: 4.562  loss_ce_3: 2.956  loss_mask_3: 1.513  loss_dice_3: 4.636  loss_ce_4: 2.814  loss_mask_4: 1.592  loss_dice_4: 4.532  loss_ce_5: 2.992  loss_mask_5: 1.606  loss_dice_5: 4.538  loss_ce_6: 3.146  loss_mask_6: 1.519  loss_dice_6: 4.532  loss_ce_7: 3.241  loss_mask_7: 1.409  loss_dice_7: 4.616  loss_ce_8: 3.244  loss_mask_8: 1.412  loss_dice_8: 4.68    time: 0.4430  last_time: 0.4383  data_time: 0.0221  last_data_time: 0.0215   lr: 1e-05  max_mem: 6709M\n",
      "\u001b[32m[07/17 13:18:59 d2.utils.events]: \u001b[0m eta: 1 day, 20:48:58  iter: 139  total_loss: 99.95  loss_ce: 3.572  loss_mask: 1.951  loss_dice: 4.632  loss_ce_0: 8.726  loss_mask_0: 1.775  loss_dice_0: 4.326  loss_ce_1: 3.341  loss_mask_1: 1.691  loss_dice_1: 4.398  loss_ce_2: 2.84  loss_mask_2: 1.621  loss_dice_2: 4.43  loss_ce_3: 3.11  loss_mask_3: 1.596  loss_dice_3: 4.396  loss_ce_4: 3.23  loss_mask_4: 1.731  loss_dice_4: 4.398  loss_ce_5: 3.343  loss_mask_5: 1.676  loss_dice_5: 4.501  loss_ce_6: 3.485  loss_mask_6: 1.491  loss_dice_6: 4.478  loss_ce_7: 3.457  loss_mask_7: 1.614  loss_dice_7: 4.51  loss_ce_8: 3.538  loss_mask_8: 1.804  loss_dice_8: 4.624    time: 0.4428  last_time: 0.4338  data_time: 0.0248  last_data_time: 0.0237   lr: 1e-05  max_mem: 6709M\n",
      "\u001b[32m[07/17 13:19:08 d2.utils.events]: \u001b[0m eta: 1 day, 20:50:12  iter: 159  total_loss: 100.9  loss_ce: 3.509  loss_mask: 1.615  loss_dice: 4.444  loss_ce_0: 8.769  loss_mask_0: 2.07  loss_dice_0: 4.09  loss_ce_1: 3.691  loss_mask_1: 1.973  loss_dice_1: 4.114  loss_ce_2: 3.329  loss_mask_2: 1.921  loss_dice_2: 4.157  loss_ce_3: 3.44  loss_mask_3: 1.859  loss_dice_3: 4.23  loss_ce_4: 3.43  loss_mask_4: 1.768  loss_dice_4: 4.22  loss_ce_5: 3.382  loss_mask_5: 1.787  loss_dice_5: 4.224  loss_ce_6: 3.459  loss_mask_6: 1.84  loss_dice_6: 4.284  loss_ce_7: 3.487  loss_mask_7: 1.709  loss_dice_7: 4.354  loss_ce_8: 3.588  loss_mask_8: 1.706  loss_dice_8: 4.407    time: 0.4429  last_time: 0.4289  data_time: 0.0261  last_data_time: 0.0223   lr: 1e-05  max_mem: 6709M\n",
      "\u001b[32m[07/17 13:19:17 d2.utils.events]: \u001b[0m eta: 1 day, 20:52:19  iter: 179  total_loss: 98.27  loss_ce: 3.372  loss_mask: 1.649  loss_dice: 4.316  loss_ce_0: 8.685  loss_mask_0: 1.553  loss_dice_0: 4.18  loss_ce_1: 3.324  loss_mask_1: 1.61  loss_dice_1: 4.189  loss_ce_2: 3.539  loss_mask_2: 1.583  loss_dice_2: 4.212  loss_ce_3: 3.514  loss_mask_3: 1.58  loss_dice_3: 4.281  loss_ce_4: 3.586  loss_mask_4: 1.508  loss_dice_4: 4.372  loss_ce_5: 3.478  loss_mask_5: 1.446  loss_dice_5: 4.438  loss_ce_6: 3.504  loss_mask_6: 1.425  loss_dice_6: 4.419  loss_ce_7: 3.412  loss_mask_7: 1.495  loss_dice_7: 4.496  loss_ce_8: 3.514  loss_mask_8: 1.388  loss_dice_8: 4.442    time: 0.4430  last_time: 0.4303  data_time: 0.0241  last_data_time: 0.0197   lr: 1e-05  max_mem: 6709M\n",
      "\u001b[32m[07/17 13:19:26 d2.utils.events]: \u001b[0m eta: 1 day, 20:52:11  iter: 199  total_loss: 95.71  loss_ce: 3.009  loss_mask: 1.867  loss_dice: 4.065  loss_ce_0: 8.609  loss_mask_0: 1.895  loss_dice_0: 3.91  loss_ce_1: 2.999  loss_mask_1: 1.801  loss_dice_1: 3.958  loss_ce_2: 2.874  loss_mask_2: 1.906  loss_dice_2: 3.974  loss_ce_3: 3.01  loss_mask_3: 1.945  loss_dice_3: 3.953  loss_ce_4: 3.066  loss_mask_4: 1.934  loss_dice_4: 4.07  loss_ce_5: 3.047  loss_mask_5: 1.957  loss_dice_5: 4.191  loss_ce_6: 3.221  loss_mask_6: 1.688  loss_dice_6: 4.341  loss_ce_7: 3.264  loss_mask_7: 1.875  loss_dice_7: 4.15  loss_ce_8: 3.233  loss_mask_8: 1.884  loss_dice_8: 4.09    time: 0.4426  last_time: 0.4289  data_time: 0.0228  last_data_time: 0.0190   lr: 1e-05  max_mem: 6709M\n",
      "\u001b[32m[07/17 13:19:35 d2.utils.events]: \u001b[0m eta: 1 day, 20:52:02  iter: 219  total_loss: 92.07  loss_ce: 2.835  loss_mask: 1.803  loss_dice: 3.973  loss_ce_0: 8.694  loss_mask_0: 2  loss_dice_0: 3.84  loss_ce_1: 2.614  loss_mask_1: 2.04  loss_dice_1: 3.876  loss_ce_2: 2.714  loss_mask_2: 1.931  loss_dice_2: 3.842  loss_ce_3: 2.581  loss_mask_3: 1.95  loss_dice_3: 3.928  loss_ce_4: 2.648  loss_mask_4: 1.952  loss_dice_4: 3.912  loss_ce_5: 2.496  loss_mask_5: 1.959  loss_dice_5: 3.971  loss_ce_6: 2.628  loss_mask_6: 1.871  loss_dice_6: 4.143  loss_ce_7: 2.658  loss_mask_7: 1.91  loss_dice_7: 4.037  loss_ce_8: 2.694  loss_mask_8: 1.868  loss_dice_8: 4.051    time: 0.4423  last_time: 0.4343  data_time: 0.0223  last_data_time: 0.0216   lr: 1e-05  max_mem: 6709M\n",
      "\u001b[32m[07/17 13:19:44 d2.utils.events]: \u001b[0m eta: 1 day, 20:51:21  iter: 239  total_loss: 96.96  loss_ce: 3.535  loss_mask: 2.041  loss_dice: 3.88  loss_ce_0: 8.634  loss_mask_0: 2.105  loss_dice_0: 3.804  loss_ce_1: 3.376  loss_mask_1: 2.029  loss_dice_1: 3.788  loss_ce_2: 3.206  loss_mask_2: 2.095  loss_dice_2: 3.846  loss_ce_3: 3.11  loss_mask_3: 2.133  loss_dice_3: 3.826  loss_ce_4: 3.104  loss_mask_4: 2.146  loss_dice_4: 3.768  loss_ce_5: 3.397  loss_mask_5: 2.046  loss_dice_5: 3.907  loss_ce_6: 3.356  loss_mask_6: 2.098  loss_dice_6: 3.869  loss_ce_7: 3.355  loss_mask_7: 2.163  loss_dice_7: 3.841  loss_ce_8: 3.217  loss_mask_8: 2.1  loss_dice_8: 4.027    time: 0.4423  last_time: 0.4416  data_time: 0.0254  last_data_time: 0.0235   lr: 1e-05  max_mem: 6709M\n",
      "\u001b[32m[07/17 13:19:52 d2.utils.events]: \u001b[0m eta: 1 day, 20:51:13  iter: 259  total_loss: 99.88  loss_ce: 3.541  loss_mask: 1.953  loss_dice: 4.004  loss_ce_0: 8.549  loss_mask_0: 2.1  loss_dice_0: 3.792  loss_ce_1: 3.258  loss_mask_1: 2.055  loss_dice_1: 3.874  loss_ce_2: 3.243  loss_mask_2: 2.027  loss_dice_2: 3.87  loss_ce_3: 3.404  loss_mask_3: 2.018  loss_dice_3: 3.876  loss_ce_4: 3.49  loss_mask_4: 2.098  loss_dice_4: 4.005  loss_ce_5: 3.585  loss_mask_5: 2.079  loss_dice_5: 3.96  loss_ce_6: 3.394  loss_mask_6: 1.856  loss_dice_6: 4.147  loss_ce_7: 3.464  loss_mask_7: 1.911  loss_dice_7: 4.162  loss_ce_8: 3.585  loss_mask_8: 2.035  loss_dice_8: 4.061    time: 0.4421  last_time: 0.4365  data_time: 0.0226  last_data_time: 0.0207   lr: 1e-05  max_mem: 6709M\n",
      "\u001b[32m[07/17 13:20:01 d2.utils.events]: \u001b[0m eta: 1 day, 20:50:41  iter: 279  total_loss: 97.39  loss_ce: 3.322  loss_mask: 1.9  loss_dice: 3.837  loss_ce_0: 8.522  loss_mask_0: 2.002  loss_dice_0: 3.591  loss_ce_1: 3.237  loss_mask_1: 1.968  loss_dice_1: 3.575  loss_ce_2: 3.257  loss_mask_2: 1.959  loss_dice_2: 3.727  loss_ce_3: 3.399  loss_mask_3: 2.047  loss_dice_3: 3.705  loss_ce_4: 3.643  loss_mask_4: 1.978  loss_dice_4: 3.699  loss_ce_5: 3.39  loss_mask_5: 1.968  loss_dice_5: 3.827  loss_ce_6: 3.468  loss_mask_6: 1.896  loss_dice_6: 3.731  loss_ce_7: 3.592  loss_mask_7: 1.941  loss_dice_7: 3.717  loss_ce_8: 3.401  loss_mask_8: 1.875  loss_dice_8: 3.761    time: 0.4419  last_time: 0.4332  data_time: 0.0236  last_data_time: 0.0208   lr: 1e-05  max_mem: 6709M\n",
      "\u001b[32m[07/17 13:20:10 d2.utils.events]: \u001b[0m eta: 1 day, 20:50:55  iter: 299  total_loss: 98.03  loss_ce: 3.522  loss_mask: 1.839  loss_dice: 3.94  loss_ce_0: 8.54  loss_mask_0: 1.825  loss_dice_0: 3.706  loss_ce_1: 3.454  loss_mask_1: 1.828  loss_dice_1: 3.782  loss_ce_2: 3.571  loss_mask_2: 1.853  loss_dice_2: 3.773  loss_ce_3: 3.552  loss_mask_3: 1.821  loss_dice_3: 3.871  loss_ce_4: 3.503  loss_mask_4: 1.838  loss_dice_4: 3.785  loss_ce_5: 3.498  loss_mask_5: 1.848  loss_dice_5: 3.848  loss_ce_6: 3.508  loss_mask_6: 1.776  loss_dice_6: 3.801  loss_ce_7: 3.569  loss_mask_7: 1.763  loss_dice_7: 3.855  loss_ce_8: 3.424  loss_mask_8: 1.75  loss_dice_8: 3.793    time: 0.4418  last_time: 0.4340  data_time: 0.0252  last_data_time: 0.0220   lr: 1e-05  max_mem: 6709M\n",
      "\u001b[32m[07/17 13:20:19 d2.utils.events]: \u001b[0m eta: 1 day, 20:49:59  iter: 319  total_loss: 94.81  loss_ce: 3.066  loss_mask: 2.129  loss_dice: 4.05  loss_ce_0: 8.38  loss_mask_0: 1.996  loss_dice_0: 3.732  loss_ce_1: 3.061  loss_mask_1: 1.993  loss_dice_1: 3.674  loss_ce_2: 3.03  loss_mask_2: 2.037  loss_dice_2: 3.705  loss_ce_3: 3.143  loss_mask_3: 2.087  loss_dice_3: 3.725  loss_ce_4: 3.264  loss_mask_4: 2.038  loss_dice_4: 3.963  loss_ce_5: 3.052  loss_mask_5: 1.993  loss_dice_5: 3.968  loss_ce_6: 3.091  loss_mask_6: 1.971  loss_dice_6: 3.791  loss_ce_7: 3.082  loss_mask_7: 2.03  loss_dice_7: 3.731  loss_ce_8: 3.122  loss_mask_8: 2.07  loss_dice_8: 3.834    time: 0.4417  last_time: 0.4552  data_time: 0.0239  last_data_time: 0.0433   lr: 1e-05  max_mem: 6709M\n",
      "\u001b[32m[07/17 13:20:28 d2.utils.events]: \u001b[0m eta: 1 day, 20:49:13  iter: 339  total_loss: 94.2  loss_ce: 3.334  loss_mask: 2.006  loss_dice: 3.899  loss_ce_0: 8.335  loss_mask_0: 1.874  loss_dice_0: 3.689  loss_ce_1: 2.998  loss_mask_1: 1.932  loss_dice_1: 3.741  loss_ce_2: 3.196  loss_mask_2: 1.928  loss_dice_2: 3.702  loss_ce_3: 3.061  loss_mask_3: 1.933  loss_dice_3: 3.806  loss_ce_4: 3.343  loss_mask_4: 1.922  loss_dice_4: 3.806  loss_ce_5: 3.328  loss_mask_5: 1.941  loss_dice_5: 3.966  loss_ce_6: 3.183  loss_mask_6: 2.073  loss_dice_6: 3.956  loss_ce_7: 3.275  loss_mask_7: 2.005  loss_dice_7: 3.843  loss_ce_8: 3.255  loss_mask_8: 2.004  loss_dice_8: 3.812    time: 0.4415  last_time: 0.4562  data_time: 0.0225  last_data_time: 0.0235   lr: 1e-05  max_mem: 6709M\n",
      "\u001b[32m[07/17 13:20:37 d2.utils.events]: \u001b[0m eta: 1 day, 20:50:29  iter: 359  total_loss: 100.3  loss_ce: 3.943  loss_mask: 2.006  loss_dice: 3.699  loss_ce_0: 8.344  loss_mask_0: 1.904  loss_dice_0: 3.566  loss_ce_1: 3.796  loss_mask_1: 2.129  loss_dice_1: 3.684  loss_ce_2: 3.696  loss_mask_2: 2.013  loss_dice_2: 3.668  loss_ce_3: 3.862  loss_mask_3: 2.012  loss_dice_3: 3.726  loss_ce_4: 3.845  loss_mask_4: 1.9  loss_dice_4: 3.779  loss_ce_5: 3.918  loss_mask_5: 1.882  loss_dice_5: 3.846  loss_ce_6: 3.778  loss_mask_6: 1.823  loss_dice_6: 3.853  loss_ce_7: 3.844  loss_mask_7: 1.95  loss_dice_7: 3.775  loss_ce_8: 3.977  loss_mask_8: 1.933  loss_dice_8: 3.793    time: 0.4416  last_time: 0.4682  data_time: 0.0266  last_data_time: 0.0460   lr: 1e-05  max_mem: 6718M\n",
      "\u001b[32m[07/17 13:20:45 d2.utils.events]: \u001b[0m eta: 1 day, 20:50:52  iter: 379  total_loss: 94.79  loss_ce: 3.293  loss_mask: 1.932  loss_dice: 3.833  loss_ce_0: 8.277  loss_mask_0: 1.965  loss_dice_0: 3.696  loss_ce_1: 3.389  loss_mask_1: 1.971  loss_dice_1: 3.684  loss_ce_2: 3.354  loss_mask_2: 1.956  loss_dice_2: 3.735  loss_ce_3: 3.4  loss_mask_3: 1.942  loss_dice_3: 3.775  loss_ce_4: 3.302  loss_mask_4: 1.837  loss_dice_4: 3.844  loss_ce_5: 3.31  loss_mask_5: 1.906  loss_dice_5: 3.819  loss_ce_6: 3.131  loss_mask_6: 1.923  loss_dice_6: 3.909  loss_ce_7: 3.2  loss_mask_7: 1.883  loss_dice_7: 3.866  loss_ce_8: 3.312  loss_mask_8: 1.96  loss_dice_8: 3.79    time: 0.4415  last_time: 0.4289  data_time: 0.0245  last_data_time: 0.0175   lr: 1e-05  max_mem: 6718M\n",
      "\u001b[32m[07/17 13:20:54 d2.utils.events]: \u001b[0m eta: 1 day, 20:50:43  iter: 399  total_loss: 95.09  loss_ce: 3.306  loss_mask: 2.056  loss_dice: 3.645  loss_ce_0: 8.253  loss_mask_0: 1.921  loss_dice_0: 3.554  loss_ce_1: 3.138  loss_mask_1: 2.013  loss_dice_1: 3.464  loss_ce_2: 3.238  loss_mask_2: 1.945  loss_dice_2: 3.476  loss_ce_3: 3.483  loss_mask_3: 2.082  loss_dice_3: 3.537  loss_ce_4: 3.446  loss_mask_4: 1.991  loss_dice_4: 3.628  loss_ce_5: 3.385  loss_mask_5: 2.017  loss_dice_5: 3.574  loss_ce_6: 3.239  loss_mask_6: 2.04  loss_dice_6: 3.708  loss_ce_7: 3.093  loss_mask_7: 2.143  loss_dice_7: 3.734  loss_ce_8: 3.274  loss_mask_8: 2.152  loss_dice_8: 3.672    time: 0.4414  last_time: 0.4346  data_time: 0.0241  last_data_time: 0.0239   lr: 1e-05  max_mem: 6718M\n",
      "\u001b[32m[07/17 13:21:03 d2.utils.events]: \u001b[0m eta: 1 day, 20:50:34  iter: 419  total_loss: 90.82  loss_ce: 2.995  loss_mask: 2.265  loss_dice: 3.301  loss_ce_0: 8.206  loss_mask_0: 2.093  loss_dice_0: 3.349  loss_ce_1: 2.925  loss_mask_1: 2.222  loss_dice_1: 3.181  loss_ce_2: 3.047  loss_mask_2: 2.113  loss_dice_2: 3.278  loss_ce_3: 2.832  loss_mask_3: 2.218  loss_dice_3: 3.291  loss_ce_4: 3.02  loss_mask_4: 2.235  loss_dice_4: 3.277  loss_ce_5: 3.134  loss_mask_5: 2.163  loss_dice_5: 3.343  loss_ce_6: 3.076  loss_mask_6: 2.229  loss_dice_6: 3.087  loss_ce_7: 3.108  loss_mask_7: 2.245  loss_dice_7: 3.241  loss_ce_8: 2.916  loss_mask_8: 2.255  loss_dice_8: 3.385    time: 0.4414  last_time: 0.4331  data_time: 0.0232  last_data_time: 0.0247   lr: 1e-05  max_mem: 6718M\n",
      "\u001b[32m[07/17 13:21:12 d2.utils.events]: \u001b[0m eta: 1 day, 20:50:25  iter: 439  total_loss: 93.87  loss_ce: 3.017  loss_mask: 2.056  loss_dice: 3.655  loss_ce_0: 8.213  loss_mask_0: 2.071  loss_dice_0: 3.42  loss_ce_1: 3.13  loss_mask_1: 2.019  loss_dice_1: 3.486  loss_ce_2: 3.12  loss_mask_2: 1.986  loss_dice_2: 3.365  loss_ce_3: 3.17  loss_mask_3: 1.94  loss_dice_3: 3.394  loss_ce_4: 3.002  loss_mask_4: 2.056  loss_dice_4: 3.465  loss_ce_5: 3.1  loss_mask_5: 1.945  loss_dice_5: 3.531  loss_ce_6: 3.3  loss_mask_6: 2.042  loss_dice_6: 3.706  loss_ce_7: 3.263  loss_mask_7: 2.081  loss_dice_7: 3.691  loss_ce_8: 3.041  loss_mask_8: 2.195  loss_dice_8: 3.65    time: 0.4413  last_time: 0.4521  data_time: 0.0227  last_data_time: 0.0170   lr: 1e-05  max_mem: 6718M\n",
      "\u001b[32m[07/17 13:21:21 d2.utils.events]: \u001b[0m eta: 1 day, 20:50:17  iter: 459  total_loss: 91.16  loss_ce: 3.401  loss_mask: 1.937  loss_dice: 3.377  loss_ce_0: 8.107  loss_mask_0: 1.879  loss_dice_0: 3.492  loss_ce_1: 3.433  loss_mask_1: 1.952  loss_dice_1: 3.445  loss_ce_2: 3.594  loss_mask_2: 1.937  loss_dice_2: 3.334  loss_ce_3: 3.578  loss_mask_3: 1.951  loss_dice_3: 3.302  loss_ce_4: 3.517  loss_mask_4: 1.89  loss_dice_4: 3.389  loss_ce_5: 3.349  loss_mask_5: 1.863  loss_dice_5: 3.468  loss_ce_6: 3.203  loss_mask_6: 2.104  loss_dice_6: 3.475  loss_ce_7: 3.346  loss_mask_7: 1.906  loss_dice_7: 3.602  loss_ce_8: 3.31  loss_mask_8: 1.837  loss_dice_8: 3.522    time: 0.4415  last_time: 0.4529  data_time: 0.0229  last_data_time: 0.0209   lr: 1e-05  max_mem: 6718M\n",
      "\u001b[32m[07/17 13:21:29 d2.utils.events]: \u001b[0m eta: 1 day, 20:49:36  iter: 479  total_loss: 90.09  loss_ce: 3.112  loss_mask: 2.127  loss_dice: 3.342  loss_ce_0: 8.042  loss_mask_0: 2.086  loss_dice_0: 3.177  loss_ce_1: 3.07  loss_mask_1: 2.106  loss_dice_1: 3.156  loss_ce_2: 3.108  loss_mask_2: 2.06  loss_dice_2: 3.066  loss_ce_3: 3.216  loss_mask_3: 2.088  loss_dice_3: 2.922  loss_ce_4: 3.174  loss_mask_4: 2.179  loss_dice_4: 3.158  loss_ce_5: 3.182  loss_mask_5: 2.174  loss_dice_5: 3.204  loss_ce_6: 3.187  loss_mask_6: 2.398  loss_dice_6: 3.193  loss_ce_7: 3.251  loss_mask_7: 2.177  loss_dice_7: 3.375  loss_ce_8: 3.207  loss_mask_8: 2.19  loss_dice_8: 3.182    time: 0.4414  last_time: 0.4604  data_time: 0.0233  last_data_time: 0.0196   lr: 1e-05  max_mem: 6718M\n",
      "\u001b[32m[07/17 13:21:38 d2.utils.events]: \u001b[0m eta: 1 day, 20:48:59  iter: 499  total_loss: 92.21  loss_ce: 2.971  loss_mask: 1.992  loss_dice: 3.431  loss_ce_0: 8.131  loss_mask_0: 1.883  loss_dice_0: 3.281  loss_ce_1: 3.137  loss_mask_1: 1.978  loss_dice_1: 3.185  loss_ce_2: 3.099  loss_mask_2: 2.097  loss_dice_2: 3.184  loss_ce_3: 3.146  loss_mask_3: 2.004  loss_dice_3: 3.173  loss_ce_4: 3.009  loss_mask_4: 2.074  loss_dice_4: 3.365  loss_ce_5: 3.023  loss_mask_5: 2.259  loss_dice_5: 3.409  loss_ce_6: 2.939  loss_mask_6: 2.304  loss_dice_6: 3.519  loss_ce_7: 3.102  loss_mask_7: 1.973  loss_dice_7: 3.4  loss_ce_8: 3.23  loss_mask_8: 2.019  loss_dice_8: 3.493    time: 0.4413  last_time: 0.4320  data_time: 0.0233  last_data_time: 0.0220   lr: 1e-05  max_mem: 6718M\n",
      "\u001b[32m[07/17 13:21:47 d2.utils.events]: \u001b[0m eta: 1 day, 20:48:56  iter: 519  total_loss: 90.66  loss_ce: 3.123  loss_mask: 2.082  loss_dice: 3.543  loss_ce_0: 8.1  loss_mask_0: 1.912  loss_dice_0: 3.502  loss_ce_1: 3.224  loss_mask_1: 2.019  loss_dice_1: 3.37  loss_ce_2: 3.327  loss_mask_2: 1.992  loss_dice_2: 3.317  loss_ce_3: 3.319  loss_mask_3: 1.929  loss_dice_3: 3.404  loss_ce_4: 3.162  loss_mask_4: 2.033  loss_dice_4: 3.427  loss_ce_5: 3.204  loss_mask_5: 1.975  loss_dice_5: 3.355  loss_ce_6: 3.213  loss_mask_6: 1.971  loss_dice_6: 3.42  loss_ce_7: 3.096  loss_mask_7: 2.031  loss_dice_7: 3.41  loss_ce_8: 3.073  loss_mask_8: 2.034  loss_dice_8: 3.503    time: 0.4413  last_time: 0.4300  data_time: 0.0241  last_data_time: 0.0208   lr: 1e-05  max_mem: 6718M\n",
      "\u001b[32m[07/17 13:21:56 d2.utils.events]: \u001b[0m eta: 1 day, 20:49:42  iter: 539  total_loss: 88.76  loss_ce: 2.863  loss_mask: 1.947  loss_dice: 3.538  loss_ce_0: 8.1  loss_mask_0: 1.979  loss_dice_0: 3.411  loss_ce_1: 2.898  loss_mask_1: 1.963  loss_dice_1: 3.29  loss_ce_2: 3.023  loss_mask_2: 1.882  loss_dice_2: 3.364  loss_ce_3: 3.073  loss_mask_3: 2.028  loss_dice_3: 3.398  loss_ce_4: 2.692  loss_mask_4: 1.998  loss_dice_4: 3.461  loss_ce_5: 2.815  loss_mask_5: 2.028  loss_dice_5: 3.417  loss_ce_6: 2.877  loss_mask_6: 1.973  loss_dice_6: 3.411  loss_ce_7: 2.885  loss_mask_7: 1.903  loss_dice_7: 3.426  loss_ce_8: 2.985  loss_mask_8: 1.971  loss_dice_8: 3.416    time: 0.4413  last_time: 0.4420  data_time: 0.0235  last_data_time: 0.0277   lr: 1e-05  max_mem: 6718M\n",
      "\u001b[32m[07/17 13:22:05 d2.utils.events]: \u001b[0m eta: 1 day, 20:49:33  iter: 559  total_loss: 91.06  loss_ce: 3.14  loss_mask: 2.116  loss_dice: 3.202  loss_ce_0: 8.077  loss_mask_0: 2.121  loss_dice_0: 3.267  loss_ce_1: 3.251  loss_mask_1: 2.216  loss_dice_1: 3.252  loss_ce_2: 3.022  loss_mask_2: 2.249  loss_dice_2: 3.097  loss_ce_3: 3.25  loss_mask_3: 2.175  loss_dice_3: 3.215  loss_ce_4: 3.022  loss_mask_4: 2.173  loss_dice_4: 3.174  loss_ce_5: 3.101  loss_mask_5: 2.083  loss_dice_5: 3.298  loss_ce_6: 3.201  loss_mask_6: 2.171  loss_dice_6: 3.235  loss_ce_7: 3.136  loss_mask_7: 2.055  loss_dice_7: 3.142  loss_ce_8: 3.082  loss_mask_8: 2.188  loss_dice_8: 3.303    time: 0.4412  last_time: 0.4514  data_time: 0.0234  last_data_time: 0.0245   lr: 1e-05  max_mem: 6718M\n",
      "\u001b[32m[07/17 13:22:14 d2.utils.events]: \u001b[0m eta: 1 day, 20:49:37  iter: 579  total_loss: 90.22  loss_ce: 3.167  loss_mask: 1.966  loss_dice: 2.997  loss_ce_0: 7.892  loss_mask_0: 1.982  loss_dice_0: 3.131  loss_ce_1: 3.065  loss_mask_1: 2.128  loss_dice_1: 2.903  loss_ce_2: 3.443  loss_mask_2: 1.975  loss_dice_2: 2.777  loss_ce_3: 3.298  loss_mask_3: 2.033  loss_dice_3: 2.849  loss_ce_4: 3.144  loss_mask_4: 2.165  loss_dice_4: 3.113  loss_ce_5: 3.051  loss_mask_5: 2.161  loss_dice_5: 2.994  loss_ce_6: 2.923  loss_mask_6: 2.008  loss_dice_6: 3.178  loss_ce_7: 3.02  loss_mask_7: 2.122  loss_dice_7: 3.068  loss_ce_8: 3.074  loss_mask_8: 2.132  loss_dice_8: 3.155    time: 0.4413  last_time: 0.4399  data_time: 0.0249  last_data_time: 0.0217   lr: 1e-05  max_mem: 6718M\n",
      "\u001b[32m[07/17 13:22:23 d2.utils.events]: \u001b[0m eta: 1 day, 20:49:28  iter: 599  total_loss: 93.72  loss_ce: 3.557  loss_mask: 2.027  loss_dice: 3.479  loss_ce_0: 7.886  loss_mask_0: 1.907  loss_dice_0: 3.394  loss_ce_1: 3.476  loss_mask_1: 1.818  loss_dice_1: 3.171  loss_ce_2: 3.681  loss_mask_2: 1.722  loss_dice_2: 3.282  loss_ce_3: 3.498  loss_mask_3: 1.865  loss_dice_3: 3.212  loss_ce_4: 3.516  loss_mask_4: 1.908  loss_dice_4: 3.322  loss_ce_5: 3.569  loss_mask_5: 1.816  loss_dice_5: 3.289  loss_ce_6: 3.497  loss_mask_6: 2.074  loss_dice_6: 3.521  loss_ce_7: 3.308  loss_mask_7: 1.873  loss_dice_7: 3.463  loss_ce_8: 3.567  loss_mask_8: 1.953  loss_dice_8: 3.464    time: 0.4414  last_time: 0.4503  data_time: 0.0261  last_data_time: 0.0247   lr: 1e-05  max_mem: 6718M\n",
      "\u001b[32m[07/17 13:22:31 d2.utils.events]: \u001b[0m eta: 1 day, 20:48:06  iter: 619  total_loss: 88.91  loss_ce: 3.221  loss_mask: 2.058  loss_dice: 3.188  loss_ce_0: 7.873  loss_mask_0: 1.947  loss_dice_0: 3.219  loss_ce_1: 3.129  loss_mask_1: 1.701  loss_dice_1: 3.013  loss_ce_2: 3.274  loss_mask_2: 1.701  loss_dice_2: 3.02  loss_ce_3: 3.209  loss_mask_3: 1.869  loss_dice_3: 3.039  loss_ce_4: 3.22  loss_mask_4: 2.005  loss_dice_4: 3.058  loss_ce_5: 3.17  loss_mask_5: 2.032  loss_dice_5: 3.063  loss_ce_6: 3.103  loss_mask_6: 2.001  loss_dice_6: 3.207  loss_ce_7: 3  loss_mask_7: 2.055  loss_dice_7: 3.383  loss_ce_8: 3.186  loss_mask_8: 2.135  loss_dice_8: 3.308    time: 0.4412  last_time: 0.4284  data_time: 0.0246  last_data_time: 0.0201   lr: 1e-05  max_mem: 6718M\n",
      "\u001b[32m[07/17 13:22:40 d2.utils.events]: \u001b[0m eta: 1 day, 20:47:58  iter: 639  total_loss: 90.54  loss_ce: 3.516  loss_mask: 2.007  loss_dice: 3.325  loss_ce_0: 7.931  loss_mask_0: 1.998  loss_dice_0: 3.236  loss_ce_1: 3.472  loss_mask_1: 1.842  loss_dice_1: 2.947  loss_ce_2: 3.429  loss_mask_2: 1.991  loss_dice_2: 3.048  loss_ce_3: 3.486  loss_mask_3: 2.089  loss_dice_3: 3.156  loss_ce_4: 3.331  loss_mask_4: 2.013  loss_dice_4: 3.244  loss_ce_5: 3.328  loss_mask_5: 1.927  loss_dice_5: 3.113  loss_ce_6: 3.191  loss_mask_6: 2.201  loss_dice_6: 3.192  loss_ce_7: 3.101  loss_mask_7: 2.348  loss_dice_7: 3.222  loss_ce_8: 3.283  loss_mask_8: 2.307  loss_dice_8: 3.339    time: 0.4412  last_time: 0.4323  data_time: 0.0239  last_data_time: 0.0199   lr: 1e-05  max_mem: 6718M\n",
      "\u001b[32m[07/17 13:22:49 d2.utils.events]: \u001b[0m eta: 1 day, 20:47:32  iter: 659  total_loss: 89.91  loss_ce: 3.237  loss_mask: 1.883  loss_dice: 3.264  loss_ce_0: 7.849  loss_mask_0: 1.718  loss_dice_0: 3.3  loss_ce_1: 3.227  loss_mask_1: 1.842  loss_dice_1: 3.232  loss_ce_2: 3.167  loss_mask_2: 1.925  loss_dice_2: 3.183  loss_ce_3: 3.32  loss_mask_3: 1.92  loss_dice_3: 3.223  loss_ce_4: 3.212  loss_mask_4: 1.957  loss_dice_4: 3.217  loss_ce_5: 3.264  loss_mask_5: 1.783  loss_dice_5: 3.188  loss_ce_6: 3.276  loss_mask_6: 1.975  loss_dice_6: 3.373  loss_ce_7: 3.202  loss_mask_7: 1.919  loss_dice_7: 3.453  loss_ce_8: 3.262  loss_mask_8: 1.851  loss_dice_8: 3.391    time: 0.4413  last_time: 0.5664  data_time: 0.0237  last_data_time: 0.0278   lr: 1e-05  max_mem: 6718M\n",
      "\u001b[32m[07/17 13:22:58 d2.utils.events]: \u001b[0m eta: 1 day, 20:47:11  iter: 679  total_loss: 85.28  loss_ce: 3.21  loss_mask: 2.193  loss_dice: 2.996  loss_ce_0: 7.871  loss_mask_0: 2.117  loss_dice_0: 2.922  loss_ce_1: 3.074  loss_mask_1: 1.956  loss_dice_1: 2.998  loss_ce_2: 3.001  loss_mask_2: 2.127  loss_dice_2: 2.922  loss_ce_3: 3.017  loss_mask_3: 2.113  loss_dice_3: 2.745  loss_ce_4: 3.119  loss_mask_4: 2.077  loss_dice_4: 2.861  loss_ce_5: 3.081  loss_mask_5: 2.012  loss_dice_5: 3.025  loss_ce_6: 2.922  loss_mask_6: 2.346  loss_dice_6: 2.968  loss_ce_7: 3.16  loss_mask_7: 2.06  loss_dice_7: 3.007  loss_ce_8: 3.085  loss_mask_8: 2.13  loss_dice_8: 3.06    time: 0.4413  last_time: 0.4699  data_time: 0.0234  last_data_time: 0.0390   lr: 1e-05  max_mem: 6718M\n",
      "\u001b[32m[07/17 13:23:07 d2.utils.events]: \u001b[0m eta: 1 day, 20:46:54  iter: 699  total_loss: 87.37  loss_ce: 2.991  loss_mask: 2.101  loss_dice: 3.387  loss_ce_0: 7.846  loss_mask_0: 1.812  loss_dice_0: 3.246  loss_ce_1: 3.02  loss_mask_1: 1.93  loss_dice_1: 3.149  loss_ce_2: 3.009  loss_mask_2: 1.91  loss_dice_2: 3.204  loss_ce_3: 2.987  loss_mask_3: 1.848  loss_dice_3: 3.24  loss_ce_4: 3.104  loss_mask_4: 1.864  loss_dice_4: 3.19  loss_ce_5: 3.175  loss_mask_5: 1.959  loss_dice_5: 3.253  loss_ce_6: 3  loss_mask_6: 2.02  loss_dice_6: 3.192  loss_ce_7: 3.119  loss_mask_7: 1.798  loss_dice_7: 3.137  loss_ce_8: 3.116  loss_mask_8: 2.005  loss_dice_8: 3.226    time: 0.4419  last_time: 0.4270  data_time: 0.0247  last_data_time: 0.0196   lr: 1e-05  max_mem: 6718M\n",
      "\u001b[32m[07/17 13:23:16 d2.utils.events]: \u001b[0m eta: 1 day, 20:46:18  iter: 719  total_loss: 88.79  loss_ce: 3.377  loss_mask: 2.047  loss_dice: 3.038  loss_ce_0: 7.719  loss_mask_0: 2  loss_dice_0: 3.128  loss_ce_1: 3.412  loss_mask_1: 1.944  loss_dice_1: 2.93  loss_ce_2: 3.314  loss_mask_2: 2.063  loss_dice_2: 2.882  loss_ce_3: 3.406  loss_mask_3: 2.099  loss_dice_3: 2.951  loss_ce_4: 3.418  loss_mask_4: 2.132  loss_dice_4: 2.936  loss_ce_5: 3.465  loss_mask_5: 2.022  loss_dice_5: 3.109  loss_ce_6: 3.422  loss_mask_6: 2.31  loss_dice_6: 3.063  loss_ce_7: 3.408  loss_mask_7: 2.266  loss_dice_7: 2.999  loss_ce_8: 3.433  loss_mask_8: 2.158  loss_dice_8: 2.952    time: 0.4418  last_time: 0.4334  data_time: 0.0252  last_data_time: 0.0216   lr: 1e-05  max_mem: 6718M\n",
      "\u001b[32m[07/17 13:23:25 d2.utils.events]: \u001b[0m eta: 1 day, 20:46:33  iter: 739  total_loss: 84.23  loss_ce: 2.855  loss_mask: 2.18  loss_dice: 2.943  loss_ce_0: 7.652  loss_mask_0: 2.074  loss_dice_0: 2.986  loss_ce_1: 2.99  loss_mask_1: 2.098  loss_dice_1: 2.719  loss_ce_2: 2.799  loss_mask_2: 2.121  loss_dice_2: 2.627  loss_ce_3: 2.875  loss_mask_3: 2.187  loss_dice_3: 2.748  loss_ce_4: 2.842  loss_mask_4: 2.171  loss_dice_4: 2.861  loss_ce_5: 2.847  loss_mask_5: 2.286  loss_dice_5: 2.928  loss_ce_6: 2.868  loss_mask_6: 2.134  loss_dice_6: 2.959  loss_ce_7: 2.891  loss_mask_7: 2.164  loss_dice_7: 3.069  loss_ce_8: 2.891  loss_mask_8: 2.316  loss_dice_8: 2.79    time: 0.4420  last_time: 0.4260  data_time: 0.0234  last_data_time: 0.0190   lr: 1e-05  max_mem: 6718M\n",
      "\u001b[32m[07/17 13:23:34 d2.utils.events]: \u001b[0m eta: 1 day, 20:46:00  iter: 759  total_loss: 88.41  loss_ce: 3.139  loss_mask: 2.225  loss_dice: 3.291  loss_ce_0: 7.702  loss_mask_0: 1.952  loss_dice_0: 3.23  loss_ce_1: 3.02  loss_mask_1: 1.873  loss_dice_1: 3.105  loss_ce_2: 2.982  loss_mask_2: 1.899  loss_dice_2: 3.089  loss_ce_3: 2.956  loss_mask_3: 2.087  loss_dice_3: 3.086  loss_ce_4: 3.022  loss_mask_4: 2.037  loss_dice_4: 3.105  loss_ce_5: 3.019  loss_mask_5: 2.173  loss_dice_5: 3.343  loss_ce_6: 3.029  loss_mask_6: 2.001  loss_dice_6: 3.174  loss_ce_7: 3.038  loss_mask_7: 2.22  loss_dice_7: 3.383  loss_ce_8: 3.065  loss_mask_8: 2.008  loss_dice_8: 3.247    time: 0.4422  last_time: 0.4331  data_time: 0.0233  last_data_time: 0.0238   lr: 1e-05  max_mem: 6718M\n",
      "\u001b[32m[07/17 13:23:43 d2.utils.events]: \u001b[0m eta: 1 day, 20:46:06  iter: 779  total_loss: 88.54  loss_ce: 3.249  loss_mask: 1.985  loss_dice: 3.015  loss_ce_0: 7.567  loss_mask_0: 2.073  loss_dice_0: 2.987  loss_ce_1: 3.05  loss_mask_1: 1.96  loss_dice_1: 2.919  loss_ce_2: 2.948  loss_mask_2: 2.236  loss_dice_2: 3.088  loss_ce_3: 3.039  loss_mask_3: 2.04  loss_dice_3: 3.194  loss_ce_4: 3.056  loss_mask_4: 2.081  loss_dice_4: 3.189  loss_ce_5: 2.986  loss_mask_5: 2.173  loss_dice_5: 3.191  loss_ce_6: 2.934  loss_mask_6: 2.034  loss_dice_6: 3.207  loss_ce_7: 3.11  loss_mask_7: 2.086  loss_dice_7: 3.014  loss_ce_8: 2.984  loss_mask_8: 2.071  loss_dice_8: 2.904    time: 0.4424  last_time: 0.4749  data_time: 0.0248  last_data_time: 0.0405   lr: 1e-05  max_mem: 6718M\n",
      "\u001b[32m[07/17 13:23:52 d2.utils.events]: \u001b[0m eta: 1 day, 20:46:18  iter: 799  total_loss: 86.92  loss_ce: 3.083  loss_mask: 2.057  loss_dice: 3.266  loss_ce_0: 7.59  loss_mask_0: 1.919  loss_dice_0: 3.498  loss_ce_1: 3.059  loss_mask_1: 1.91  loss_dice_1: 3.286  loss_ce_2: 3.014  loss_mask_2: 2.012  loss_dice_2: 3.071  loss_ce_3: 3.127  loss_mask_3: 1.754  loss_dice_3: 3.128  loss_ce_4: 3.132  loss_mask_4: 2.054  loss_dice_4: 3.176  loss_ce_5: 3.067  loss_mask_5: 1.904  loss_dice_5: 3.149  loss_ce_6: 2.95  loss_mask_6: 1.978  loss_dice_6: 3.356  loss_ce_7: 3.051  loss_mask_7: 1.859  loss_dice_7: 3.132  loss_ce_8: 3.019  loss_mask_8: 1.771  loss_dice_8: 2.951    time: 0.4425  last_time: 0.4393  data_time: 0.0239  last_data_time: 0.0259   lr: 1e-05  max_mem: 6718M\n",
      "\u001b[32m[07/17 13:24:01 d2.utils.events]: \u001b[0m eta: 1 day, 20:46:33  iter: 819  total_loss: 88.4  loss_ce: 3.419  loss_mask: 1.933  loss_dice: 3.251  loss_ce_0: 7.597  loss_mask_0: 1.833  loss_dice_0: 3.211  loss_ce_1: 3.425  loss_mask_1: 1.924  loss_dice_1: 3.134  loss_ce_2: 3.366  loss_mask_2: 1.862  loss_dice_2: 2.906  loss_ce_3: 3.419  loss_mask_3: 1.898  loss_dice_3: 3.037  loss_ce_4: 3.387  loss_mask_4: 1.931  loss_dice_4: 2.999  loss_ce_5: 3.432  loss_mask_5: 1.98  loss_dice_5: 3.189  loss_ce_6: 3.339  loss_mask_6: 2.033  loss_dice_6: 3.099  loss_ce_7: 3.467  loss_mask_7: 2.008  loss_dice_7: 3.048  loss_ce_8: 3.384  loss_mask_8: 1.861  loss_dice_8: 3.042    time: 0.4428  last_time: 0.4400  data_time: 0.0253  last_data_time: 0.0213   lr: 1e-05  max_mem: 6718M\n",
      "\u001b[32m[07/17 13:24:10 d2.utils.events]: \u001b[0m eta: 1 day, 20:46:24  iter: 839  total_loss: 84.86  loss_ce: 2.946  loss_mask: 1.86  loss_dice: 3.089  loss_ce_0: 7.467  loss_mask_0: 1.909  loss_dice_0: 3.024  loss_ce_1: 2.894  loss_mask_1: 1.937  loss_dice_1: 3.011  loss_ce_2: 2.919  loss_mask_2: 2.081  loss_dice_2: 2.849  loss_ce_3: 2.908  loss_mask_3: 2.174  loss_dice_3: 2.807  loss_ce_4: 2.984  loss_mask_4: 2.148  loss_dice_4: 2.988  loss_ce_5: 2.926  loss_mask_5: 2.079  loss_dice_5: 3.022  loss_ce_6: 3.05  loss_mask_6: 2.094  loss_dice_6: 2.938  loss_ce_7: 2.908  loss_mask_7: 2.114  loss_dice_7: 3.097  loss_ce_8: 2.93  loss_mask_8: 2.131  loss_dice_8: 3.104    time: 0.4428  last_time: 0.4596  data_time: 0.0227  last_data_time: 0.0217   lr: 1e-05  max_mem: 6718M\n",
      "\u001b[32m[07/17 13:24:19 d2.utils.events]: \u001b[0m eta: 1 day, 20:46:04  iter: 859  total_loss: 93.96  loss_ce: 3.26  loss_mask: 2.128  loss_dice: 3.52  loss_ce_0: 7.482  loss_mask_0: 1.893  loss_dice_0: 3.454  loss_ce_1: 3.28  loss_mask_1: 1.773  loss_dice_1: 3.295  loss_ce_2: 3.493  loss_mask_2: 1.954  loss_dice_2: 3.292  loss_ce_3: 3.418  loss_mask_3: 2.045  loss_dice_3: 3.361  loss_ce_4: 3.291  loss_mask_4: 2.032  loss_dice_4: 3.337  loss_ce_5: 3.528  loss_mask_5: 2.014  loss_dice_5: 3.256  loss_ce_6: 3.538  loss_mask_6: 2.143  loss_dice_6: 3.524  loss_ce_7: 3.499  loss_mask_7: 1.998  loss_dice_7: 3.388  loss_ce_8: 3.412  loss_mask_8: 2.026  loss_dice_8: 3.445    time: 0.4430  last_time: 0.4504  data_time: 0.0239  last_data_time: 0.0255   lr: 1e-05  max_mem: 6718M\n",
      "\u001b[32m[07/17 13:24:28 d2.utils.events]: \u001b[0m eta: 1 day, 20:46:06  iter: 879  total_loss: 87.96  loss_ce: 3.372  loss_mask: 2.107  loss_dice: 3.084  loss_ce_0: 7.432  loss_mask_0: 2.096  loss_dice_0: 3.161  loss_ce_1: 3.139  loss_mask_1: 1.905  loss_dice_1: 3.018  loss_ce_2: 3.074  loss_mask_2: 1.99  loss_dice_2: 2.959  loss_ce_3: 3.037  loss_mask_3: 2.113  loss_dice_3: 3.161  loss_ce_4: 3.105  loss_mask_4: 2.154  loss_dice_4: 3.1  loss_ce_5: 3.15  loss_mask_5: 2.112  loss_dice_5: 3.007  loss_ce_6: 3.22  loss_mask_6: 2.105  loss_dice_6: 3.019  loss_ce_7: 3.262  loss_mask_7: 2.118  loss_dice_7: 3.177  loss_ce_8: 3.219  loss_mask_8: 2.113  loss_dice_8: 3.107    time: 0.4432  last_time: 0.4306  data_time: 0.0236  last_data_time: 0.0223   lr: 1e-05  max_mem: 6718M\n",
      "\u001b[32m[07/17 13:24:37 d2.utils.events]: \u001b[0m eta: 1 day, 20:45:27  iter: 899  total_loss: 83.23  loss_ce: 2.987  loss_mask: 1.875  loss_dice: 3.173  loss_ce_0: 7.47  loss_mask_0: 1.824  loss_dice_0: 3.274  loss_ce_1: 2.927  loss_mask_1: 1.779  loss_dice_1: 3.044  loss_ce_2: 2.97  loss_mask_2: 1.769  loss_dice_2: 3.141  loss_ce_3: 2.869  loss_mask_3: 1.938  loss_dice_3: 3.002  loss_ce_4: 2.975  loss_mask_4: 2.111  loss_dice_4: 3.218  loss_ce_5: 3.02  loss_mask_5: 1.744  loss_dice_5: 3.2  loss_ce_6: 2.898  loss_mask_6: 1.791  loss_dice_6: 3.016  loss_ce_7: 3.011  loss_mask_7: 1.762  loss_dice_7: 3.195  loss_ce_8: 2.941  loss_mask_8: 1.794  loss_dice_8: 3.104    time: 0.4433  last_time: 0.4234  data_time: 0.0242  last_data_time: 0.0178   lr: 1e-05  max_mem: 6718M\n",
      "\u001b[32m[07/17 13:24:46 d2.utils.events]: \u001b[0m eta: 1 day, 20:45:14  iter: 919  total_loss: 88.57  loss_ce: 3.138  loss_mask: 2.17  loss_dice: 3.188  loss_ce_0: 7.347  loss_mask_0: 1.952  loss_dice_0: 3.147  loss_ce_1: 3.281  loss_mask_1: 1.975  loss_dice_1: 2.855  loss_ce_2: 3.144  loss_mask_2: 1.953  loss_dice_2: 2.775  loss_ce_3: 3.272  loss_mask_3: 1.925  loss_dice_3: 2.87  loss_ce_4: 3.141  loss_mask_4: 1.983  loss_dice_4: 2.78  loss_ce_5: 3.077  loss_mask_5: 1.949  loss_dice_5: 2.959  loss_ce_6: 2.924  loss_mask_6: 2.133  loss_dice_6: 2.943  loss_ce_7: 3.02  loss_mask_7: 2.187  loss_dice_7: 2.952  loss_ce_8: 3.191  loss_mask_8: 2.052  loss_dice_8: 2.915    time: 0.4434  last_time: 0.4347  data_time: 0.0241  last_data_time: 0.0242   lr: 1e-05  max_mem: 6718M\n",
      "\u001b[32m[07/17 13:24:55 d2.utils.events]: \u001b[0m eta: 1 day, 20:45:17  iter: 939  total_loss: 86.68  loss_ce: 3.116  loss_mask: 2.15  loss_dice: 3.293  loss_ce_0: 7.384  loss_mask_0: 1.967  loss_dice_0: 3.177  loss_ce_1: 3.143  loss_mask_1: 2.029  loss_dice_1: 3.087  loss_ce_2: 3.307  loss_mask_2: 1.905  loss_dice_2: 3.001  loss_ce_3: 3.338  loss_mask_3: 2.163  loss_dice_3: 2.941  loss_ce_4: 3.184  loss_mask_4: 2.065  loss_dice_4: 2.972  loss_ce_5: 3.197  loss_mask_5: 1.834  loss_dice_5: 2.933  loss_ce_6: 3.142  loss_mask_6: 1.968  loss_dice_6: 3.141  loss_ce_7: 3.138  loss_mask_7: 2.012  loss_dice_7: 3.208  loss_ce_8: 3.078  loss_mask_8: 2.039  loss_dice_8: 3.152    time: 0.4437  last_time: 0.5275  data_time: 0.0236  last_data_time: 0.0391   lr: 1e-05  max_mem: 6718M\n",
      "\u001b[32m[07/17 13:25:04 d2.utils.events]: \u001b[0m eta: 1 day, 20:45:08  iter: 959  total_loss: 87.14  loss_ce: 3.254  loss_mask: 1.989  loss_dice: 3.167  loss_ce_0: 7.32  loss_mask_0: 1.911  loss_dice_0: 3.301  loss_ce_1: 3.312  loss_mask_1: 1.855  loss_dice_1: 3.049  loss_ce_2: 3.361  loss_mask_2: 1.985  loss_dice_2: 3.224  loss_ce_3: 3.246  loss_mask_3: 1.964  loss_dice_3: 3.078  loss_ce_4: 3.359  loss_mask_4: 1.942  loss_dice_4: 3.054  loss_ce_5: 3.371  loss_mask_5: 1.779  loss_dice_5: 3.114  loss_ce_6: 3.383  loss_mask_6: 1.953  loss_dice_6: 3.236  loss_ce_7: 3.322  loss_mask_7: 2.018  loss_dice_7: 3.047  loss_ce_8: 3.32  loss_mask_8: 2.063  loss_dice_8: 3.201    time: 0.4437  last_time: 0.4912  data_time: 0.0249  last_data_time: 0.0232   lr: 1e-05  max_mem: 6718M\n",
      "\u001b[32m[07/17 13:25:13 d2.utils.events]: \u001b[0m eta: 1 day, 20:44:59  iter: 979  total_loss: 85.96  loss_ce: 3.186  loss_mask: 1.986  loss_dice: 2.917  loss_ce_0: 7.33  loss_mask_0: 2.015  loss_dice_0: 3.062  loss_ce_1: 3.093  loss_mask_1: 1.856  loss_dice_1: 2.9  loss_ce_2: 3.124  loss_mask_2: 2.075  loss_dice_2: 3.013  loss_ce_3: 3.191  loss_mask_3: 1.918  loss_dice_3: 2.837  loss_ce_4: 3.245  loss_mask_4: 1.949  loss_dice_4: 2.859  loss_ce_5: 3.175  loss_mask_5: 2  loss_dice_5: 3.031  loss_ce_6: 3.283  loss_mask_6: 2.146  loss_dice_6: 2.963  loss_ce_7: 3.042  loss_mask_7: 2.105  loss_dice_7: 3.014  loss_ce_8: 3.024  loss_mask_8: 2.262  loss_dice_8: 3.003    time: 0.4438  last_time: 0.4408  data_time: 0.0250  last_data_time: 0.0189   lr: 1e-05  max_mem: 6718M\n",
      "\u001b[32m[07/17 13:25:22 d2.utils.events]: \u001b[0m eta: 1 day, 20:45:03  iter: 999  total_loss: 85.26  loss_ce: 3.011  loss_mask: 1.885  loss_dice: 3.058  loss_ce_0: 7.347  loss_mask_0: 2.068  loss_dice_0: 3.292  loss_ce_1: 3.107  loss_mask_1: 1.976  loss_dice_1: 3.045  loss_ce_2: 3.106  loss_mask_2: 2.045  loss_dice_2: 3.014  loss_ce_3: 3.109  loss_mask_3: 2.22  loss_dice_3: 2.98  loss_ce_4: 3.111  loss_mask_4: 1.98  loss_dice_4: 2.974  loss_ce_5: 3.064  loss_mask_5: 2.064  loss_dice_5: 2.968  loss_ce_6: 2.891  loss_mask_6: 2.141  loss_dice_6: 3.102  loss_ce_7: 2.961  loss_mask_7: 2.016  loss_dice_7: 3.13  loss_ce_8: 3.103  loss_mask_8: 2.078  loss_dice_8: 3.018    time: 0.4438  last_time: 0.4322  data_time: 0.0240  last_data_time: 0.0219   lr: 1e-05  max_mem: 6718M\n",
      "\u001b[32m[07/17 13:25:31 d2.utils.events]: \u001b[0m eta: 1 day, 20:44:34  iter: 1019  total_loss: 83.42  loss_ce: 3.117  loss_mask: 1.864  loss_dice: 3.044  loss_ce_0: 7.25  loss_mask_0: 1.796  loss_dice_0: 3.033  loss_ce_1: 2.931  loss_mask_1: 1.852  loss_dice_1: 2.767  loss_ce_2: 2.947  loss_mask_2: 1.838  loss_dice_2: 2.883  loss_ce_3: 2.921  loss_mask_3: 1.749  loss_dice_3: 2.891  loss_ce_4: 3.132  loss_mask_4: 1.805  loss_dice_4: 2.895  loss_ce_5: 3.106  loss_mask_5: 1.914  loss_dice_5: 2.959  loss_ce_6: 2.983  loss_mask_6: 2.016  loss_dice_6: 3.085  loss_ce_7: 2.946  loss_mask_7: 2.039  loss_dice_7: 3.096  loss_ce_8: 3.035  loss_mask_8: 1.769  loss_dice_8: 2.964    time: 0.4439  last_time: 0.4435  data_time: 0.0235  last_data_time: 0.0266   lr: 1e-05  max_mem: 6718M\n",
      "\u001b[32m[07/17 13:25:40 d2.utils.events]: \u001b[0m eta: 1 day, 20:44:33  iter: 1039  total_loss: 87.96  loss_ce: 3.49  loss_mask: 1.915  loss_dice: 3.015  loss_ce_0: 7.394  loss_mask_0: 1.962  loss_dice_0: 3.162  loss_ce_1: 3.491  loss_mask_1: 1.99  loss_dice_1: 2.98  loss_ce_2: 3.41  loss_mask_2: 2.047  loss_dice_2: 2.851  loss_ce_3: 3.432  loss_mask_3: 1.946  loss_dice_3: 2.923  loss_ce_4: 3.594  loss_mask_4: 2.163  loss_dice_4: 2.957  loss_ce_5: 3.547  loss_mask_5: 2.058  loss_dice_5: 3.114  loss_ce_6: 3.426  loss_mask_6: 2.172  loss_dice_6: 3.074  loss_ce_7: 3.352  loss_mask_7: 2.181  loss_dice_7: 3.039  loss_ce_8: 3.607  loss_mask_8: 1.956  loss_dice_8: 2.864    time: 0.4441  last_time: 0.4625  data_time: 0.0241  last_data_time: 0.0207   lr: 1e-05  max_mem: 6718M\n",
      "\u001b[32m[07/17 13:25:49 d2.utils.events]: \u001b[0m eta: 1 day, 20:44:17  iter: 1059  total_loss: 86.87  loss_ce: 3.441  loss_mask: 1.962  loss_dice: 3.013  loss_ce_0: 7.224  loss_mask_0: 1.879  loss_dice_0: 3.137  loss_ce_1: 3.332  loss_mask_1: 1.745  loss_dice_1: 2.93  loss_ce_2: 3.206  loss_mask_2: 1.843  loss_dice_2: 2.939  loss_ce_3: 3.326  loss_mask_3: 1.897  loss_dice_3: 2.936  loss_ce_4: 3.446  loss_mask_4: 1.762  loss_dice_4: 2.914  loss_ce_5: 3.27  loss_mask_5: 1.835  loss_dice_5: 2.946  loss_ce_6: 3.15  loss_mask_6: 2.023  loss_dice_6: 3.101  loss_ce_7: 3.191  loss_mask_7: 1.886  loss_dice_7: 2.919  loss_ce_8: 3.346  loss_mask_8: 1.823  loss_dice_8: 3.024    time: 0.4442  last_time: 0.5360  data_time: 0.0242  last_data_time: 0.0402   lr: 1e-05  max_mem: 6718M\n",
      "\u001b[32m[07/17 13:25:58 d2.utils.events]: \u001b[0m eta: 1 day, 20:44:04  iter: 1079  total_loss: 86.52  loss_ce: 3  loss_mask: 2.097  loss_dice: 3.006  loss_ce_0: 7.14  loss_mask_0: 1.952  loss_dice_0: 3.027  loss_ce_1: 3.098  loss_mask_1: 1.992  loss_dice_1: 2.845  loss_ce_2: 2.995  loss_mask_2: 2.043  loss_dice_2: 2.839  loss_ce_3: 2.953  loss_mask_3: 2.073  loss_dice_3: 2.857  loss_ce_4: 3.094  loss_mask_4: 1.992  loss_dice_4: 2.763  loss_ce_5: 3.115  loss_mask_5: 2.15  loss_dice_5: 2.946  loss_ce_6: 2.954  loss_mask_6: 1.778  loss_dice_6: 2.78  loss_ce_7: 2.988  loss_mask_7: 2.381  loss_dice_7: 2.932  loss_ce_8: 3.101  loss_mask_8: 2.228  loss_dice_8: 2.803    time: 0.4441  last_time: 0.4492  data_time: 0.0232  last_data_time: 0.0228   lr: 1e-05  max_mem: 6718M\n",
      "\u001b[32m[07/17 13:26:07 d2.utils.events]: \u001b[0m eta: 1 day, 20:43:59  iter: 1099  total_loss: 82.94  loss_ce: 3.034  loss_mask: 1.833  loss_dice: 3.006  loss_ce_0: 7.185  loss_mask_0: 1.743  loss_dice_0: 3.226  loss_ce_1: 2.952  loss_mask_1: 1.86  loss_dice_1: 3.079  loss_ce_2: 2.968  loss_mask_2: 1.659  loss_dice_2: 3.046  loss_ce_3: 3.167  loss_mask_3: 1.755  loss_dice_3: 2.898  loss_ce_4: 3.004  loss_mask_4: 1.767  loss_dice_4: 2.932  loss_ce_5: 2.948  loss_mask_5: 2.045  loss_dice_5: 3.153  loss_ce_6: 2.858  loss_mask_6: 1.816  loss_dice_6: 3.056  loss_ce_7: 3.124  loss_mask_7: 1.65  loss_dice_7: 2.953  loss_ce_8: 3.112  loss_mask_8: 1.802  loss_dice_8: 3.174    time: 0.4443  last_time: 0.4299  data_time: 0.0249  last_data_time: 0.0198   lr: 1e-05  max_mem: 6718M\n",
      "\u001b[32m[07/17 13:26:16 d2.utils.events]: \u001b[0m eta: 1 day, 20:43:47  iter: 1119  total_loss: 85.85  loss_ce: 3.083  loss_mask: 1.884  loss_dice: 3.192  loss_ce_0: 7.137  loss_mask_0: 1.871  loss_dice_0: 3.168  loss_ce_1: 3.088  loss_mask_1: 1.722  loss_dice_1: 2.973  loss_ce_2: 3.14  loss_mask_2: 1.895  loss_dice_2: 3.004  loss_ce_3: 3.245  loss_mask_3: 1.877  loss_dice_3: 3.001  loss_ce_4: 3.114  loss_mask_4: 1.939  loss_dice_4: 3.065  loss_ce_5: 3.209  loss_mask_5: 2.014  loss_dice_5: 2.98  loss_ce_6: 3.09  loss_mask_6: 2.019  loss_dice_6: 2.975  loss_ce_7: 3.175  loss_mask_7: 2.038  loss_dice_7: 3.022  loss_ce_8: 3.198  loss_mask_8: 2.051  loss_dice_8: 3.063    time: 0.4443  last_time: 0.4471  data_time: 0.0228  last_data_time: 0.0222   lr: 1e-05  max_mem: 6718M\n",
      "\u001b[32m[07/17 13:26:25 d2.utils.events]: \u001b[0m eta: 1 day, 20:44:12  iter: 1139  total_loss: 87.86  loss_ce: 3.405  loss_mask: 1.901  loss_dice: 3.064  loss_ce_0: 7.203  loss_mask_0: 1.901  loss_dice_0: 3.077  loss_ce_1: 3.365  loss_mask_1: 2.065  loss_dice_1: 2.897  loss_ce_2: 3.46  loss_mask_2: 1.956  loss_dice_2: 2.899  loss_ce_3: 3.354  loss_mask_3: 1.886  loss_dice_3: 2.902  loss_ce_4: 3.28  loss_mask_4: 1.795  loss_dice_4: 2.836  loss_ce_5: 3.29  loss_mask_5: 2.027  loss_dice_5: 2.966  loss_ce_6: 3.29  loss_mask_6: 2.19  loss_dice_6: 3.007  loss_ce_7: 3.377  loss_mask_7: 1.928  loss_dice_7: 2.956  loss_ce_8: 3.383  loss_mask_8: 1.794  loss_dice_8: 3.037    time: 0.4444  last_time: 0.4414  data_time: 0.0246  last_data_time: 0.0240   lr: 1e-05  max_mem: 6718M\n",
      "\u001b[32m[07/17 13:26:34 d2.utils.events]: \u001b[0m eta: 1 day, 20:43:33  iter: 1159  total_loss: 83.81  loss_ce: 3.171  loss_mask: 2.003  loss_dice: 2.931  loss_ce_0: 6.95  loss_mask_0: 1.852  loss_dice_0: 3.069  loss_ce_1: 3.278  loss_mask_1: 1.943  loss_dice_1: 2.959  loss_ce_2: 3.21  loss_mask_2: 1.961  loss_dice_2: 2.911  loss_ce_3: 3.245  loss_mask_3: 1.966  loss_dice_3: 2.822  loss_ce_4: 3.242  loss_mask_4: 2.085  loss_dice_4: 2.889  loss_ce_5: 3.405  loss_mask_5: 1.914  loss_dice_5: 2.807  loss_ce_6: 3.278  loss_mask_6: 2.008  loss_dice_6: 2.947  loss_ce_7: 3.302  loss_mask_7: 1.897  loss_dice_7: 2.854  loss_ce_8: 3.094  loss_mask_8: 2.209  loss_dice_8: 2.881    time: 0.4446  last_time: 0.4346  data_time: 0.0253  last_data_time: 0.0193   lr: 1e-05  max_mem: 6718M\n",
      "\u001b[32m[07/17 13:26:43 d2.utils.events]: \u001b[0m eta: 1 day, 20:43:32  iter: 1179  total_loss: 87.63  loss_ce: 3.522  loss_mask: 1.706  loss_dice: 3.099  loss_ce_0: 7.007  loss_mask_0: 1.832  loss_dice_0: 3.183  loss_ce_1: 3.57  loss_mask_1: 1.705  loss_dice_1: 2.818  loss_ce_2: 3.592  loss_mask_2: 1.856  loss_dice_2: 2.994  loss_ce_3: 3.61  loss_mask_3: 1.762  loss_dice_3: 2.868  loss_ce_4: 3.561  loss_mask_4: 1.739  loss_dice_4: 3.015  loss_ce_5: 3.637  loss_mask_5: 1.768  loss_dice_5: 3.051  loss_ce_6: 3.614  loss_mask_6: 2.058  loss_dice_6: 3.122  loss_ce_7: 3.521  loss_mask_7: 1.956  loss_dice_7: 3.131  loss_ce_8: 3.513  loss_mask_8: 1.963  loss_dice_8: 3.043    time: 0.4447  last_time: 0.5332  data_time: 0.0272  last_data_time: 0.0284   lr: 1e-05  max_mem: 6718M\n",
      "\u001b[32m[07/17 13:26:52 d2.utils.events]: \u001b[0m eta: 1 day, 20:43:35  iter: 1199  total_loss: 87.89  loss_ce: 3.195  loss_mask: 1.931  loss_dice: 3.109  loss_ce_0: 6.93  loss_mask_0: 1.927  loss_dice_0: 3.106  loss_ce_1: 3.126  loss_mask_1: 1.894  loss_dice_1: 2.902  loss_ce_2: 3.248  loss_mask_2: 1.906  loss_dice_2: 2.859  loss_ce_3: 3.256  loss_mask_3: 1.957  loss_dice_3: 2.786  loss_ce_4: 3.281  loss_mask_4: 1.917  loss_dice_4: 2.859  loss_ce_5: 3.303  loss_mask_5: 1.903  loss_dice_5: 2.944  loss_ce_6: 3.282  loss_mask_6: 1.917  loss_dice_6: 2.961  loss_ce_7: 3.353  loss_mask_7: 1.809  loss_dice_7: 3.03  loss_ce_8: 3.278  loss_mask_8: 1.872  loss_dice_8: 3.007    time: 0.4447  last_time: 0.4482  data_time: 0.0252  last_data_time: 0.0277   lr: 1e-05  max_mem: 6718M\n",
      "\u001b[32m[07/17 13:27:01 d2.utils.events]: \u001b[0m eta: 1 day, 20:43:27  iter: 1219  total_loss: 87.56  loss_ce: 3.284  loss_mask: 2.03  loss_dice: 2.876  loss_ce_0: 6.889  loss_mask_0: 2.059  loss_dice_0: 3.065  loss_ce_1: 3.133  loss_mask_1: 1.854  loss_dice_1: 2.996  loss_ce_2: 3.198  loss_mask_2: 2.126  loss_dice_2: 3.018  loss_ce_3: 3.27  loss_mask_3: 1.976  loss_dice_3: 2.779  loss_ce_4: 3.351  loss_mask_4: 1.98  loss_dice_4: 2.786  loss_ce_5: 3.212  loss_mask_5: 2.052  loss_dice_5: 2.967  loss_ce_6: 3.487  loss_mask_6: 2.242  loss_dice_6: 2.996  loss_ce_7: 3.347  loss_mask_7: 2.077  loss_dice_7: 3.002  loss_ce_8: 3.289  loss_mask_8: 2.017  loss_dice_8: 2.828    time: 0.4448  last_time: 0.4324  data_time: 0.0250  last_data_time: 0.0187   lr: 1e-05  max_mem: 6718M\n",
      "\u001b[32m[07/17 13:27:10 d2.utils.events]: \u001b[0m eta: 1 day, 20:43:11  iter: 1239  total_loss: 84.52  loss_ce: 2.809  loss_mask: 1.913  loss_dice: 2.889  loss_ce_0: 6.8  loss_mask_0: 2.001  loss_dice_0: 2.932  loss_ce_1: 2.952  loss_mask_1: 1.899  loss_dice_1: 2.832  loss_ce_2: 2.733  loss_mask_2: 1.842  loss_dice_2: 2.912  loss_ce_3: 2.82  loss_mask_3: 1.961  loss_dice_3: 2.892  loss_ce_4: 2.921  loss_mask_4: 2.026  loss_dice_4: 2.883  loss_ce_5: 2.671  loss_mask_5: 2.066  loss_dice_5: 3.02  loss_ce_6: 2.98  loss_mask_6: 2.039  loss_dice_6: 2.928  loss_ce_7: 2.943  loss_mask_7: 1.908  loss_dice_7: 3.083  loss_ce_8: 2.807  loss_mask_8: 2.025  loss_dice_8: 2.954    time: 0.4449  last_time: 0.4336  data_time: 0.0246  last_data_time: 0.0218   lr: 1e-05  max_mem: 6718M\n",
      "\u001b[32m[07/17 13:27:19 d2.utils.events]: \u001b[0m eta: 1 day, 20:43:20  iter: 1259  total_loss: 87.51  loss_ce: 3.285  loss_mask: 1.924  loss_dice: 2.804  loss_ce_0: 6.878  loss_mask_0: 1.805  loss_dice_0: 2.994  loss_ce_1: 3.242  loss_mask_1: 1.829  loss_dice_1: 2.772  loss_ce_2: 3.245  loss_mask_2: 1.85  loss_dice_2: 2.867  loss_ce_3: 3.317  loss_mask_3: 1.892  loss_dice_3: 2.887  loss_ce_4: 3.322  loss_mask_4: 1.956  loss_dice_4: 2.871  loss_ce_5: 3.27  loss_mask_5: 2.004  loss_dice_5: 3.029  loss_ce_6: 3.286  loss_mask_6: 1.865  loss_dice_6: 3.041  loss_ce_7: 3.343  loss_mask_7: 1.96  loss_dice_7: 2.95  loss_ce_8: 3.466  loss_mask_8: 1.848  loss_dice_8: 2.832    time: 0.4450  last_time: 0.4631  data_time: 0.0255  last_data_time: 0.0427   lr: 1e-05  max_mem: 6718M\n",
      "\u001b[32m[07/17 13:27:28 d2.utils.events]: \u001b[0m eta: 1 day, 20:43:41  iter: 1279  total_loss: 88.59  loss_ce: 3.436  loss_mask: 2.021  loss_dice: 3.117  loss_ce_0: 6.875  loss_mask_0: 1.839  loss_dice_0: 3.048  loss_ce_1: 3.462  loss_mask_1: 1.797  loss_dice_1: 2.916  loss_ce_2: 3.578  loss_mask_2: 1.902  loss_dice_2: 2.91  loss_ce_3: 3.563  loss_mask_3: 2.006  loss_dice_3: 2.84  loss_ce_4: 3.441  loss_mask_4: 1.828  loss_dice_4: 2.929  loss_ce_5: 3.623  loss_mask_5: 1.845  loss_dice_5: 2.992  loss_ce_6: 3.685  loss_mask_6: 2.071  loss_dice_6: 3.095  loss_ce_7: 3.499  loss_mask_7: 2.075  loss_dice_7: 2.918  loss_ce_8: 3.474  loss_mask_8: 1.95  loss_dice_8: 2.911    time: 0.4451  last_time: 0.4409  data_time: 0.0256  last_data_time: 0.0226   lr: 1e-05  max_mem: 6718M\n",
      "\u001b[32m[07/17 13:27:37 d2.utils.events]: \u001b[0m eta: 1 day, 20:42:35  iter: 1299  total_loss: 82.56  loss_ce: 2.655  loss_mask: 1.972  loss_dice: 2.982  loss_ce_0: 6.72  loss_mask_0: 1.949  loss_dice_0: 2.934  loss_ce_1: 2.662  loss_mask_1: 1.94  loss_dice_1: 2.756  loss_ce_2: 2.703  loss_mask_2: 2.037  loss_dice_2: 2.748  loss_ce_3: 2.501  loss_mask_3: 1.901  loss_dice_3: 2.878  loss_ce_4: 2.699  loss_mask_4: 2.02  loss_dice_4: 2.612  loss_ce_5: 2.794  loss_mask_5: 1.846  loss_dice_5: 2.811  loss_ce_6: 2.788  loss_mask_6: 2.146  loss_dice_6: 2.976  loss_ce_7: 2.655  loss_mask_7: 2.003  loss_dice_7: 2.794  loss_ce_8: 2.754  loss_mask_8: 1.829  loss_dice_8: 2.709    time: 0.4450  last_time: 0.4535  data_time: 0.0218  last_data_time: 0.0230   lr: 1e-05  max_mem: 6718M\n",
      "\u001b[32m[07/17 13:27:45 d2.utils.events]: \u001b[0m eta: 1 day, 20:42:23  iter: 1319  total_loss: 84.52  loss_ce: 2.433  loss_mask: 2.059  loss_dice: 2.952  loss_ce_0: 6.668  loss_mask_0: 2.006  loss_dice_0: 3.206  loss_ce_1: 2.576  loss_mask_1: 2.135  loss_dice_1: 2.924  loss_ce_2: 2.449  loss_mask_2: 2.49  loss_dice_2: 3.072  loss_ce_3: 2.35  loss_mask_3: 2.207  loss_dice_3: 3.345  loss_ce_4: 2.274  loss_mask_4: 2.495  loss_dice_4: 3.361  loss_ce_5: 2.41  loss_mask_5: 2.086  loss_dice_5: 3.02  loss_ce_6: 2.368  loss_mask_6: 2.552  loss_dice_6: 3.44  loss_ce_7: 2.407  loss_mask_7: 2.114  loss_dice_7: 3.003  loss_ce_8: 2.469  loss_mask_8: 2.224  loss_dice_8: 3.08    time: 0.4449  last_time: 0.4451  data_time: 0.0237  last_data_time: 0.0239   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:27:54 d2.utils.events]: \u001b[0m eta: 1 day, 20:42:14  iter: 1339  total_loss: 85.1  loss_ce: 3.239  loss_mask: 2.01  loss_dice: 3.133  loss_ce_0: 6.652  loss_mask_0: 1.866  loss_dice_0: 2.993  loss_ce_1: 3.132  loss_mask_1: 1.861  loss_dice_1: 2.828  loss_ce_2: 3.185  loss_mask_2: 2.004  loss_dice_2: 2.76  loss_ce_3: 3.383  loss_mask_3: 2.072  loss_dice_3: 2.87  loss_ce_4: 2.968  loss_mask_4: 2.072  loss_dice_4: 3.05  loss_ce_5: 3.175  loss_mask_5: 1.969  loss_dice_5: 3.065  loss_ce_6: 3.109  loss_mask_6: 1.994  loss_dice_6: 2.996  loss_ce_7: 3.03  loss_mask_7: 1.962  loss_dice_7: 2.99  loss_ce_8: 3.265  loss_mask_8: 1.93  loss_dice_8: 2.911    time: 0.4449  last_time: 0.4332  data_time: 0.0238  last_data_time: 0.0239   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:28:03 d2.utils.events]: \u001b[0m eta: 1 day, 20:42:05  iter: 1359  total_loss: 86.87  loss_ce: 3.206  loss_mask: 1.967  loss_dice: 3.327  loss_ce_0: 6.712  loss_mask_0: 1.744  loss_dice_0: 3.322  loss_ce_1: 3.076  loss_mask_1: 1.73  loss_dice_1: 3.228  loss_ce_2: 3.066  loss_mask_2: 2.009  loss_dice_2: 3.255  loss_ce_3: 3.257  loss_mask_3: 1.958  loss_dice_3: 3.198  loss_ce_4: 3.103  loss_mask_4: 1.853  loss_dice_4: 3.196  loss_ce_5: 3.23  loss_mask_5: 1.928  loss_dice_5: 3.28  loss_ce_6: 3.336  loss_mask_6: 1.919  loss_dice_6: 3.335  loss_ce_7: 3.256  loss_mask_7: 1.907  loss_dice_7: 3.443  loss_ce_8: 3.301  loss_mask_8: 1.982  loss_dice_8: 3.329    time: 0.4451  last_time: 0.4404  data_time: 0.0266  last_data_time: 0.0232   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:28:13 d2.utils.events]: \u001b[0m eta: 1 day, 20:42:00  iter: 1379  total_loss: 82.91  loss_ce: 2.874  loss_mask: 2.14  loss_dice: 3.025  loss_ce_0: 6.519  loss_mask_0: 1.886  loss_dice_0: 3.016  loss_ce_1: 2.876  loss_mask_1: 2.094  loss_dice_1: 2.897  loss_ce_2: 2.912  loss_mask_2: 1.95  loss_dice_2: 2.967  loss_ce_3: 2.962  loss_mask_3: 2.235  loss_dice_3: 3.044  loss_ce_4: 2.925  loss_mask_4: 2.266  loss_dice_4: 3.029  loss_ce_5: 2.9  loss_mask_5: 2.287  loss_dice_5: 3.046  loss_ce_6: 3.032  loss_mask_6: 2.152  loss_dice_6: 3.055  loss_ce_7: 2.944  loss_mask_7: 2.322  loss_dice_7: 3.001  loss_ce_8: 2.84  loss_mask_8: 2.343  loss_dice_8: 2.967    time: 0.4452  last_time: 0.4550  data_time: 0.0237  last_data_time: 0.0217   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:28:22 d2.utils.events]: \u001b[0m eta: 1 day, 20:42:09  iter: 1399  total_loss: 84.69  loss_ce: 3.273  loss_mask: 2.14  loss_dice: 2.859  loss_ce_0: 6.636  loss_mask_0: 2.038  loss_dice_0: 2.891  loss_ce_1: 3.37  loss_mask_1: 2.065  loss_dice_1: 2.696  loss_ce_2: 3.196  loss_mask_2: 2.154  loss_dice_2: 2.731  loss_ce_3: 3.311  loss_mask_3: 1.983  loss_dice_3: 2.576  loss_ce_4: 3.36  loss_mask_4: 1.876  loss_dice_4: 2.664  loss_ce_5: 3.353  loss_mask_5: 2.049  loss_dice_5: 2.767  loss_ce_6: 3.282  loss_mask_6: 2.062  loss_dice_6: 2.842  loss_ce_7: 3.248  loss_mask_7: 2.189  loss_dice_7: 2.965  loss_ce_8: 3.26  loss_mask_8: 2.139  loss_dice_8: 2.723    time: 0.4454  last_time: 0.4710  data_time: 0.0319  last_data_time: 0.0201   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:28:31 d2.utils.events]: \u001b[0m eta: 1 day, 20:41:48  iter: 1419  total_loss: 82.87  loss_ce: 2.88  loss_mask: 1.975  loss_dice: 2.721  loss_ce_0: 6.558  loss_mask_0: 1.951  loss_dice_0: 2.828  loss_ce_1: 2.803  loss_mask_1: 1.935  loss_dice_1: 2.704  loss_ce_2: 2.862  loss_mask_2: 2.018  loss_dice_2: 2.765  loss_ce_3: 2.935  loss_mask_3: 2.052  loss_dice_3: 2.644  loss_ce_4: 2.941  loss_mask_4: 1.953  loss_dice_4: 2.775  loss_ce_5: 2.836  loss_mask_5: 1.996  loss_dice_5: 2.853  loss_ce_6: 2.864  loss_mask_6: 2.095  loss_dice_6: 2.823  loss_ce_7: 2.829  loss_mask_7: 2.244  loss_dice_7: 2.909  loss_ce_8: 2.95  loss_mask_8: 2.162  loss_dice_8: 2.78    time: 0.4453  last_time: 0.4388  data_time: 0.0229  last_data_time: 0.0246   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:28:39 d2.utils.events]: \u001b[0m eta: 1 day, 20:41:33  iter: 1439  total_loss: 89.59  loss_ce: 3.417  loss_mask: 2.047  loss_dice: 2.887  loss_ce_0: 6.622  loss_mask_0: 2.085  loss_dice_0: 2.942  loss_ce_1: 3.382  loss_mask_1: 2.09  loss_dice_1: 2.827  loss_ce_2: 3.367  loss_mask_2: 1.927  loss_dice_2: 2.778  loss_ce_3: 3.259  loss_mask_3: 2.07  loss_dice_3: 2.863  loss_ce_4: 3.321  loss_mask_4: 2.051  loss_dice_4: 2.893  loss_ce_5: 3.327  loss_mask_5: 2.19  loss_dice_5: 2.954  loss_ce_6: 3.378  loss_mask_6: 2.204  loss_dice_6: 3.019  loss_ce_7: 3.494  loss_mask_7: 2.052  loss_dice_7: 2.85  loss_ce_8: 3.305  loss_mask_8: 2.16  loss_dice_8: 2.914    time: 0.4453  last_time: 0.4347  data_time: 0.0235  last_data_time: 0.0193   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:28:49 d2.utils.events]: \u001b[0m eta: 1 day, 20:41:21  iter: 1459  total_loss: 85.54  loss_ce: 2.823  loss_mask: 1.922  loss_dice: 3.226  loss_ce_0: 6.443  loss_mask_0: 1.73  loss_dice_0: 3.24  loss_ce_1: 3.045  loss_mask_1: 1.786  loss_dice_1: 3.053  loss_ce_2: 2.932  loss_mask_2: 1.729  loss_dice_2: 2.949  loss_ce_3: 3.032  loss_mask_3: 1.737  loss_dice_3: 2.905  loss_ce_4: 2.981  loss_mask_4: 2.117  loss_dice_4: 3.109  loss_ce_5: 2.88  loss_mask_5: 2.045  loss_dice_5: 3.06  loss_ce_6: 2.874  loss_mask_6: 2.019  loss_dice_6: 3.174  loss_ce_7: 3.031  loss_mask_7: 1.961  loss_dice_7: 3.104  loss_ce_8: 3.03  loss_mask_8: 1.947  loss_dice_8: 3.004    time: 0.4455  last_time: 0.4829  data_time: 0.0254  last_data_time: 0.0264   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:28:58 d2.utils.events]: \u001b[0m eta: 1 day, 20:41:16  iter: 1479  total_loss: 85.94  loss_ce: 3.357  loss_mask: 1.898  loss_dice: 2.851  loss_ce_0: 6.44  loss_mask_0: 1.873  loss_dice_0: 2.938  loss_ce_1: 3.316  loss_mask_1: 1.808  loss_dice_1: 2.725  loss_ce_2: 3.417  loss_mask_2: 2.025  loss_dice_2: 2.782  loss_ce_3: 3.432  loss_mask_3: 1.866  loss_dice_3: 2.871  loss_ce_4: 3.47  loss_mask_4: 1.915  loss_dice_4: 2.771  loss_ce_5: 3.475  loss_mask_5: 2.033  loss_dice_5: 2.947  loss_ce_6: 3.52  loss_mask_6: 2  loss_dice_6: 2.95  loss_ce_7: 3.374  loss_mask_7: 2.007  loss_dice_7: 2.963  loss_ce_8: 3.394  loss_mask_8: 2.106  loss_dice_8: 3.015    time: 0.4456  last_time: 0.4342  data_time: 0.0259  last_data_time: 0.0226   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:29:07 d2.utils.events]: \u001b[0m eta: 1 day, 20:41:09  iter: 1499  total_loss: 81.86  loss_ce: 2.882  loss_mask: 1.947  loss_dice: 2.736  loss_ce_0: 6.498  loss_mask_0: 1.951  loss_dice_0: 2.886  loss_ce_1: 2.808  loss_mask_1: 1.87  loss_dice_1: 2.563  loss_ce_2: 2.996  loss_mask_2: 1.938  loss_dice_2: 2.651  loss_ce_3: 3.024  loss_mask_3: 2.038  loss_dice_3: 2.754  loss_ce_4: 3.125  loss_mask_4: 2.095  loss_dice_4: 2.806  loss_ce_5: 2.982  loss_mask_5: 2.183  loss_dice_5: 2.917  loss_ce_6: 2.752  loss_mask_6: 2.197  loss_dice_6: 2.857  loss_ce_7: 2.892  loss_mask_7: 2.468  loss_dice_7: 2.999  loss_ce_8: 2.923  loss_mask_8: 2.049  loss_dice_8: 2.81    time: 0.4461  last_time: 0.4375  data_time: 0.0227  last_data_time: 0.0201   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:29:16 d2.utils.events]: \u001b[0m eta: 1 day, 20:41:04  iter: 1519  total_loss: 83.76  loss_ce: 3.192  loss_mask: 2.142  loss_dice: 2.812  loss_ce_0: 6.493  loss_mask_0: 2.071  loss_dice_0: 2.913  loss_ce_1: 3.159  loss_mask_1: 1.92  loss_dice_1: 2.8  loss_ce_2: 3.136  loss_mask_2: 2.136  loss_dice_2: 2.805  loss_ce_3: 3.158  loss_mask_3: 2.082  loss_dice_3: 2.722  loss_ce_4: 3.284  loss_mask_4: 2.062  loss_dice_4: 2.866  loss_ce_5: 3.145  loss_mask_5: 2.147  loss_dice_5: 2.788  loss_ce_6: 3.091  loss_mask_6: 2.098  loss_dice_6: 2.997  loss_ce_7: 3.16  loss_mask_7: 2.103  loss_dice_7: 2.785  loss_ce_8: 3.128  loss_mask_8: 1.958  loss_dice_8: 2.791    time: 0.4461  last_time: 0.4344  data_time: 0.0256  last_data_time: 0.0251   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:29:25 d2.utils.events]: \u001b[0m eta: 1 day, 20:40:46  iter: 1539  total_loss: 89.12  loss_ce: 3.606  loss_mask: 2.149  loss_dice: 2.901  loss_ce_0: 6.497  loss_mask_0: 1.918  loss_dice_0: 2.975  loss_ce_1: 3.48  loss_mask_1: 1.78  loss_dice_1: 2.902  loss_ce_2: 3.519  loss_mask_2: 2.171  loss_dice_2: 2.931  loss_ce_3: 3.647  loss_mask_3: 1.979  loss_dice_3: 2.834  loss_ce_4: 3.762  loss_mask_4: 1.936  loss_dice_4: 2.812  loss_ce_5: 3.596  loss_mask_5: 2.01  loss_dice_5: 3.095  loss_ce_6: 3.612  loss_mask_6: 1.99  loss_dice_6: 2.869  loss_ce_7: 3.631  loss_mask_7: 2.225  loss_dice_7: 3.022  loss_ce_8: 3.594  loss_mask_8: 2.032  loss_dice_8: 2.919    time: 0.4460  last_time: 0.4286  data_time: 0.0245  last_data_time: 0.0219   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:29:34 d2.utils.events]: \u001b[0m eta: 1 day, 20:39:49  iter: 1559  total_loss: 79.95  loss_ce: 2.908  loss_mask: 1.945  loss_dice: 2.764  loss_ce_0: 6.321  loss_mask_0: 2.128  loss_dice_0: 3.022  loss_ce_1: 2.823  loss_mask_1: 1.955  loss_dice_1: 2.86  loss_ce_2: 2.962  loss_mask_2: 2.036  loss_dice_2: 2.904  loss_ce_3: 2.961  loss_mask_3: 2.102  loss_dice_3: 2.859  loss_ce_4: 2.888  loss_mask_4: 2.14  loss_dice_4: 2.941  loss_ce_5: 2.958  loss_mask_5: 1.992  loss_dice_5: 2.85  loss_ce_6: 3.006  loss_mask_6: 2.003  loss_dice_6: 2.959  loss_ce_7: 3.049  loss_mask_7: 1.991  loss_dice_7: 3.031  loss_ce_8: 3.07  loss_mask_8: 2.041  loss_dice_8: 3    time: 0.4459  last_time: 0.4519  data_time: 0.0219  last_data_time: 0.0329   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:29:43 d2.utils.events]: \u001b[0m eta: 1 day, 20:39:40  iter: 1579  total_loss: 84.75  loss_ce: 3.091  loss_mask: 2.021  loss_dice: 3.046  loss_ce_0: 6.401  loss_mask_0: 1.936  loss_dice_0: 3.093  loss_ce_1: 3.161  loss_mask_1: 1.693  loss_dice_1: 2.882  loss_ce_2: 3.232  loss_mask_2: 1.993  loss_dice_2: 2.956  loss_ce_3: 3.109  loss_mask_3: 1.943  loss_dice_3: 2.906  loss_ce_4: 3.161  loss_mask_4: 1.798  loss_dice_4: 2.871  loss_ce_5: 3.292  loss_mask_5: 1.919  loss_dice_5: 3.007  loss_ce_6: 3.136  loss_mask_6: 1.804  loss_dice_6: 2.885  loss_ce_7: 3.251  loss_mask_7: 1.932  loss_dice_7: 2.907  loss_ce_8: 3.107  loss_mask_8: 2.036  loss_dice_8: 3.043    time: 0.4463  last_time: 0.4432  data_time: 0.0230  last_data_time: 0.0270   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:29:52 d2.utils.events]: \u001b[0m eta: 1 day, 20:39:15  iter: 1599  total_loss: 84.32  loss_ce: 3.142  loss_mask: 1.927  loss_dice: 3.074  loss_ce_0: 6.33  loss_mask_0: 1.793  loss_dice_0: 3.134  loss_ce_1: 3.208  loss_mask_1: 1.775  loss_dice_1: 2.972  loss_ce_2: 3.197  loss_mask_2: 1.919  loss_dice_2: 3.001  loss_ce_3: 3.109  loss_mask_3: 1.946  loss_dice_3: 3.224  loss_ce_4: 3.126  loss_mask_4: 2.032  loss_dice_4: 3.28  loss_ce_5: 3.175  loss_mask_5: 1.957  loss_dice_5: 3.171  loss_ce_6: 3.106  loss_mask_6: 1.832  loss_dice_6: 3.145  loss_ce_7: 3.152  loss_mask_7: 1.918  loss_dice_7: 3.198  loss_ce_8: 2.997  loss_mask_8: 1.873  loss_dice_8: 3.17    time: 0.4462  last_time: 0.4368  data_time: 0.0219  last_data_time: 0.0205   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:30:01 d2.utils.events]: \u001b[0m eta: 1 day, 20:39:23  iter: 1619  total_loss: 87.95  loss_ce: 3.352  loss_mask: 2.137  loss_dice: 3.053  loss_ce_0: 6.3  loss_mask_0: 2.03  loss_dice_0: 3.058  loss_ce_1: 3.31  loss_mask_1: 1.913  loss_dice_1: 2.955  loss_ce_2: 3.273  loss_mask_2: 2.111  loss_dice_2: 2.968  loss_ce_3: 3.102  loss_mask_3: 2.203  loss_dice_3: 3.107  loss_ce_4: 3.224  loss_mask_4: 2.197  loss_dice_4: 3.131  loss_ce_5: 3.212  loss_mask_5: 2.283  loss_dice_5: 3.292  loss_ce_6: 3.487  loss_mask_6: 2.401  loss_dice_6: 3.391  loss_ce_7: 3.258  loss_mask_7: 1.943  loss_dice_7: 3.07  loss_ce_8: 3.146  loss_mask_8: 2.21  loss_dice_8: 3.134    time: 0.4462  last_time: 0.4381  data_time: 0.0248  last_data_time: 0.0220   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:30:11 d2.utils.events]: \u001b[0m eta: 1 day, 20:38:57  iter: 1639  total_loss: 85.9  loss_ce: 3.298  loss_mask: 1.727  loss_dice: 2.966  loss_ce_0: 6.208  loss_mask_0: 1.809  loss_dice_0: 3.052  loss_ce_1: 3.267  loss_mask_1: 1.715  loss_dice_1: 2.882  loss_ce_2: 3.192  loss_mask_2: 1.922  loss_dice_2: 2.938  loss_ce_3: 3.18  loss_mask_3: 2.093  loss_dice_3: 3.052  loss_ce_4: 3.192  loss_mask_4: 1.999  loss_dice_4: 3.101  loss_ce_5: 3.249  loss_mask_5: 1.965  loss_dice_5: 3.11  loss_ce_6: 3.237  loss_mask_6: 2.046  loss_dice_6: 3.082  loss_ce_7: 3.183  loss_mask_7: 1.949  loss_dice_7: 3.082  loss_ce_8: 3.312  loss_mask_8: 1.837  loss_dice_8: 3.02    time: 0.4468  last_time: 0.4369  data_time: 0.0241  last_data_time: 0.0258   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:30:20 d2.utils.events]: \u001b[0m eta: 1 day, 20:38:34  iter: 1659  total_loss: 82.9  loss_ce: 3.079  loss_mask: 1.951  loss_dice: 2.991  loss_ce_0: 6.163  loss_mask_0: 1.884  loss_dice_0: 2.851  loss_ce_1: 3.013  loss_mask_1: 1.748  loss_dice_1: 2.703  loss_ce_2: 3.106  loss_mask_2: 1.88  loss_dice_2: 2.705  loss_ce_3: 3.158  loss_mask_3: 1.957  loss_dice_3: 2.825  loss_ce_4: 3.235  loss_mask_4: 2.018  loss_dice_4: 2.833  loss_ce_5: 3.26  loss_mask_5: 1.952  loss_dice_5: 2.886  loss_ce_6: 3.235  loss_mask_6: 1.932  loss_dice_6: 2.905  loss_ce_7: 3.107  loss_mask_7: 1.99  loss_dice_7: 3.028  loss_ce_8: 3.079  loss_mask_8: 2.313  loss_dice_8: 2.892    time: 0.4468  last_time: 0.4319  data_time: 0.0241  last_data_time: 0.0236   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:30:29 d2.utils.events]: \u001b[0m eta: 1 day, 20:38:40  iter: 1679  total_loss: 80.91  loss_ce: 2.782  loss_mask: 1.884  loss_dice: 2.871  loss_ce_0: 6.063  loss_mask_0: 1.85  loss_dice_0: 3.007  loss_ce_1: 2.784  loss_mask_1: 1.957  loss_dice_1: 2.951  loss_ce_2: 2.671  loss_mask_2: 1.955  loss_dice_2: 2.847  loss_ce_3: 2.666  loss_mask_3: 1.927  loss_dice_3: 2.801  loss_ce_4: 2.756  loss_mask_4: 2.032  loss_dice_4: 2.943  loss_ce_5: 2.714  loss_mask_5: 2.104  loss_dice_5: 2.886  loss_ce_6: 2.751  loss_mask_6: 1.984  loss_dice_6: 3.04  loss_ce_7: 2.796  loss_mask_7: 1.975  loss_dice_7: 2.95  loss_ce_8: 2.813  loss_mask_8: 2.092  loss_dice_8: 2.883    time: 0.4468  last_time: 0.5393  data_time: 0.0233  last_data_time: 0.0551   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:30:38 d2.utils.events]: \u001b[0m eta: 1 day, 20:38:47  iter: 1699  total_loss: 86.27  loss_ce: 3.428  loss_mask: 1.976  loss_dice: 3.008  loss_ce_0: 6.184  loss_mask_0: 1.826  loss_dice_0: 3.072  loss_ce_1: 3.356  loss_mask_1: 1.727  loss_dice_1: 2.916  loss_ce_2: 3.504  loss_mask_2: 1.734  loss_dice_2: 3.006  loss_ce_3: 3.486  loss_mask_3: 1.778  loss_dice_3: 2.933  loss_ce_4: 3.544  loss_mask_4: 1.704  loss_dice_4: 2.977  loss_ce_5: 3.446  loss_mask_5: 1.91  loss_dice_5: 2.976  loss_ce_6: 3.388  loss_mask_6: 1.906  loss_dice_6: 3.012  loss_ce_7: 3.421  loss_mask_7: 1.952  loss_dice_7: 2.953  loss_ce_8: 3.373  loss_mask_8: 1.934  loss_dice_8: 2.955    time: 0.4467  last_time: 0.4641  data_time: 0.0240  last_data_time: 0.0408   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:30:47 d2.utils.events]: \u001b[0m eta: 1 day, 20:39:16  iter: 1719  total_loss: 83.85  loss_ce: 2.851  loss_mask: 1.963  loss_dice: 3.21  loss_ce_0: 6.093  loss_mask_0: 1.712  loss_dice_0: 3.303  loss_ce_1: 2.961  loss_mask_1: 1.745  loss_dice_1: 2.922  loss_ce_2: 2.909  loss_mask_2: 1.85  loss_dice_2: 3.058  loss_ce_3: 2.926  loss_mask_3: 1.769  loss_dice_3: 3.083  loss_ce_4: 2.98  loss_mask_4: 1.643  loss_dice_4: 3.203  loss_ce_5: 2.907  loss_mask_5: 1.664  loss_dice_5: 3.249  loss_ce_6: 2.933  loss_mask_6: 1.815  loss_dice_6: 3.192  loss_ce_7: 3.023  loss_mask_7: 1.978  loss_dice_7: 3.139  loss_ce_8: 3.005  loss_mask_8: 1.963  loss_dice_8: 3.237    time: 0.4467  last_time: 0.4486  data_time: 0.0229  last_data_time: 0.0207   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:30:56 d2.utils.events]: \u001b[0m eta: 1 day, 20:38:10  iter: 1739  total_loss: 80.83  loss_ce: 3.118  loss_mask: 1.958  loss_dice: 2.975  loss_ce_0: 6.035  loss_mask_0: 1.901  loss_dice_0: 2.971  loss_ce_1: 3.129  loss_mask_1: 1.863  loss_dice_1: 2.702  loss_ce_2: 3.011  loss_mask_2: 1.884  loss_dice_2: 2.696  loss_ce_3: 3.189  loss_mask_3: 1.761  loss_dice_3: 2.876  loss_ce_4: 3.19  loss_mask_4: 1.889  loss_dice_4: 2.871  loss_ce_5: 3.053  loss_mask_5: 2.035  loss_dice_5: 2.884  loss_ce_6: 3.008  loss_mask_6: 2.014  loss_dice_6: 2.84  loss_ce_7: 3.21  loss_mask_7: 2.004  loss_dice_7: 2.991  loss_ce_8: 3.145  loss_mask_8: 2.117  loss_dice_8: 2.938    time: 0.4466  last_time: 0.4355  data_time: 0.0244  last_data_time: 0.0194   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:31:04 d2.utils.events]: \u001b[0m eta: 1 day, 20:37:42  iter: 1759  total_loss: 77.26  loss_ce: 2.419  loss_mask: 2.126  loss_dice: 2.988  loss_ce_0: 5.815  loss_mask_0: 1.812  loss_dice_0: 2.814  loss_ce_1: 2.498  loss_mask_1: 1.813  loss_dice_1: 2.678  loss_ce_2: 2.549  loss_mask_2: 1.895  loss_dice_2: 2.669  loss_ce_3: 2.521  loss_mask_3: 1.818  loss_dice_3: 2.647  loss_ce_4: 2.463  loss_mask_4: 1.914  loss_dice_4: 2.894  loss_ce_5: 2.439  loss_mask_5: 1.951  loss_dice_5: 2.881  loss_ce_6: 2.677  loss_mask_6: 2.016  loss_dice_6: 3.07  loss_ce_7: 2.448  loss_mask_7: 2.063  loss_dice_7: 3.001  loss_ce_8: 2.325  loss_mask_8: 2.185  loss_dice_8: 2.903    time: 0.4465  last_time: 0.4441  data_time: 0.0226  last_data_time: 0.0343   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:31:13 d2.utils.events]: \u001b[0m eta: 1 day, 20:36:53  iter: 1779  total_loss: 84.7  loss_ce: 3.341  loss_mask: 1.932  loss_dice: 2.987  loss_ce_0: 5.995  loss_mask_0: 1.781  loss_dice_0: 2.995  loss_ce_1: 3.413  loss_mask_1: 1.894  loss_dice_1: 2.838  loss_ce_2: 3.182  loss_mask_2: 2.078  loss_dice_2: 2.815  loss_ce_3: 3.19  loss_mask_3: 1.964  loss_dice_3: 2.77  loss_ce_4: 3.201  loss_mask_4: 2.044  loss_dice_4: 2.93  loss_ce_5: 3.297  loss_mask_5: 2.136  loss_dice_5: 2.887  loss_ce_6: 3.364  loss_mask_6: 1.975  loss_dice_6: 2.912  loss_ce_7: 3.246  loss_mask_7: 2.093  loss_dice_7: 2.915  loss_ce_8: 3.271  loss_mask_8: 2.024  loss_dice_8: 3.043    time: 0.4464  last_time: 0.4383  data_time: 0.0227  last_data_time: 0.0241   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:31:22 d2.utils.events]: \u001b[0m eta: 1 day, 20:36:36  iter: 1799  total_loss: 86.37  loss_ce: 3.637  loss_mask: 1.907  loss_dice: 2.853  loss_ce_0: 6  loss_mask_0: 1.773  loss_dice_0: 3.015  loss_ce_1: 3.691  loss_mask_1: 1.79  loss_dice_1: 2.802  loss_ce_2: 3.609  loss_mask_2: 1.901  loss_dice_2: 2.814  loss_ce_3: 3.6  loss_mask_3: 2.065  loss_dice_3: 2.87  loss_ce_4: 3.495  loss_mask_4: 1.955  loss_dice_4: 2.938  loss_ce_5: 3.756  loss_mask_5: 1.929  loss_dice_5: 2.776  loss_ce_6: 3.764  loss_mask_6: 1.834  loss_dice_6: 2.914  loss_ce_7: 3.71  loss_mask_7: 1.853  loss_dice_7: 2.877  loss_ce_8: 3.606  loss_mask_8: 1.822  loss_dice_8: 3.043    time: 0.4464  last_time: 0.4380  data_time: 0.0257  last_data_time: 0.0235   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:31:31 d2.utils.events]: \u001b[0m eta: 1 day, 20:35:39  iter: 1819  total_loss: 79.71  loss_ce: 2.966  loss_mask: 2.049  loss_dice: 2.85  loss_ce_0: 5.985  loss_mask_0: 1.8  loss_dice_0: 3  loss_ce_1: 3.065  loss_mask_1: 1.829  loss_dice_1: 2.796  loss_ce_2: 2.768  loss_mask_2: 1.974  loss_dice_2: 2.758  loss_ce_3: 2.935  loss_mask_3: 2.105  loss_dice_3: 2.6  loss_ce_4: 2.937  loss_mask_4: 1.945  loss_dice_4: 2.796  loss_ce_5: 2.922  loss_mask_5: 1.812  loss_dice_5: 2.929  loss_ce_6: 3.03  loss_mask_6: 1.984  loss_dice_6: 2.951  loss_ce_7: 3.031  loss_mask_7: 1.862  loss_dice_7: 2.736  loss_ce_8: 2.825  loss_mask_8: 1.978  loss_dice_8: 2.813    time: 0.4463  last_time: 0.4350  data_time: 0.0240  last_data_time: 0.0225   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:31:40 d2.utils.events]: \u001b[0m eta: 1 day, 20:35:06  iter: 1839  total_loss: 85.85  loss_ce: 2.996  loss_mask: 2.016  loss_dice: 2.971  loss_ce_0: 5.871  loss_mask_0: 1.868  loss_dice_0: 2.929  loss_ce_1: 3.109  loss_mask_1: 1.809  loss_dice_1: 2.85  loss_ce_2: 2.971  loss_mask_2: 1.981  loss_dice_2: 2.873  loss_ce_3: 3.102  loss_mask_3: 1.973  loss_dice_3: 2.977  loss_ce_4: 3.115  loss_mask_4: 1.796  loss_dice_4: 2.919  loss_ce_5: 3.036  loss_mask_5: 1.748  loss_dice_5: 3.088  loss_ce_6: 3.132  loss_mask_6: 2.012  loss_dice_6: 3.015  loss_ce_7: 3.138  loss_mask_7: 1.688  loss_dice_7: 2.868  loss_ce_8: 2.899  loss_mask_8: 2.004  loss_dice_8: 3.116    time: 0.4463  last_time: 0.4730  data_time: 0.0257  last_data_time: 0.0341   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:31:48 d2.utils.events]: \u001b[0m eta: 1 day, 20:34:22  iter: 1859  total_loss: 82.22  loss_ce: 2.944  loss_mask: 1.905  loss_dice: 2.761  loss_ce_0: 5.744  loss_mask_0: 1.726  loss_dice_0: 2.854  loss_ce_1: 2.798  loss_mask_1: 1.835  loss_dice_1: 2.95  loss_ce_2: 2.781  loss_mask_2: 1.81  loss_dice_2: 2.811  loss_ce_3: 2.788  loss_mask_3: 2.036  loss_dice_3: 2.94  loss_ce_4: 2.688  loss_mask_4: 2.002  loss_dice_4: 2.794  loss_ce_5: 2.844  loss_mask_5: 2.025  loss_dice_5: 3.001  loss_ce_6: 2.908  loss_mask_6: 1.803  loss_dice_6: 2.884  loss_ce_7: 2.89  loss_mask_7: 2.068  loss_dice_7: 2.783  loss_ce_8: 2.792  loss_mask_8: 2.013  loss_dice_8: 2.807    time: 0.4461  last_time: 0.4244  data_time: 0.0218  last_data_time: 0.0191   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:31:57 d2.utils.events]: \u001b[0m eta: 1 day, 20:32:38  iter: 1879  total_loss: 85.55  loss_ce: 3.223  loss_mask: 1.937  loss_dice: 2.945  loss_ce_0: 5.937  loss_mask_0: 1.904  loss_dice_0: 3.009  loss_ce_1: 3.092  loss_mask_1: 2.145  loss_dice_1: 2.766  loss_ce_2: 3.155  loss_mask_2: 1.879  loss_dice_2: 2.821  loss_ce_3: 3.056  loss_mask_3: 2.143  loss_dice_3: 2.888  loss_ce_4: 3.005  loss_mask_4: 2.017  loss_dice_4: 2.999  loss_ce_5: 3.109  loss_mask_5: 2.089  loss_dice_5: 2.897  loss_ce_6: 3.196  loss_mask_6: 1.981  loss_dice_6: 2.864  loss_ce_7: 3.162  loss_mask_7: 2.017  loss_dice_7: 2.978  loss_ce_8: 3.083  loss_mask_8: 1.968  loss_dice_8: 2.798    time: 0.4460  last_time: 0.4358  data_time: 0.0228  last_data_time: 0.0271   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:32:06 d2.utils.events]: \u001b[0m eta: 1 day, 20:32:29  iter: 1899  total_loss: 82.93  loss_ce: 3.042  loss_mask: 1.881  loss_dice: 2.996  loss_ce_0: 5.792  loss_mask_0: 1.922  loss_dice_0: 2.875  loss_ce_1: 2.948  loss_mask_1: 1.691  loss_dice_1: 2.827  loss_ce_2: 2.863  loss_mask_2: 1.868  loss_dice_2: 2.663  loss_ce_3: 2.901  loss_mask_3: 2.132  loss_dice_3: 2.877  loss_ce_4: 2.794  loss_mask_4: 2.147  loss_dice_4: 2.922  loss_ce_5: 2.821  loss_mask_5: 1.837  loss_dice_5: 2.83  loss_ce_6: 2.936  loss_mask_6: 1.776  loss_dice_6: 2.937  loss_ce_7: 2.958  loss_mask_7: 1.854  loss_dice_7: 2.849  loss_ce_8: 2.864  loss_mask_8: 1.918  loss_dice_8: 2.814    time: 0.4459  last_time: 0.4476  data_time: 0.0224  last_data_time: 0.0215   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:32:14 d2.utils.events]: \u001b[0m eta: 1 day, 20:31:56  iter: 1919  total_loss: 84.76  loss_ce: 3.432  loss_mask: 1.993  loss_dice: 2.883  loss_ce_0: 5.715  loss_mask_0: 1.872  loss_dice_0: 2.962  loss_ce_1: 3.22  loss_mask_1: 1.91  loss_dice_1: 2.701  loss_ce_2: 3.256  loss_mask_2: 2.066  loss_dice_2: 2.872  loss_ce_3: 3.359  loss_mask_3: 2.027  loss_dice_3: 2.931  loss_ce_4: 3.344  loss_mask_4: 2.07  loss_dice_4: 2.719  loss_ce_5: 3.496  loss_mask_5: 2.055  loss_dice_5: 2.863  loss_ce_6: 3.483  loss_mask_6: 1.955  loss_dice_6: 2.867  loss_ce_7: 3.468  loss_mask_7: 2.032  loss_dice_7: 2.84  loss_ce_8: 3.464  loss_mask_8: 2.06  loss_dice_8: 3.004    time: 0.4458  last_time: 0.4310  data_time: 0.0225  last_data_time: 0.0211   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:32:23 d2.utils.events]: \u001b[0m eta: 1 day, 20:31:11  iter: 1939  total_loss: 79.59  loss_ce: 2.805  loss_mask: 2.255  loss_dice: 2.953  loss_ce_0: 5.576  loss_mask_0: 2.091  loss_dice_0: 2.88  loss_ce_1: 2.699  loss_mask_1: 2.041  loss_dice_1: 2.672  loss_ce_2: 2.689  loss_mask_2: 2.151  loss_dice_2: 2.882  loss_ce_3: 2.671  loss_mask_3: 2.437  loss_dice_3: 2.826  loss_ce_4: 2.748  loss_mask_4: 2.187  loss_dice_4: 2.974  loss_ce_5: 2.777  loss_mask_5: 2.057  loss_dice_5: 2.839  loss_ce_6: 2.845  loss_mask_6: 2.131  loss_dice_6: 2.906  loss_ce_7: 2.852  loss_mask_7: 2.369  loss_dice_7: 2.985  loss_ce_8: 2.737  loss_mask_8: 2.208  loss_dice_8: 2.805    time: 0.4456  last_time: 0.4205  data_time: 0.0213  last_data_time: 0.0186   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:32:32 d2.utils.events]: \u001b[0m eta: 1 day, 20:30:39  iter: 1959  total_loss: 82.02  loss_ce: 2.958  loss_mask: 2.232  loss_dice: 2.675  loss_ce_0: 5.609  loss_mask_0: 2.05  loss_dice_0: 2.801  loss_ce_1: 2.824  loss_mask_1: 1.956  loss_dice_1: 2.642  loss_ce_2: 2.875  loss_mask_2: 2.131  loss_dice_2: 2.605  loss_ce_3: 2.82  loss_mask_3: 2.283  loss_dice_3: 2.83  loss_ce_4: 2.935  loss_mask_4: 2.109  loss_dice_4: 2.64  loss_ce_5: 2.9  loss_mask_5: 2.186  loss_dice_5: 2.688  loss_ce_6: 3.056  loss_mask_6: 2.186  loss_dice_6: 2.62  loss_ce_7: 2.917  loss_mask_7: 2.258  loss_dice_7: 2.666  loss_ce_8: 2.902  loss_mask_8: 2.2  loss_dice_8: 2.755    time: 0.4455  last_time: 0.4491  data_time: 0.0233  last_data_time: 0.0242   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:32:40 d2.utils.events]: \u001b[0m eta: 1 day, 20:30:06  iter: 1979  total_loss: 78.2  loss_ce: 2.771  loss_mask: 1.969  loss_dice: 2.836  loss_ce_0: 5.533  loss_mask_0: 2.164  loss_dice_0: 2.773  loss_ce_1: 2.766  loss_mask_1: 1.854  loss_dice_1: 2.509  loss_ce_2: 2.75  loss_mask_2: 1.897  loss_dice_2: 2.594  loss_ce_3: 2.703  loss_mask_3: 1.938  loss_dice_3: 2.807  loss_ce_4: 2.705  loss_mask_4: 2.143  loss_dice_4: 2.77  loss_ce_5: 2.777  loss_mask_5: 2.015  loss_dice_5: 2.665  loss_ce_6: 2.912  loss_mask_6: 1.894  loss_dice_6: 2.681  loss_ce_7: 2.898  loss_mask_7: 1.929  loss_dice_7: 2.712  loss_ce_8: 2.895  loss_mask_8: 2.09  loss_dice_8: 2.718    time: 0.4454  last_time: 0.4351  data_time: 0.0215  last_data_time: 0.0191   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:32:49 d2.evaluation.evaluator]: \u001b[0mStart inference on 69 batches\n",
      "\u001b[32m[07/17 13:32:52 d2.evaluation.evaluator]: \u001b[0mInference done 11/69. Dataloading: 0.0802 s/iter. Inference: 0.1330 s/iter. Eval: 0.0339 s/iter. Total: 0.2472 s/iter. ETA=0:00:14\n",
      "\u001b[32m[07/17 13:32:58 d2.evaluation.evaluator]: \u001b[0mInference done 29/69. Dataloading: 0.1122 s/iter. Inference: 0.1358 s/iter. Eval: 0.0341 s/iter. Total: 0.2823 s/iter. ETA=0:00:11\n",
      "\u001b[32m[07/17 13:33:03 d2.evaluation.evaluator]: \u001b[0mInference done 48/69. Dataloading: 0.1103 s/iter. Inference: 0.1347 s/iter. Eval: 0.0341 s/iter. Total: 0.2792 s/iter. ETA=0:00:05\n",
      "\u001b[32m[07/17 13:33:08 d2.evaluation.evaluator]: \u001b[0mInference done 65/69. Dataloading: 0.1170 s/iter. Inference: 0.1349 s/iter. Eval: 0.0341 s/iter. Total: 0.2861 s/iter. ETA=0:00:01\n",
      "\u001b[32m[07/17 13:33:09 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:18.291824 (0.285810 s / iter per device, on 1 devices)\n",
      "\u001b[32m[07/17 13:33:09 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:08 (0.134831 s / iter per device, on 1 devices)\n",
      "Computing IoU\n",
      "Computing IoU\n",
      "{'IoU_0.5': 0.0, 'IoU_0.75': 0.0}\n",
      "\u001b[32m[07/17 13:33:09 d2.engine.defaults]: \u001b[0mEvaluation results for coco_2017_val in csv format:\n",
      "\u001b[32m[07/17 13:33:09 d2.evaluation.testing]: \u001b[0mcopypaste: IoU_0.5=0.0\n",
      "\u001b[32m[07/17 13:33:09 d2.evaluation.testing]: \u001b[0mcopypaste: IoU_0.75=0.0\n",
      "\u001b[32m[07/17 13:33:09 d2.utils.events]: \u001b[0m eta: 1 day, 20:28:01  iter: 1999  total_loss: 80.28  loss_ce: 2.769  loss_mask: 2.303  loss_dice: 3.069  loss_ce_0: 5.52  loss_mask_0: 1.827  loss_dice_0: 2.961  loss_ce_1: 2.702  loss_mask_1: 1.987  loss_dice_1: 2.627  loss_ce_2: 2.734  loss_mask_2: 2.081  loss_dice_2: 2.717  loss_ce_3: 2.684  loss_mask_3: 2.004  loss_dice_3: 2.904  loss_ce_4: 2.63  loss_mask_4: 1.978  loss_dice_4: 2.823  loss_ce_5: 2.784  loss_mask_5: 2.053  loss_dice_5: 2.975  loss_ce_6: 2.817  loss_mask_6: 1.932  loss_dice_6: 2.729  loss_ce_7: 2.781  loss_mask_7: 2.114  loss_dice_7: 2.785  loss_ce_8: 2.799  loss_mask_8: 2.193  loss_dice_8: 2.767    time: 0.4453  last_time: 0.4306  data_time: 0.0218  last_data_time: 0.0223   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:33:19 d2.utils.events]: \u001b[0m eta: 1 day, 20:28:31  iter: 2019  total_loss: 85.96  loss_ce: 3.447  loss_mask: 1.869  loss_dice: 2.866  loss_ce_0: 5.668  loss_mask_0: 1.796  loss_dice_0: 3.037  loss_ce_1: 3.533  loss_mask_1: 1.843  loss_dice_1: 2.907  loss_ce_2: 3.396  loss_mask_2: 1.95  loss_dice_2: 2.896  loss_ce_3: 3.366  loss_mask_3: 1.891  loss_dice_3: 3.072  loss_ce_4: 3.396  loss_mask_4: 1.953  loss_dice_4: 2.922  loss_ce_5: 3.327  loss_mask_5: 1.842  loss_dice_5: 3.078  loss_ce_6: 3.484  loss_mask_6: 1.841  loss_dice_6: 2.911  loss_ce_7: 3.494  loss_mask_7: 1.952  loss_dice_7: 2.834  loss_ce_8: 3.517  loss_mask_8: 1.932  loss_dice_8: 3.045    time: 0.4457  last_time: 0.4401  data_time: 0.0269  last_data_time: 0.0259   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:33:28 d2.utils.events]: \u001b[0m eta: 1 day, 20:28:27  iter: 2039  total_loss: 86.78  loss_ce: 3.373  loss_mask: 1.694  loss_dice: 3.209  loss_ce_0: 5.61  loss_mask_0: 1.769  loss_dice_0: 3.339  loss_ce_1: 3.27  loss_mask_1: 1.831  loss_dice_1: 3.125  loss_ce_2: 3.302  loss_mask_2: 1.82  loss_dice_2: 3.173  loss_ce_3: 3.437  loss_mask_3: 1.769  loss_dice_3: 3.3  loss_ce_4: 3.387  loss_mask_4: 1.758  loss_dice_4: 3.225  loss_ce_5: 3.331  loss_mask_5: 1.846  loss_dice_5: 3.259  loss_ce_6: 3.41  loss_mask_6: 1.727  loss_dice_6: 3.275  loss_ce_7: 3.472  loss_mask_7: 1.821  loss_dice_7: 3.302  loss_ce_8: 3.422  loss_mask_8: 1.82  loss_dice_8: 3.239    time: 0.4456  last_time: 0.5286  data_time: 0.0249  last_data_time: 0.0380   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:33:37 d2.utils.events]: \u001b[0m eta: 1 day, 20:28:18  iter: 2059  total_loss: 82.62  loss_ce: 3.258  loss_mask: 2.035  loss_dice: 2.952  loss_ce_0: 5.394  loss_mask_0: 1.99  loss_dice_0: 2.938  loss_ce_1: 3.23  loss_mask_1: 1.867  loss_dice_1: 2.865  loss_ce_2: 3.143  loss_mask_2: 2.045  loss_dice_2: 2.954  loss_ce_3: 3.122  loss_mask_3: 1.988  loss_dice_3: 2.836  loss_ce_4: 3.187  loss_mask_4: 1.927  loss_dice_4: 2.839  loss_ce_5: 3.146  loss_mask_5: 2.067  loss_dice_5: 2.923  loss_ce_6: 3.247  loss_mask_6: 1.919  loss_dice_6: 2.853  loss_ce_7: 3.171  loss_mask_7: 2.085  loss_dice_7: 2.836  loss_ce_8: 3.17  loss_mask_8: 1.972  loss_dice_8: 2.874    time: 0.4460  last_time: 0.4337  data_time: 0.0279  last_data_time: 0.0227   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:33:46 d2.utils.events]: \u001b[0m eta: 1 day, 20:29:20  iter: 2079  total_loss: 86.47  loss_ce: 3.521  loss_mask: 1.881  loss_dice: 2.847  loss_ce_0: 5.646  loss_mask_0: 1.787  loss_dice_0: 2.974  loss_ce_1: 3.505  loss_mask_1: 1.96  loss_dice_1: 2.761  loss_ce_2: 3.339  loss_mask_2: 1.849  loss_dice_2: 2.832  loss_ce_3: 3.467  loss_mask_3: 1.865  loss_dice_3: 2.853  loss_ce_4: 3.643  loss_mask_4: 1.701  loss_dice_4: 2.771  loss_ce_5: 3.611  loss_mask_5: 1.867  loss_dice_5: 2.854  loss_ce_6: 3.583  loss_mask_6: 1.78  loss_dice_6: 2.826  loss_ce_7: 3.598  loss_mask_7: 1.905  loss_dice_7: 2.84  loss_ce_8: 3.625  loss_mask_8: 1.873  loss_dice_8: 2.752    time: 0.4459  last_time: 0.4292  data_time: 0.0246  last_data_time: 0.0181   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:33:55 d2.utils.events]: \u001b[0m eta: 1 day, 20:29:11  iter: 2099  total_loss: 88.43  loss_ce: 3.447  loss_mask: 2.029  loss_dice: 3.016  loss_ce_0: 5.561  loss_mask_0: 1.857  loss_dice_0: 3.114  loss_ce_1: 3.477  loss_mask_1: 1.695  loss_dice_1: 2.928  loss_ce_2: 3.502  loss_mask_2: 1.752  loss_dice_2: 3.072  loss_ce_3: 3.603  loss_mask_3: 2.091  loss_dice_3: 3.088  loss_ce_4: 3.611  loss_mask_4: 1.775  loss_dice_4: 3.132  loss_ce_5: 3.681  loss_mask_5: 1.773  loss_dice_5: 3.011  loss_ce_6: 3.568  loss_mask_6: 1.7  loss_dice_6: 3.08  loss_ce_7: 3.518  loss_mask_7: 1.867  loss_dice_7: 3.185  loss_ce_8: 3.556  loss_mask_8: 1.726  loss_dice_8: 3.083    time: 0.4458  last_time: 0.4319  data_time: 0.0234  last_data_time: 0.0222   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:34:04 d2.utils.events]: \u001b[0m eta: 1 day, 20:28:31  iter: 2119  total_loss: 81.99  loss_ce: 3.027  loss_mask: 2.24  loss_dice: 2.726  loss_ce_0: 5.42  loss_mask_0: 1.956  loss_dice_0: 2.965  loss_ce_1: 2.96  loss_mask_1: 2.135  loss_dice_1: 2.859  loss_ce_2: 2.87  loss_mask_2: 2.202  loss_dice_2: 2.856  loss_ce_3: 2.772  loss_mask_3: 2.115  loss_dice_3: 2.721  loss_ce_4: 2.953  loss_mask_4: 1.959  loss_dice_4: 2.793  loss_ce_5: 3.049  loss_mask_5: 1.999  loss_dice_5: 2.749  loss_ce_6: 2.953  loss_mask_6: 2.094  loss_dice_6: 2.934  loss_ce_7: 2.957  loss_mask_7: 2.033  loss_dice_7: 2.891  loss_ce_8: 3.025  loss_mask_8: 2.227  loss_dice_8: 3.029    time: 0.4457  last_time: 0.4326  data_time: 0.0219  last_data_time: 0.0222   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:34:12 d2.utils.events]: \u001b[0m eta: 1 day, 20:27:31  iter: 2139  total_loss: 78.91  loss_ce: 2.95  loss_mask: 2.035  loss_dice: 2.683  loss_ce_0: 5.347  loss_mask_0: 1.908  loss_dice_0: 2.867  loss_ce_1: 2.968  loss_mask_1: 1.956  loss_dice_1: 2.779  loss_ce_2: 3.068  loss_mask_2: 1.991  loss_dice_2: 2.856  loss_ce_3: 2.971  loss_mask_3: 1.99  loss_dice_3: 2.79  loss_ce_4: 2.988  loss_mask_4: 1.782  loss_dice_4: 2.616  loss_ce_5: 2.987  loss_mask_5: 2.003  loss_dice_5: 2.705  loss_ce_6: 2.948  loss_mask_6: 2.082  loss_dice_6: 2.815  loss_ce_7: 3.053  loss_mask_7: 1.991  loss_dice_7: 2.924  loss_ce_8: 2.995  loss_mask_8: 2.17  loss_dice_8: 2.796    time: 0.4456  last_time: 0.4279  data_time: 0.0233  last_data_time: 0.0203   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:34:21 d2.utils.events]: \u001b[0m eta: 1 day, 20:27:30  iter: 2159  total_loss: 79.27  loss_ce: 3.318  loss_mask: 2.034  loss_dice: 2.607  loss_ce_0: 5.435  loss_mask_0: 2.14  loss_dice_0: 2.557  loss_ce_1: 3.165  loss_mask_1: 2.171  loss_dice_1: 2.47  loss_ce_2: 3.278  loss_mask_2: 1.998  loss_dice_2: 2.435  loss_ce_3: 3.411  loss_mask_3: 2.013  loss_dice_3: 2.399  loss_ce_4: 3.409  loss_mask_4: 2.031  loss_dice_4: 2.632  loss_ce_5: 3.305  loss_mask_5: 1.956  loss_dice_5: 2.479  loss_ce_6: 3.409  loss_mask_6: 2.141  loss_dice_6: 2.663  loss_ce_7: 3.391  loss_mask_7: 2.02  loss_dice_7: 2.638  loss_ce_8: 3.082  loss_mask_8: 1.958  loss_dice_8: 2.343    time: 0.4456  last_time: 0.4265  data_time: 0.0250  last_data_time: 0.0199   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:34:30 d2.utils.events]: \u001b[0m eta: 1 day, 20:26:26  iter: 2179  total_loss: 81.44  loss_ce: 3.164  loss_mask: 1.971  loss_dice: 2.825  loss_ce_0: 5.279  loss_mask_0: 1.904  loss_dice_0: 2.907  loss_ce_1: 3.043  loss_mask_1: 1.881  loss_dice_1: 2.73  loss_ce_2: 3.295  loss_mask_2: 1.881  loss_dice_2: 2.734  loss_ce_3: 3.122  loss_mask_3: 1.933  loss_dice_3: 2.821  loss_ce_4: 3.228  loss_mask_4: 1.971  loss_dice_4: 2.81  loss_ce_5: 3.079  loss_mask_5: 1.912  loss_dice_5: 3.021  loss_ce_6: 3.193  loss_mask_6: 1.888  loss_dice_6: 2.772  loss_ce_7: 3.213  loss_mask_7: 1.881  loss_dice_7: 2.758  loss_ce_8: 3.26  loss_mask_8: 1.95  loss_dice_8: 2.831    time: 0.4455  last_time: 0.4372  data_time: 0.0233  last_data_time: 0.0201   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:34:39 d2.utils.events]: \u001b[0m eta: 1 day, 20:25:48  iter: 2199  total_loss: 87.12  loss_ce: 3.326  loss_mask: 2.029  loss_dice: 3.03  loss_ce_0: 5.407  loss_mask_0: 1.851  loss_dice_0: 3.003  loss_ce_1: 3.413  loss_mask_1: 1.777  loss_dice_1: 2.844  loss_ce_2: 3.466  loss_mask_2: 1.911  loss_dice_2: 2.993  loss_ce_3: 3.398  loss_mask_3: 2.074  loss_dice_3: 2.962  loss_ce_4: 3.47  loss_mask_4: 1.904  loss_dice_4: 3.024  loss_ce_5: 3.383  loss_mask_5: 1.903  loss_dice_5: 2.951  loss_ce_6: 3.42  loss_mask_6: 1.911  loss_dice_6: 3.047  loss_ce_7: 3.474  loss_mask_7: 1.995  loss_dice_7: 3.023  loss_ce_8: 3.56  loss_mask_8: 1.91  loss_dice_8: 2.962    time: 0.4455  last_time: 0.4663  data_time: 0.0238  last_data_time: 0.0239   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:34:48 d2.utils.events]: \u001b[0m eta: 1 day, 20:25:42  iter: 2219  total_loss: 83.35  loss_ce: 3.301  loss_mask: 1.865  loss_dice: 2.923  loss_ce_0: 5.276  loss_mask_0: 1.877  loss_dice_0: 2.961  loss_ce_1: 3.376  loss_mask_1: 1.868  loss_dice_1: 2.844  loss_ce_2: 3.262  loss_mask_2: 1.787  loss_dice_2: 2.838  loss_ce_3: 3.424  loss_mask_3: 2.069  loss_dice_3: 2.909  loss_ce_4: 3.351  loss_mask_4: 1.972  loss_dice_4: 2.865  loss_ce_5: 3.314  loss_mask_5: 1.877  loss_dice_5: 2.794  loss_ce_6: 3.45  loss_mask_6: 1.755  loss_dice_6: 2.903  loss_ce_7: 3.373  loss_mask_7: 1.873  loss_dice_7: 2.896  loss_ce_8: 3.37  loss_mask_8: 1.839  loss_dice_8: 2.818    time: 0.4455  last_time: 0.4537  data_time: 0.0254  last_data_time: 0.0255   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:34:56 d2.utils.events]: \u001b[0m eta: 1 day, 20:25:30  iter: 2239  total_loss: 82.75  loss_ce: 3.413  loss_mask: 2.018  loss_dice: 2.844  loss_ce_0: 5.363  loss_mask_0: 1.733  loss_dice_0: 2.882  loss_ce_1: 3.33  loss_mask_1: 1.777  loss_dice_1: 2.622  loss_ce_2: 3.306  loss_mask_2: 2.021  loss_dice_2: 2.755  loss_ce_3: 3.512  loss_mask_3: 1.783  loss_dice_3: 2.662  loss_ce_4: 3.417  loss_mask_4: 1.993  loss_dice_4: 2.846  loss_ce_5: 3.274  loss_mask_5: 1.754  loss_dice_5: 2.861  loss_ce_6: 3.45  loss_mask_6: 1.841  loss_dice_6: 2.778  loss_ce_7: 3.454  loss_mask_7: 1.953  loss_dice_7: 2.791  loss_ce_8: 3.401  loss_mask_8: 1.8  loss_dice_8: 2.848    time: 0.4454  last_time: 0.4548  data_time: 0.0244  last_data_time: 0.0204   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:35:05 d2.utils.events]: \u001b[0m eta: 1 day, 20:24:13  iter: 2259  total_loss: 79.81  loss_ce: 2.981  loss_mask: 2.092  loss_dice: 2.767  loss_ce_0: 5.162  loss_mask_0: 2.025  loss_dice_0: 2.794  loss_ce_1: 2.988  loss_mask_1: 1.895  loss_dice_1: 2.778  loss_ce_2: 3.013  loss_mask_2: 2.08  loss_dice_2: 2.743  loss_ce_3: 2.96  loss_mask_3: 1.99  loss_dice_3: 2.593  loss_ce_4: 2.985  loss_mask_4: 2.104  loss_dice_4: 2.691  loss_ce_5: 2.872  loss_mask_5: 2.029  loss_dice_5: 2.765  loss_ce_6: 2.834  loss_mask_6: 2.034  loss_dice_6: 2.691  loss_ce_7: 2.956  loss_mask_7: 1.992  loss_dice_7: 2.838  loss_ce_8: 3.003  loss_mask_8: 1.896  loss_dice_8: 2.734    time: 0.4453  last_time: 0.4353  data_time: 0.0228  last_data_time: 0.0248   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:35:14 d2.utils.events]: \u001b[0m eta: 1 day, 20:23:44  iter: 2279  total_loss: 78.58  loss_ce: 3.136  loss_mask: 1.813  loss_dice: 3.06  loss_ce_0: 5.245  loss_mask_0: 1.837  loss_dice_0: 3.083  loss_ce_1: 3.126  loss_mask_1: 1.631  loss_dice_1: 2.992  loss_ce_2: 3.188  loss_mask_2: 1.638  loss_dice_2: 3.028  loss_ce_3: 3.189  loss_mask_3: 1.824  loss_dice_3: 2.994  loss_ce_4: 3.044  loss_mask_4: 1.865  loss_dice_4: 2.995  loss_ce_5: 3.19  loss_mask_5: 1.809  loss_dice_5: 2.936  loss_ce_6: 3.045  loss_mask_6: 1.642  loss_dice_6: 2.999  loss_ce_7: 3.118  loss_mask_7: 1.716  loss_dice_7: 2.988  loss_ce_8: 3.103  loss_mask_8: 1.629  loss_dice_8: 3.175    time: 0.4453  last_time: 0.4320  data_time: 0.0229  last_data_time: 0.0229   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:35:23 d2.utils.events]: \u001b[0m eta: 1 day, 20:23:44  iter: 2299  total_loss: 81.54  loss_ce: 3.246  loss_mask: 2.089  loss_dice: 2.671  loss_ce_0: 5.24  loss_mask_0: 2.085  loss_dice_0: 2.771  loss_ce_1: 3.254  loss_mask_1: 1.927  loss_dice_1: 2.51  loss_ce_2: 3.28  loss_mask_2: 1.964  loss_dice_2: 2.579  loss_ce_3: 3.161  loss_mask_3: 1.945  loss_dice_3: 2.659  loss_ce_4: 3.246  loss_mask_4: 2.149  loss_dice_4: 2.721  loss_ce_5: 3.132  loss_mask_5: 2.019  loss_dice_5: 2.579  loss_ce_6: 3.144  loss_mask_6: 2.086  loss_dice_6: 2.63  loss_ce_7: 3.226  loss_mask_7: 2.19  loss_dice_7: 2.653  loss_ce_8: 3.246  loss_mask_8: 2.18  loss_dice_8: 2.755    time: 0.4452  last_time: 0.4555  data_time: 0.0231  last_data_time: 0.0405   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:35:32 d2.utils.events]: \u001b[0m eta: 1 day, 20:23:46  iter: 2319  total_loss: 82.88  loss_ce: 3.007  loss_mask: 2.027  loss_dice: 3.027  loss_ce_0: 5.174  loss_mask_0: 1.85  loss_dice_0: 3.116  loss_ce_1: 3.046  loss_mask_1: 2.037  loss_dice_1: 2.924  loss_ce_2: 3.18  loss_mask_2: 1.915  loss_dice_2: 2.979  loss_ce_3: 3.217  loss_mask_3: 2.059  loss_dice_3: 2.805  loss_ce_4: 3.118  loss_mask_4: 1.986  loss_dice_4: 2.924  loss_ce_5: 3.036  loss_mask_5: 1.898  loss_dice_5: 2.926  loss_ce_6: 3.134  loss_mask_6: 1.961  loss_dice_6: 3.061  loss_ce_7: 3.112  loss_mask_7: 1.89  loss_dice_7: 2.828  loss_ce_8: 3.075  loss_mask_8: 1.821  loss_dice_8: 2.937    time: 0.4453  last_time: 0.4742  data_time: 0.0262  last_data_time: 0.0402   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:35:41 d2.utils.events]: \u001b[0m eta: 1 day, 20:23:33  iter: 2339  total_loss: 81.57  loss_ce: 3.003  loss_mask: 1.837  loss_dice: 3.18  loss_ce_0: 5.004  loss_mask_0: 1.813  loss_dice_0: 3.229  loss_ce_1: 3.026  loss_mask_1: 1.698  loss_dice_1: 3.131  loss_ce_2: 3.105  loss_mask_2: 1.82  loss_dice_2: 3.063  loss_ce_3: 3.009  loss_mask_3: 1.705  loss_dice_3: 2.992  loss_ce_4: 3.042  loss_mask_4: 1.701  loss_dice_4: 3.089  loss_ce_5: 3.037  loss_mask_5: 1.707  loss_dice_5: 3.176  loss_ce_6: 2.957  loss_mask_6: 1.721  loss_dice_6: 3.112  loss_ce_7: 2.971  loss_mask_7: 1.857  loss_dice_7: 3.054  loss_ce_8: 3  loss_mask_8: 1.711  loss_dice_8: 3.111    time: 0.4452  last_time: 0.4309  data_time: 0.0227  last_data_time: 0.0209   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:35:49 d2.utils.events]: \u001b[0m eta: 1 day, 20:22:41  iter: 2359  total_loss: 83.9  loss_ce: 3.297  loss_mask: 1.976  loss_dice: 2.857  loss_ce_0: 5.089  loss_mask_0: 1.907  loss_dice_0: 2.975  loss_ce_1: 3.177  loss_mask_1: 1.725  loss_dice_1: 2.771  loss_ce_2: 3.305  loss_mask_2: 1.855  loss_dice_2: 2.663  loss_ce_3: 3.293  loss_mask_3: 1.921  loss_dice_3: 2.788  loss_ce_4: 3.2  loss_mask_4: 1.795  loss_dice_4: 2.827  loss_ce_5: 3.181  loss_mask_5: 1.888  loss_dice_5: 2.863  loss_ce_6: 3.135  loss_mask_6: 1.95  loss_dice_6: 2.899  loss_ce_7: 3.231  loss_mask_7: 2.107  loss_dice_7: 3.123  loss_ce_8: 3.366  loss_mask_8: 1.957  loss_dice_8: 2.789    time: 0.4451  last_time: 0.4347  data_time: 0.0229  last_data_time: 0.0202   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:35:58 d2.utils.events]: \u001b[0m eta: 1 day, 20:22:17  iter: 2379  total_loss: 83.24  loss_ce: 3.44  loss_mask: 1.806  loss_dice: 2.917  loss_ce_0: 5.12  loss_mask_0: 1.729  loss_dice_0: 2.853  loss_ce_1: 3.356  loss_mask_1: 1.674  loss_dice_1: 2.812  loss_ce_2: 3.381  loss_mask_2: 1.871  loss_dice_2: 2.769  loss_ce_3: 3.364  loss_mask_3: 1.821  loss_dice_3: 2.778  loss_ce_4: 3.398  loss_mask_4: 1.86  loss_dice_4: 2.741  loss_ce_5: 3.31  loss_mask_5: 1.887  loss_dice_5: 3.022  loss_ce_6: 3.425  loss_mask_6: 2.019  loss_dice_6: 3.007  loss_ce_7: 3.416  loss_mask_7: 1.848  loss_dice_7: 2.958  loss_ce_8: 3.387  loss_mask_8: 1.916  loss_dice_8: 3.047    time: 0.4451  last_time: 0.4390  data_time: 0.0245  last_data_time: 0.0213   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:36:07 d2.utils.events]: \u001b[0m eta: 1 day, 20:21:40  iter: 2399  total_loss: 83.12  loss_ce: 3.178  loss_mask: 1.881  loss_dice: 2.886  loss_ce_0: 5.142  loss_mask_0: 1.7  loss_dice_0: 2.913  loss_ce_1: 3.242  loss_mask_1: 1.787  loss_dice_1: 2.728  loss_ce_2: 3.146  loss_mask_2: 1.776  loss_dice_2: 2.703  loss_ce_3: 3.256  loss_mask_3: 1.883  loss_dice_3: 2.847  loss_ce_4: 3.219  loss_mask_4: 1.663  loss_dice_4: 2.838  loss_ce_5: 3.379  loss_mask_5: 1.68  loss_dice_5: 2.885  loss_ce_6: 3.199  loss_mask_6: 1.766  loss_dice_6: 2.866  loss_ce_7: 3.308  loss_mask_7: 1.94  loss_dice_7: 3.04  loss_ce_8: 3.338  loss_mask_8: 1.809  loss_dice_8: 2.907    time: 0.4450  last_time: 0.4412  data_time: 0.0235  last_data_time: 0.0247   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:36:16 d2.utils.events]: \u001b[0m eta: 1 day, 20:21:35  iter: 2419  total_loss: 78.69  loss_ce: 2.967  loss_mask: 2.119  loss_dice: 2.78  loss_ce_0: 4.865  loss_mask_0: 2.108  loss_dice_0: 2.842  loss_ce_1: 2.844  loss_mask_1: 1.977  loss_dice_1: 2.651  loss_ce_2: 3.038  loss_mask_2: 2.108  loss_dice_2: 2.632  loss_ce_3: 2.966  loss_mask_3: 2.055  loss_dice_3: 2.645  loss_ce_4: 2.914  loss_mask_4: 2.211  loss_dice_4: 2.856  loss_ce_5: 3.013  loss_mask_5: 2.003  loss_dice_5: 2.801  loss_ce_6: 2.894  loss_mask_6: 2.065  loss_dice_6: 2.819  loss_ce_7: 3.011  loss_mask_7: 1.99  loss_dice_7: 2.761  loss_ce_8: 2.983  loss_mask_8: 2.016  loss_dice_8: 2.657    time: 0.4453  last_time: 0.4377  data_time: 0.0236  last_data_time: 0.0229   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:36:25 d2.utils.events]: \u001b[0m eta: 1 day, 20:21:23  iter: 2439  total_loss: 81.08  loss_ce: 3.171  loss_mask: 1.992  loss_dice: 2.868  loss_ce_0: 4.988  loss_mask_0: 2.071  loss_dice_0: 2.779  loss_ce_1: 3.024  loss_mask_1: 1.83  loss_dice_1: 2.785  loss_ce_2: 2.945  loss_mask_2: 1.984  loss_dice_2: 2.738  loss_ce_3: 2.945  loss_mask_3: 1.933  loss_dice_3: 2.875  loss_ce_4: 2.981  loss_mask_4: 1.968  loss_dice_4: 2.895  loss_ce_5: 3.16  loss_mask_5: 2.06  loss_dice_5: 2.838  loss_ce_6: 3.027  loss_mask_6: 2.176  loss_dice_6: 3.106  loss_ce_7: 3.034  loss_mask_7: 2.052  loss_dice_7: 2.928  loss_ce_8: 3.18  loss_mask_8: 1.91  loss_dice_8: 2.743    time: 0.4453  last_time: 0.4286  data_time: 0.0233  last_data_time: 0.0195   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:36:34 d2.utils.events]: \u001b[0m eta: 1 day, 20:21:08  iter: 2459  total_loss: 86.02  loss_ce: 3.644  loss_mask: 1.859  loss_dice: 2.868  loss_ce_0: 5.12  loss_mask_0: 1.937  loss_dice_0: 2.873  loss_ce_1: 3.6  loss_mask_1: 1.978  loss_dice_1: 2.845  loss_ce_2: 3.731  loss_mask_2: 1.858  loss_dice_2: 2.811  loss_ce_3: 3.614  loss_mask_3: 1.815  loss_dice_3: 2.771  loss_ce_4: 3.63  loss_mask_4: 1.886  loss_dice_4: 2.905  loss_ce_5: 3.637  loss_mask_5: 1.854  loss_dice_5: 2.884  loss_ce_6: 3.554  loss_mask_6: 1.931  loss_dice_6: 3.017  loss_ce_7: 3.622  loss_mask_7: 1.899  loss_dice_7: 2.945  loss_ce_8: 3.68  loss_mask_8: 1.938  loss_dice_8: 2.996    time: 0.4452  last_time: 0.4356  data_time: 0.0234  last_data_time: 0.0250   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:36:43 d2.utils.events]: \u001b[0m eta: 1 day, 20:20:52  iter: 2479  total_loss: 79.53  loss_ce: 2.982  loss_mask: 1.905  loss_dice: 2.894  loss_ce_0: 4.754  loss_mask_0: 1.916  loss_dice_0: 2.88  loss_ce_1: 3.017  loss_mask_1: 1.943  loss_dice_1: 2.764  loss_ce_2: 2.98  loss_mask_2: 1.906  loss_dice_2: 2.749  loss_ce_3: 3.004  loss_mask_3: 1.899  loss_dice_3: 2.698  loss_ce_4: 3.02  loss_mask_4: 1.903  loss_dice_4: 2.786  loss_ce_5: 2.997  loss_mask_5: 1.933  loss_dice_5: 2.772  loss_ce_6: 3.015  loss_mask_6: 2.042  loss_dice_6: 2.94  loss_ce_7: 2.905  loss_mask_7: 1.87  loss_dice_7: 2.748  loss_ce_8: 2.809  loss_mask_8: 1.958  loss_dice_8: 2.868    time: 0.4451  last_time: 0.4600  data_time: 0.0235  last_data_time: 0.0208   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:36:52 d2.utils.events]: \u001b[0m eta: 1 day, 20:20:56  iter: 2499  total_loss: 81.78  loss_ce: 3.551  loss_mask: 1.762  loss_dice: 2.669  loss_ce_0: 5.055  loss_mask_0: 2.013  loss_dice_0: 2.816  loss_ce_1: 3.396  loss_mask_1: 1.957  loss_dice_1: 2.538  loss_ce_2: 3.556  loss_mask_2: 1.897  loss_dice_2: 2.607  loss_ce_3: 3.566  loss_mask_3: 1.84  loss_dice_3: 2.542  loss_ce_4: 3.593  loss_mask_4: 1.871  loss_dice_4: 2.629  loss_ce_5: 3.527  loss_mask_5: 1.8  loss_dice_5: 2.634  loss_ce_6: 3.655  loss_mask_6: 1.957  loss_dice_6: 2.718  loss_ce_7: 3.552  loss_mask_7: 1.985  loss_dice_7: 2.495  loss_ce_8: 3.602  loss_mask_8: 2.074  loss_dice_8: 2.607    time: 0.4451  last_time: 0.4475  data_time: 0.0256  last_data_time: 0.0251   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:37:00 d2.utils.events]: \u001b[0m eta: 1 day, 20:20:28  iter: 2519  total_loss: 84.62  loss_ce: 3.45  loss_mask: 1.99  loss_dice: 2.805  loss_ce_0: 5.015  loss_mask_0: 1.966  loss_dice_0: 2.921  loss_ce_1: 3.309  loss_mask_1: 2.044  loss_dice_1: 2.787  loss_ce_2: 3.363  loss_mask_2: 1.885  loss_dice_2: 2.559  loss_ce_3: 3.423  loss_mask_3: 2.062  loss_dice_3: 2.703  loss_ce_4: 3.416  loss_mask_4: 1.823  loss_dice_4: 2.694  loss_ce_5: 3.432  loss_mask_5: 2.022  loss_dice_5: 2.711  loss_ce_6: 3.508  loss_mask_6: 2.047  loss_dice_6: 2.94  loss_ce_7: 3.439  loss_mask_7: 1.938  loss_dice_7: 2.809  loss_ce_8: 3.518  loss_mask_8: 2.034  loss_dice_8: 2.878    time: 0.4451  last_time: 0.4423  data_time: 0.0245  last_data_time: 0.0213   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:37:09 d2.utils.events]: \u001b[0m eta: 1 day, 20:20:19  iter: 2539  total_loss: 78.49  loss_ce: 3.064  loss_mask: 2.003  loss_dice: 2.762  loss_ce_0: 4.883  loss_mask_0: 2.002  loss_dice_0: 2.751  loss_ce_1: 2.895  loss_mask_1: 1.987  loss_dice_1: 2.671  loss_ce_2: 3.043  loss_mask_2: 1.978  loss_dice_2: 2.598  loss_ce_3: 3.089  loss_mask_3: 1.967  loss_dice_3: 2.749  loss_ce_4: 2.952  loss_mask_4: 1.99  loss_dice_4: 2.589  loss_ce_5: 3.043  loss_mask_5: 1.925  loss_dice_5: 2.789  loss_ce_6: 3.118  loss_mask_6: 1.988  loss_dice_6: 2.824  loss_ce_7: 3.088  loss_mask_7: 2.041  loss_dice_7: 2.679  loss_ce_8: 3.12  loss_mask_8: 2.025  loss_dice_8: 2.67    time: 0.4450  last_time: 0.4317  data_time: 0.0246  last_data_time: 0.0219   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:37:18 d2.utils.events]: \u001b[0m eta: 1 day, 20:20:10  iter: 2559  total_loss: 76.01  loss_ce: 2.826  loss_mask: 2.007  loss_dice: 2.594  loss_ce_0: 4.649  loss_mask_0: 2.201  loss_dice_0: 2.679  loss_ce_1: 2.518  loss_mask_1: 2.293  loss_dice_1: 2.522  loss_ce_2: 2.76  loss_mask_2: 2.094  loss_dice_2: 2.485  loss_ce_3: 2.726  loss_mask_3: 2.22  loss_dice_3: 2.492  loss_ce_4: 2.616  loss_mask_4: 2.084  loss_dice_4: 2.434  loss_ce_5: 2.666  loss_mask_5: 2.089  loss_dice_5: 2.437  loss_ce_6: 2.712  loss_mask_6: 2.142  loss_dice_6: 2.723  loss_ce_7: 2.795  loss_mask_7: 2.064  loss_dice_7: 2.621  loss_ce_8: 2.746  loss_mask_8: 2.075  loss_dice_8: 2.633    time: 0.4449  last_time: 0.4393  data_time: 0.0211  last_data_time: 0.0223   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:37:27 d2.utils.events]: \u001b[0m eta: 1 day, 20:19:57  iter: 2579  total_loss: 81.76  loss_ce: 3.266  loss_mask: 1.882  loss_dice: 2.875  loss_ce_0: 4.872  loss_mask_0: 1.812  loss_dice_0: 2.833  loss_ce_1: 3.117  loss_mask_1: 1.676  loss_dice_1: 2.718  loss_ce_2: 3.347  loss_mask_2: 1.787  loss_dice_2: 2.637  loss_ce_3: 3.293  loss_mask_3: 1.899  loss_dice_3: 2.806  loss_ce_4: 3.328  loss_mask_4: 1.901  loss_dice_4: 2.735  loss_ce_5: 3.277  loss_mask_5: 1.815  loss_dice_5: 2.916  loss_ce_6: 3.274  loss_mask_6: 1.852  loss_dice_6: 2.849  loss_ce_7: 3.314  loss_mask_7: 1.934  loss_dice_7: 2.935  loss_ce_8: 3.313  loss_mask_8: 1.834  loss_dice_8: 2.869    time: 0.4449  last_time: 0.4803  data_time: 0.0232  last_data_time: 0.0300   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:37:36 d2.utils.events]: \u001b[0m eta: 1 day, 20:19:55  iter: 2599  total_loss: 85.26  loss_ce: 3.646  loss_mask: 1.933  loss_dice: 2.612  loss_ce_0: 5.033  loss_mask_0: 1.906  loss_dice_0: 2.715  loss_ce_1: 3.641  loss_mask_1: 1.884  loss_dice_1: 2.685  loss_ce_2: 3.671  loss_mask_2: 1.882  loss_dice_2: 2.636  loss_ce_3: 3.609  loss_mask_3: 1.782  loss_dice_3: 2.595  loss_ce_4: 3.588  loss_mask_4: 1.906  loss_dice_4: 2.649  loss_ce_5: 3.694  loss_mask_5: 1.761  loss_dice_5: 2.67  loss_ce_6: 3.687  loss_mask_6: 1.89  loss_dice_6: 2.755  loss_ce_7: 3.556  loss_mask_7: 1.971  loss_dice_7: 2.689  loss_ce_8: 3.574  loss_mask_8: 1.901  loss_dice_8: 2.716    time: 0.4451  last_time: 0.4485  data_time: 0.0267  last_data_time: 0.0233   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:37:45 d2.utils.events]: \u001b[0m eta: 1 day, 20:19:43  iter: 2619  total_loss: 79.37  loss_ce: 3.288  loss_mask: 1.858  loss_dice: 3.031  loss_ce_0: 4.812  loss_mask_0: 1.707  loss_dice_0: 3.024  loss_ce_1: 3.408  loss_mask_1: 1.653  loss_dice_1: 2.698  loss_ce_2: 3.34  loss_mask_2: 1.71  loss_dice_2: 2.87  loss_ce_3: 3.418  loss_mask_3: 1.763  loss_dice_3: 2.71  loss_ce_4: 3.404  loss_mask_4: 1.824  loss_dice_4: 2.86  loss_ce_5: 3.372  loss_mask_5: 1.594  loss_dice_5: 2.823  loss_ce_6: 3.332  loss_mask_6: 1.818  loss_dice_6: 3.096  loss_ce_7: 3.319  loss_mask_7: 1.654  loss_dice_7: 2.892  loss_ce_8: 3.416  loss_mask_8: 1.597  loss_dice_8: 2.841    time: 0.4451  last_time: 0.4357  data_time: 0.0232  last_data_time: 0.0254   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:37:54 d2.utils.events]: \u001b[0m eta: 1 day, 20:19:24  iter: 2639  total_loss: 77.66  loss_ce: 3  loss_mask: 1.973  loss_dice: 2.663  loss_ce_0: 4.668  loss_mask_0: 1.734  loss_dice_0: 2.828  loss_ce_1: 2.916  loss_mask_1: 1.897  loss_dice_1: 2.642  loss_ce_2: 2.971  loss_mask_2: 1.805  loss_dice_2: 2.659  loss_ce_3: 2.947  loss_mask_3: 2.067  loss_dice_3: 2.701  loss_ce_4: 2.946  loss_mask_4: 1.886  loss_dice_4: 2.564  loss_ce_5: 2.936  loss_mask_5: 1.901  loss_dice_5: 2.653  loss_ce_6: 3.086  loss_mask_6: 1.712  loss_dice_6: 2.825  loss_ce_7: 2.88  loss_mask_7: 1.931  loss_dice_7: 2.681  loss_ce_8: 3.003  loss_mask_8: 1.943  loss_dice_8: 2.676    time: 0.4450  last_time: 0.4385  data_time: 0.0222  last_data_time: 0.0289   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:38:03 d2.utils.events]: \u001b[0m eta: 1 day, 20:19:26  iter: 2659  total_loss: 81.33  loss_ce: 3.321  loss_mask: 1.659  loss_dice: 2.77  loss_ce_0: 4.783  loss_mask_0: 1.787  loss_dice_0: 2.854  loss_ce_1: 3.327  loss_mask_1: 1.733  loss_dice_1: 2.639  loss_ce_2: 3.473  loss_mask_2: 1.629  loss_dice_2: 2.578  loss_ce_3: 3.404  loss_mask_3: 1.552  loss_dice_3: 2.662  loss_ce_4: 3.378  loss_mask_4: 1.675  loss_dice_4: 2.739  loss_ce_5: 3.443  loss_mask_5: 1.693  loss_dice_5: 2.717  loss_ce_6: 3.441  loss_mask_6: 1.609  loss_dice_6: 2.745  loss_ce_7: 3.437  loss_mask_7: 1.682  loss_dice_7: 2.839  loss_ce_8: 3.409  loss_mask_8: 1.814  loss_dice_8: 2.771    time: 0.4450  last_time: 0.4664  data_time: 0.0235  last_data_time: 0.0224   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:38:11 d2.utils.events]: \u001b[0m eta: 1 day, 20:18:51  iter: 2679  total_loss: 77.99  loss_ce: 2.622  loss_mask: 2.193  loss_dice: 2.543  loss_ce_0: 4.513  loss_mask_0: 2.129  loss_dice_0: 2.643  loss_ce_1: 2.624  loss_mask_1: 2.349  loss_dice_1: 2.507  loss_ce_2: 2.791  loss_mask_2: 2.22  loss_dice_2: 2.413  loss_ce_3: 2.716  loss_mask_3: 2.131  loss_dice_3: 2.628  loss_ce_4: 2.513  loss_mask_4: 2.369  loss_dice_4: 2.622  loss_ce_5: 2.557  loss_mask_5: 2.259  loss_dice_5: 2.775  loss_ce_6: 2.853  loss_mask_6: 2.128  loss_dice_6: 2.689  loss_ce_7: 2.726  loss_mask_7: 1.994  loss_dice_7: 2.68  loss_ce_8: 2.816  loss_mask_8: 2.34  loss_dice_8: 2.676    time: 0.4449  last_time: 0.4364  data_time: 0.0214  last_data_time: 0.0244   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:38:20 d2.utils.events]: \u001b[0m eta: 1 day, 20:18:35  iter: 2699  total_loss: 81.02  loss_ce: 3.414  loss_mask: 1.914  loss_dice: 2.766  loss_ce_0: 4.713  loss_mask_0: 1.859  loss_dice_0: 2.865  loss_ce_1: 3.193  loss_mask_1: 1.908  loss_dice_1: 2.702  loss_ce_2: 3.288  loss_mask_2: 2.08  loss_dice_2: 2.827  loss_ce_3: 3.42  loss_mask_3: 1.827  loss_dice_3: 2.795  loss_ce_4: 3.219  loss_mask_4: 1.977  loss_dice_4: 2.866  loss_ce_5: 3.272  loss_mask_5: 1.971  loss_dice_5: 2.694  loss_ce_6: 3.329  loss_mask_6: 2.027  loss_dice_6: 2.938  loss_ce_7: 3.454  loss_mask_7: 1.895  loss_dice_7: 2.814  loss_ce_8: 3.358  loss_mask_8: 1.825  loss_dice_8: 2.812    time: 0.4449  last_time: 0.4349  data_time: 0.0247  last_data_time: 0.0248   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:38:29 d2.utils.events]: \u001b[0m eta: 1 day, 20:18:17  iter: 2719  total_loss: 85.47  loss_ce: 3.318  loss_mask: 2.192  loss_dice: 2.836  loss_ce_0: 4.775  loss_mask_0: 1.932  loss_dice_0: 2.8  loss_ce_1: 3.263  loss_mask_1: 2.149  loss_dice_1: 2.723  loss_ce_2: 3.32  loss_mask_2: 2.117  loss_dice_2: 2.702  loss_ce_3: 3.311  loss_mask_3: 2.143  loss_dice_3: 2.757  loss_ce_4: 3.255  loss_mask_4: 2.142  loss_dice_4: 2.851  loss_ce_5: 3.339  loss_mask_5: 2.122  loss_dice_5: 2.816  loss_ce_6: 3.347  loss_mask_6: 2.068  loss_dice_6: 2.931  loss_ce_7: 3.312  loss_mask_7: 1.906  loss_dice_7: 2.742  loss_ce_8: 3.266  loss_mask_8: 2.056  loss_dice_8: 2.814    time: 0.4448  last_time: 0.4315  data_time: 0.0247  last_data_time: 0.0239   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:38:38 d2.utils.events]: \u001b[0m eta: 1 day, 20:18:11  iter: 2739  total_loss: 85  loss_ce: 3.582  loss_mask: 1.821  loss_dice: 2.951  loss_ce_0: 4.676  loss_mask_0: 1.799  loss_dice_0: 3.091  loss_ce_1: 3.618  loss_mask_1: 1.927  loss_dice_1: 2.863  loss_ce_2: 3.534  loss_mask_2: 1.751  loss_dice_2: 2.823  loss_ce_3: 3.634  loss_mask_3: 1.851  loss_dice_3: 2.871  loss_ce_4: 3.622  loss_mask_4: 1.831  loss_dice_4: 2.834  loss_ce_5: 3.7  loss_mask_5: 1.754  loss_dice_5: 2.76  loss_ce_6: 3.538  loss_mask_6: 1.826  loss_dice_6: 3.09  loss_ce_7: 3.511  loss_mask_7: 1.877  loss_dice_7: 2.965  loss_ce_8: 3.463  loss_mask_8: 2.055  loss_dice_8: 3.005    time: 0.4448  last_time: 0.4357  data_time: 0.0236  last_data_time: 0.0229   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:38:46 d2.utils.events]: \u001b[0m eta: 1 day, 20:18:16  iter: 2759  total_loss: 77.53  loss_ce: 2.975  loss_mask: 2.166  loss_dice: 2.629  loss_ce_0: 4.474  loss_mask_0: 1.868  loss_dice_0: 2.812  loss_ce_1: 2.885  loss_mask_1: 1.985  loss_dice_1: 2.62  loss_ce_2: 2.949  loss_mask_2: 1.803  loss_dice_2: 2.706  loss_ce_3: 2.839  loss_mask_3: 1.957  loss_dice_3: 2.759  loss_ce_4: 3.047  loss_mask_4: 1.962  loss_dice_4: 2.708  loss_ce_5: 3.006  loss_mask_5: 1.934  loss_dice_5: 2.721  loss_ce_6: 2.899  loss_mask_6: 2.003  loss_dice_6: 2.904  loss_ce_7: 2.994  loss_mask_7: 1.951  loss_dice_7: 2.843  loss_ce_8: 2.935  loss_mask_8: 1.913  loss_dice_8: 2.63    time: 0.4447  last_time: 0.4400  data_time: 0.0232  last_data_time: 0.0244   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:38:55 d2.utils.events]: \u001b[0m eta: 1 day, 20:18:00  iter: 2779  total_loss: 78.46  loss_ce: 3.115  loss_mask: 2.099  loss_dice: 2.668  loss_ce_0: 4.477  loss_mask_0: 1.887  loss_dice_0: 2.806  loss_ce_1: 3.193  loss_mask_1: 1.741  loss_dice_1: 2.629  loss_ce_2: 3.146  loss_mask_2: 2.059  loss_dice_2: 2.633  loss_ce_3: 3.123  loss_mask_3: 1.958  loss_dice_3: 2.695  loss_ce_4: 3.085  loss_mask_4: 2.025  loss_dice_4: 2.754  loss_ce_5: 3.141  loss_mask_5: 1.884  loss_dice_5: 2.719  loss_ce_6: 3.09  loss_mask_6: 2.079  loss_dice_6: 2.87  loss_ce_7: 3.243  loss_mask_7: 2.066  loss_dice_7: 2.74  loss_ce_8: 3.082  loss_mask_8: 1.98  loss_dice_8: 2.889    time: 0.4446  last_time: 0.4345  data_time: 0.0220  last_data_time: 0.0230   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:39:04 d2.utils.events]: \u001b[0m eta: 1 day, 20:17:19  iter: 2799  total_loss: 73.42  loss_ce: 2.507  loss_mask: 2.037  loss_dice: 2.89  loss_ce_0: 4.101  loss_mask_0: 1.886  loss_dice_0: 2.759  loss_ce_1: 2.49  loss_mask_1: 1.773  loss_dice_1: 2.574  loss_ce_2: 2.364  loss_mask_2: 1.817  loss_dice_2: 2.651  loss_ce_3: 2.422  loss_mask_3: 2.015  loss_dice_3: 2.69  loss_ce_4: 2.419  loss_mask_4: 1.951  loss_dice_4: 2.811  loss_ce_5: 2.538  loss_mask_5: 1.694  loss_dice_5: 2.727  loss_ce_6: 2.461  loss_mask_6: 1.919  loss_dice_6: 2.661  loss_ce_7: 2.332  loss_mask_7: 2.056  loss_dice_7: 2.884  loss_ce_8: 2.305  loss_mask_8: 1.951  loss_dice_8: 2.657    time: 0.4446  last_time: 0.4239  data_time: 0.0212  last_data_time: 0.0178   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:39:13 d2.utils.events]: \u001b[0m eta: 1 day, 20:17:12  iter: 2819  total_loss: 81.06  loss_ce: 3.261  loss_mask: 2.083  loss_dice: 2.624  loss_ce_0: 4.471  loss_mask_0: 1.955  loss_dice_0: 2.719  loss_ce_1: 3.317  loss_mask_1: 2.156  loss_dice_1: 2.688  loss_ce_2: 3.291  loss_mask_2: 2.131  loss_dice_2: 2.647  loss_ce_3: 3.125  loss_mask_3: 2.15  loss_dice_3: 2.789  loss_ce_4: 3.224  loss_mask_4: 1.827  loss_dice_4: 2.827  loss_ce_5: 3.394  loss_mask_5: 2.09  loss_dice_5: 2.702  loss_ce_6: 3.353  loss_mask_6: 1.932  loss_dice_6: 2.587  loss_ce_7: 3.365  loss_mask_7: 2  loss_dice_7: 2.627  loss_ce_8: 3.277  loss_mask_8: 2.076  loss_dice_8: 2.695    time: 0.4445  last_time: 0.4427  data_time: 0.0246  last_data_time: 0.0251   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:39:21 d2.utils.events]: \u001b[0m eta: 1 day, 20:17:02  iter: 2839  total_loss: 81.92  loss_ce: 3.2  loss_mask: 1.939  loss_dice: 2.734  loss_ce_0: 4.365  loss_mask_0: 1.933  loss_dice_0: 2.769  loss_ce_1: 3.033  loss_mask_1: 1.965  loss_dice_1: 2.578  loss_ce_2: 3.092  loss_mask_2: 2.005  loss_dice_2: 2.791  loss_ce_3: 3.141  loss_mask_3: 1.968  loss_dice_3: 2.736  loss_ce_4: 3.13  loss_mask_4: 2.094  loss_dice_4: 2.735  loss_ce_5: 3.21  loss_mask_5: 2.029  loss_dice_5: 2.747  loss_ce_6: 3.094  loss_mask_6: 1.978  loss_dice_6: 2.89  loss_ce_7: 3.129  loss_mask_7: 2.212  loss_dice_7: 2.788  loss_ce_8: 3.056  loss_mask_8: 2.1  loss_dice_8: 2.839    time: 0.4445  last_time: 0.4410  data_time: 0.0235  last_data_time: 0.0261   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:39:30 d2.utils.events]: \u001b[0m eta: 1 day, 20:17:01  iter: 2859  total_loss: 79.48  loss_ce: 2.797  loss_mask: 2.147  loss_dice: 2.575  loss_ce_0: 4.501  loss_mask_0: 1.997  loss_dice_0: 2.585  loss_ce_1: 2.706  loss_mask_1: 2.086  loss_dice_1: 2.503  loss_ce_2: 2.936  loss_mask_2: 1.925  loss_dice_2: 2.383  loss_ce_3: 2.894  loss_mask_3: 2.039  loss_dice_3: 2.356  loss_ce_4: 2.897  loss_mask_4: 2.217  loss_dice_4: 2.497  loss_ce_5: 2.786  loss_mask_5: 2.171  loss_dice_5: 2.689  loss_ce_6: 2.754  loss_mask_6: 2.281  loss_dice_6: 2.68  loss_ce_7: 2.801  loss_mask_7: 2.13  loss_dice_7: 2.642  loss_ce_8: 2.844  loss_mask_8: 2.343  loss_dice_8: 2.619    time: 0.4445  last_time: 0.4605  data_time: 0.0233  last_data_time: 0.0340   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:39:39 d2.utils.events]: \u001b[0m eta: 1 day, 20:17:07  iter: 2879  total_loss: 82.41  loss_ce: 3.33  loss_mask: 2.061  loss_dice: 3.021  loss_ce_0: 4.404  loss_mask_0: 1.898  loss_dice_0: 3.033  loss_ce_1: 3.183  loss_mask_1: 1.936  loss_dice_1: 2.97  loss_ce_2: 3.243  loss_mask_2: 1.92  loss_dice_2: 2.934  loss_ce_3: 3.332  loss_mask_3: 1.844  loss_dice_3: 2.909  loss_ce_4: 3.335  loss_mask_4: 1.829  loss_dice_4: 2.993  loss_ce_5: 3.356  loss_mask_5: 1.953  loss_dice_5: 2.955  loss_ce_6: 3.318  loss_mask_6: 2.144  loss_dice_6: 3.016  loss_ce_7: 3.319  loss_mask_7: 2.1  loss_dice_7: 3.026  loss_ce_8: 3.388  loss_mask_8: 2.001  loss_dice_8: 2.935    time: 0.4444  last_time: 0.4364  data_time: 0.0230  last_data_time: 0.0248   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:39:48 d2.utils.events]: \u001b[0m eta: 1 day, 20:16:56  iter: 2899  total_loss: 81.86  loss_ce: 3.098  loss_mask: 1.89  loss_dice: 2.875  loss_ce_0: 4.427  loss_mask_0: 1.807  loss_dice_0: 2.976  loss_ce_1: 3.134  loss_mask_1: 1.635  loss_dice_1: 2.829  loss_ce_2: 3.205  loss_mask_2: 1.746  loss_dice_2: 2.84  loss_ce_3: 3.139  loss_mask_3: 1.684  loss_dice_3: 2.985  loss_ce_4: 3.191  loss_mask_4: 1.877  loss_dice_4: 2.751  loss_ce_5: 2.953  loss_mask_5: 1.818  loss_dice_5: 2.74  loss_ce_6: 3.131  loss_mask_6: 1.799  loss_dice_6: 2.956  loss_ce_7: 3.134  loss_mask_7: 1.874  loss_dice_7: 2.977  loss_ce_8: 3.221  loss_mask_8: 1.745  loss_dice_8: 2.845    time: 0.4443  last_time: 0.4353  data_time: 0.0232  last_data_time: 0.0216   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:39:57 d2.utils.events]: \u001b[0m eta: 1 day, 20:17:05  iter: 2919  total_loss: 75.89  loss_ce: 2.903  loss_mask: 1.884  loss_dice: 2.823  loss_ce_0: 4.208  loss_mask_0: 1.852  loss_dice_0: 2.855  loss_ce_1: 2.781  loss_mask_1: 1.66  loss_dice_1: 2.575  loss_ce_2: 2.919  loss_mask_2: 1.606  loss_dice_2: 2.716  loss_ce_3: 2.951  loss_mask_3: 1.725  loss_dice_3: 2.735  loss_ce_4: 2.879  loss_mask_4: 1.776  loss_dice_4: 2.672  loss_ce_5: 2.842  loss_mask_5: 1.88  loss_dice_5: 2.637  loss_ce_6: 2.944  loss_mask_6: 1.728  loss_dice_6: 2.845  loss_ce_7: 3.029  loss_mask_7: 1.696  loss_dice_7: 2.824  loss_ce_8: 3.063  loss_mask_8: 1.834  loss_dice_8: 2.831    time: 0.4446  last_time: 0.4399  data_time: 0.0242  last_data_time: 0.0262   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:40:06 d2.utils.events]: \u001b[0m eta: 1 day, 20:17:41  iter: 2939  total_loss: 83.38  loss_ce: 3.484  loss_mask: 2.12  loss_dice: 2.985  loss_ce_0: 4.56  loss_mask_0: 1.912  loss_dice_0: 2.765  loss_ce_1: 3.658  loss_mask_1: 1.708  loss_dice_1: 2.633  loss_ce_2: 3.555  loss_mask_2: 1.916  loss_dice_2: 2.666  loss_ce_3: 3.397  loss_mask_3: 2.076  loss_dice_3: 2.853  loss_ce_4: 3.401  loss_mask_4: 2.094  loss_dice_4: 2.811  loss_ce_5: 3.507  loss_mask_5: 1.924  loss_dice_5: 2.794  loss_ce_6: 3.647  loss_mask_6: 1.928  loss_dice_6: 2.87  loss_ce_7: 3.499  loss_mask_7: 1.999  loss_dice_7: 2.946  loss_ce_8: 3.497  loss_mask_8: 1.806  loss_dice_8: 2.667    time: 0.4445  last_time: 0.4309  data_time: 0.0240  last_data_time: 0.0249   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:40:15 d2.utils.events]: \u001b[0m eta: 1 day, 20:17:46  iter: 2959  total_loss: 80.46  loss_ce: 3.04  loss_mask: 1.944  loss_dice: 2.937  loss_ce_0: 4.277  loss_mask_0: 1.801  loss_dice_0: 2.741  loss_ce_1: 3.128  loss_mask_1: 1.659  loss_dice_1: 2.653  loss_ce_2: 3.018  loss_mask_2: 1.767  loss_dice_2: 2.634  loss_ce_3: 3.08  loss_mask_3: 1.758  loss_dice_3: 2.581  loss_ce_4: 3.193  loss_mask_4: 1.573  loss_dice_4: 2.66  loss_ce_5: 3.116  loss_mask_5: 1.812  loss_dice_5: 2.609  loss_ce_6: 3.066  loss_mask_6: 1.857  loss_dice_6: 2.991  loss_ce_7: 3.085  loss_mask_7: 1.799  loss_dice_7: 2.787  loss_ce_8: 3.076  loss_mask_8: 1.953  loss_dice_8: 2.917    time: 0.4446  last_time: 0.4280  data_time: 0.0240  last_data_time: 0.0225   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:40:24 d2.utils.events]: \u001b[0m eta: 1 day, 20:17:31  iter: 2979  total_loss: 78.07  loss_ce: 3.014  loss_mask: 1.977  loss_dice: 2.647  loss_ce_0: 4.315  loss_mask_0: 1.854  loss_dice_0: 2.815  loss_ce_1: 3.099  loss_mask_1: 1.658  loss_dice_1: 2.586  loss_ce_2: 3.073  loss_mask_2: 1.765  loss_dice_2: 2.53  loss_ce_3: 3.002  loss_mask_3: 2.048  loss_dice_3: 2.831  loss_ce_4: 2.947  loss_mask_4: 1.852  loss_dice_4: 2.84  loss_ce_5: 3.017  loss_mask_5: 2.036  loss_dice_5: 2.968  loss_ce_6: 2.984  loss_mask_6: 1.899  loss_dice_6: 2.787  loss_ce_7: 3.015  loss_mask_7: 1.709  loss_dice_7: 2.871  loss_ce_8: 2.986  loss_mask_8: 1.948  loss_dice_8: 2.717    time: 0.4445  last_time: 0.4474  data_time: 0.0224  last_data_time: 0.0188   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:40:32 d2.utils.events]: \u001b[0m eta: 1 day, 20:17:32  iter: 2999  total_loss: 79.79  loss_ce: 3.412  loss_mask: 1.914  loss_dice: 2.644  loss_ce_0: 4.459  loss_mask_0: 1.822  loss_dice_0: 2.805  loss_ce_1: 3.353  loss_mask_1: 1.788  loss_dice_1: 2.487  loss_ce_2: 3.37  loss_mask_2: 1.865  loss_dice_2: 2.45  loss_ce_3: 3.276  loss_mask_3: 1.954  loss_dice_3: 2.593  loss_ce_4: 3.507  loss_mask_4: 1.834  loss_dice_4: 2.772  loss_ce_5: 3.21  loss_mask_5: 1.897  loss_dice_5: 2.983  loss_ce_6: 3.299  loss_mask_6: 1.79  loss_dice_6: 2.615  loss_ce_7: 3.34  loss_mask_7: 1.72  loss_dice_7: 2.562  loss_ce_8: 3.425  loss_mask_8: 1.896  loss_dice_8: 2.611    time: 0.4444  last_time: 0.4392  data_time: 0.0222  last_data_time: 0.0238   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:40:41 d2.utils.events]: \u001b[0m eta: 1 day, 20:16:06  iter: 3019  total_loss: 82.81  loss_ce: 3.259  loss_mask: 1.846  loss_dice: 2.901  loss_ce_0: 4.297  loss_mask_0: 1.81  loss_dice_0: 2.919  loss_ce_1: 3.187  loss_mask_1: 1.916  loss_dice_1: 2.623  loss_ce_2: 3.176  loss_mask_2: 1.996  loss_dice_2: 2.873  loss_ce_3: 3.33  loss_mask_3: 1.725  loss_dice_3: 2.897  loss_ce_4: 3.38  loss_mask_4: 1.991  loss_dice_4: 2.885  loss_ce_5: 3.31  loss_mask_5: 1.979  loss_dice_5: 2.934  loss_ce_6: 3.258  loss_mask_6: 1.934  loss_dice_6: 2.899  loss_ce_7: 3.324  loss_mask_7: 1.815  loss_dice_7: 2.912  loss_ce_8: 3.232  loss_mask_8: 1.897  loss_dice_8: 2.932    time: 0.4443  last_time: 0.4259  data_time: 0.0238  last_data_time: 0.0230   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:40:50 d2.utils.events]: \u001b[0m eta: 1 day, 20:15:19  iter: 3039  total_loss: 82.59  loss_ce: 3.529  loss_mask: 1.815  loss_dice: 2.923  loss_ce_0: 4.491  loss_mask_0: 1.703  loss_dice_0: 2.969  loss_ce_1: 3.591  loss_mask_1: 1.877  loss_dice_1: 2.823  loss_ce_2: 3.457  loss_mask_2: 1.888  loss_dice_2: 2.855  loss_ce_3: 3.525  loss_mask_3: 1.802  loss_dice_3: 2.834  loss_ce_4: 3.54  loss_mask_4: 1.823  loss_dice_4: 2.962  loss_ce_5: 3.504  loss_mask_5: 1.833  loss_dice_5: 2.868  loss_ce_6: 3.345  loss_mask_6: 1.912  loss_dice_6: 3.071  loss_ce_7: 3.596  loss_mask_7: 1.755  loss_dice_7: 2.85  loss_ce_8: 3.587  loss_mask_8: 1.852  loss_dice_8: 2.891    time: 0.4443  last_time: 0.4302  data_time: 0.0227  last_data_time: 0.0234   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:40:58 d2.utils.events]: \u001b[0m eta: 1 day, 20:13:59  iter: 3059  total_loss: 80.14  loss_ce: 3.165  loss_mask: 2.064  loss_dice: 2.731  loss_ce_0: 4.174  loss_mask_0: 1.983  loss_dice_0: 2.774  loss_ce_1: 3.116  loss_mask_1: 1.915  loss_dice_1: 2.598  loss_ce_2: 3.179  loss_mask_2: 1.946  loss_dice_2: 2.637  loss_ce_3: 3.245  loss_mask_3: 1.88  loss_dice_3: 2.664  loss_ce_4: 3.181  loss_mask_4: 1.858  loss_dice_4: 2.743  loss_ce_5: 3.171  loss_mask_5: 1.919  loss_dice_5: 2.731  loss_ce_6: 3.044  loss_mask_6: 2.158  loss_dice_6: 2.802  loss_ce_7: 3.17  loss_mask_7: 1.892  loss_dice_7: 2.735  loss_ce_8: 3.155  loss_mask_8: 2.054  loss_dice_8: 2.674    time: 0.4442  last_time: 0.4235  data_time: 0.0231  last_data_time: 0.0205   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:41:07 d2.utils.events]: \u001b[0m eta: 1 day, 20:11:55  iter: 3079  total_loss: 79.39  loss_ce: 3.101  loss_mask: 2.077  loss_dice: 2.711  loss_ce_0: 4.184  loss_mask_0: 1.98  loss_dice_0: 2.697  loss_ce_1: 3.08  loss_mask_1: 1.938  loss_dice_1: 2.638  loss_ce_2: 3.003  loss_mask_2: 1.886  loss_dice_2: 2.768  loss_ce_3: 3.121  loss_mask_3: 2.065  loss_dice_3: 2.785  loss_ce_4: 3.032  loss_mask_4: 1.978  loss_dice_4: 2.741  loss_ce_5: 3.074  loss_mask_5: 1.893  loss_dice_5: 2.851  loss_ce_6: 2.925  loss_mask_6: 2.202  loss_dice_6: 2.891  loss_ce_7: 3.117  loss_mask_7: 1.947  loss_dice_7: 2.791  loss_ce_8: 2.886  loss_mask_8: 2.24  loss_dice_8: 2.938    time: 0.4441  last_time: 0.4270  data_time: 0.0226  last_data_time: 0.0234   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:41:16 d2.utils.events]: \u001b[0m eta: 1 day, 20:10:10  iter: 3099  total_loss: 80.73  loss_ce: 3.032  loss_mask: 1.82  loss_dice: 3.006  loss_ce_0: 4.063  loss_mask_0: 1.885  loss_dice_0: 2.935  loss_ce_1: 3.05  loss_mask_1: 1.843  loss_dice_1: 2.835  loss_ce_2: 3.088  loss_mask_2: 1.867  loss_dice_2: 2.876  loss_ce_3: 3.122  loss_mask_3: 1.712  loss_dice_3: 2.76  loss_ce_4: 3.08  loss_mask_4: 1.882  loss_dice_4: 2.752  loss_ce_5: 3.02  loss_mask_5: 1.902  loss_dice_5: 2.9  loss_ce_6: 3.159  loss_mask_6: 1.805  loss_dice_6: 2.926  loss_ce_7: 3.156  loss_mask_7: 1.886  loss_dice_7: 2.958  loss_ce_8: 3.112  loss_mask_8: 1.941  loss_dice_8: 2.777    time: 0.4440  last_time: 0.4359  data_time: 0.0223  last_data_time: 0.0245   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:41:24 d2.utils.events]: \u001b[0m eta: 1 day, 20:09:56  iter: 3119  total_loss: 80.36  loss_ce: 3.275  loss_mask: 2.055  loss_dice: 2.833  loss_ce_0: 4.13  loss_mask_0: 1.813  loss_dice_0: 2.818  loss_ce_1: 3.13  loss_mask_1: 1.777  loss_dice_1: 2.543  loss_ce_2: 3.158  loss_mask_2: 1.897  loss_dice_2: 2.558  loss_ce_3: 3.234  loss_mask_3: 1.818  loss_dice_3: 2.747  loss_ce_4: 3.149  loss_mask_4: 2.018  loss_dice_4: 2.749  loss_ce_5: 3.197  loss_mask_5: 2.036  loss_dice_5: 2.742  loss_ce_6: 3.223  loss_mask_6: 1.903  loss_dice_6: 2.724  loss_ce_7: 3.261  loss_mask_7: 1.919  loss_dice_7: 2.782  loss_ce_8: 3.232  loss_mask_8: 1.853  loss_dice_8: 2.771    time: 0.4439  last_time: 0.4228  data_time: 0.0218  last_data_time: 0.0195   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:41:33 d2.utils.events]: \u001b[0m eta: 1 day, 20:08:55  iter: 3139  total_loss: 80.74  loss_ce: 3.205  loss_mask: 1.887  loss_dice: 2.764  loss_ce_0: 4.161  loss_mask_0: 1.904  loss_dice_0: 2.81  loss_ce_1: 3.292  loss_mask_1: 1.79  loss_dice_1: 2.644  loss_ce_2: 3.276  loss_mask_2: 1.764  loss_dice_2: 2.632  loss_ce_3: 3.298  loss_mask_3: 1.753  loss_dice_3: 2.593  loss_ce_4: 3.235  loss_mask_4: 1.86  loss_dice_4: 2.769  loss_ce_5: 3.229  loss_mask_5: 1.857  loss_dice_5: 2.714  loss_ce_6: 3.383  loss_mask_6: 1.903  loss_dice_6: 2.823  loss_ce_7: 3.324  loss_mask_7: 1.965  loss_dice_7: 2.847  loss_ce_8: 3.205  loss_mask_8: 1.812  loss_dice_8: 2.846    time: 0.4438  last_time: 0.4275  data_time: 0.0221  last_data_time: 0.0214   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:41:41 d2.utils.events]: \u001b[0m eta: 1 day, 20:07:49  iter: 3159  total_loss: 78.9  loss_ce: 2.739  loss_mask: 2.28  loss_dice: 2.897  loss_ce_0: 3.93  loss_mask_0: 1.926  loss_dice_0: 2.828  loss_ce_1: 2.733  loss_mask_1: 2.106  loss_dice_1: 2.877  loss_ce_2: 2.777  loss_mask_2: 2.333  loss_dice_2: 2.723  loss_ce_3: 2.789  loss_mask_3: 2.084  loss_dice_3: 2.823  loss_ce_4: 2.86  loss_mask_4: 2.085  loss_dice_4: 2.765  loss_ce_5: 2.766  loss_mask_5: 2.039  loss_dice_5: 2.748  loss_ce_6: 2.896  loss_mask_6: 2.128  loss_dice_6: 2.888  loss_ce_7: 2.776  loss_mask_7: 2.026  loss_dice_7: 2.835  loss_ce_8: 2.902  loss_mask_8: 2.114  loss_dice_8: 2.967    time: 0.4437  last_time: 0.4236  data_time: 0.0216  last_data_time: 0.0215   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:41:50 d2.utils.events]: \u001b[0m eta: 1 day, 20:07:06  iter: 3179  total_loss: 79.57  loss_ce: 3.292  loss_mask: 1.645  loss_dice: 2.873  loss_ce_0: 4.195  loss_mask_0: 1.644  loss_dice_0: 2.891  loss_ce_1: 3.196  loss_mask_1: 1.582  loss_dice_1: 2.643  loss_ce_2: 3.299  loss_mask_2: 1.724  loss_dice_2: 2.607  loss_ce_3: 3.266  loss_mask_3: 1.806  loss_dice_3: 2.712  loss_ce_4: 3.333  loss_mask_4: 1.687  loss_dice_4: 2.793  loss_ce_5: 3.291  loss_mask_5: 1.82  loss_dice_5: 3.044  loss_ce_6: 3.358  loss_mask_6: 1.765  loss_dice_6: 2.97  loss_ce_7: 3.304  loss_mask_7: 1.614  loss_dice_7: 2.983  loss_ce_8: 3.283  loss_mask_8: 1.737  loss_dice_8: 2.793    time: 0.4436  last_time: 0.4235  data_time: 0.0232  last_data_time: 0.0212   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:41:59 d2.utils.events]: \u001b[0m eta: 1 day, 20:05:45  iter: 3199  total_loss: 78.16  loss_ce: 3.379  loss_mask: 1.989  loss_dice: 2.664  loss_ce_0: 4.317  loss_mask_0: 1.947  loss_dice_0: 2.719  loss_ce_1: 3.322  loss_mask_1: 1.863  loss_dice_1: 2.467  loss_ce_2: 3.451  loss_mask_2: 1.747  loss_dice_2: 2.446  loss_ce_3: 3.433  loss_mask_3: 1.878  loss_dice_3: 2.566  loss_ce_4: 3.4  loss_mask_4: 1.869  loss_dice_4: 2.537  loss_ce_5: 3.377  loss_mask_5: 1.86  loss_dice_5: 2.616  loss_ce_6: 3.387  loss_mask_6: 1.925  loss_dice_6: 2.621  loss_ce_7: 3.419  loss_mask_7: 1.949  loss_dice_7: 2.77  loss_ce_8: 3.454  loss_mask_8: 1.806  loss_dice_8: 2.716    time: 0.4435  last_time: 0.4263  data_time: 0.0222  last_data_time: 0.0189   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:42:07 d2.utils.events]: \u001b[0m eta: 1 day, 20:04:47  iter: 3219  total_loss: 77.6  loss_ce: 2.72  loss_mask: 1.842  loss_dice: 2.846  loss_ce_0: 3.921  loss_mask_0: 1.597  loss_dice_0: 2.991  loss_ce_1: 2.72  loss_mask_1: 1.611  loss_dice_1: 2.654  loss_ce_2: 2.793  loss_mask_2: 1.768  loss_dice_2: 2.706  loss_ce_3: 2.691  loss_mask_3: 1.738  loss_dice_3: 2.636  loss_ce_4: 2.683  loss_mask_4: 1.637  loss_dice_4: 2.681  loss_ce_5: 2.834  loss_mask_5: 1.818  loss_dice_5: 2.867  loss_ce_6: 2.836  loss_mask_6: 1.98  loss_dice_6: 2.903  loss_ce_7: 2.752  loss_mask_7: 1.907  loss_dice_7: 2.818  loss_ce_8: 2.754  loss_mask_8: 1.815  loss_dice_8: 2.87    time: 0.4434  last_time: 0.4204  data_time: 0.0216  last_data_time: 0.0194   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:42:16 d2.utils.events]: \u001b[0m eta: 1 day, 20:03:42  iter: 3239  total_loss: 84.36  loss_ce: 3.279  loss_mask: 2.061  loss_dice: 2.861  loss_ce_0: 4.155  loss_mask_0: 1.919  loss_dice_0: 2.869  loss_ce_1: 3.157  loss_mask_1: 1.983  loss_dice_1: 2.718  loss_ce_2: 3.256  loss_mask_2: 1.974  loss_dice_2: 2.63  loss_ce_3: 3.276  loss_mask_3: 1.958  loss_dice_3: 2.891  loss_ce_4: 3.326  loss_mask_4: 1.881  loss_dice_4: 2.624  loss_ce_5: 3.327  loss_mask_5: 1.988  loss_dice_5: 2.879  loss_ce_6: 3.384  loss_mask_6: 2.007  loss_dice_6: 2.703  loss_ce_7: 3.386  loss_mask_7: 1.929  loss_dice_7: 2.787  loss_ce_8: 3.333  loss_mask_8: 1.973  loss_dice_8: 2.833    time: 0.4434  last_time: 0.4315  data_time: 0.0229  last_data_time: 0.0277   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:42:24 d2.utils.events]: \u001b[0m eta: 1 day, 20:03:03  iter: 3259  total_loss: 84.42  loss_ce: 3.502  loss_mask: 1.766  loss_dice: 2.884  loss_ce_0: 4.227  loss_mask_0: 1.938  loss_dice_0: 2.897  loss_ce_1: 3.493  loss_mask_1: 1.783  loss_dice_1: 2.813  loss_ce_2: 3.554  loss_mask_2: 1.829  loss_dice_2: 2.741  loss_ce_3: 3.441  loss_mask_3: 1.864  loss_dice_3: 2.734  loss_ce_4: 3.632  loss_mask_4: 1.763  loss_dice_4: 2.675  loss_ce_5: 3.544  loss_mask_5: 1.72  loss_dice_5: 2.887  loss_ce_6: 3.446  loss_mask_6: 2.063  loss_dice_6: 2.982  loss_ce_7: 3.396  loss_mask_7: 1.945  loss_dice_7: 2.987  loss_ce_8: 3.533  loss_mask_8: 1.807  loss_dice_8: 2.825    time: 0.4433  last_time: 0.4327  data_time: 0.0229  last_data_time: 0.0204   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:42:33 d2.utils.events]: \u001b[0m eta: 1 day, 20:02:29  iter: 3279  total_loss: 79.42  loss_ce: 3.468  loss_mask: 1.661  loss_dice: 2.655  loss_ce_0: 4.022  loss_mask_0: 1.784  loss_dice_0: 2.738  loss_ce_1: 3.435  loss_mask_1: 1.651  loss_dice_1: 2.584  loss_ce_2: 3.643  loss_mask_2: 1.673  loss_dice_2: 2.509  loss_ce_3: 3.588  loss_mask_3: 1.799  loss_dice_3: 2.699  loss_ce_4: 3.432  loss_mask_4: 1.872  loss_dice_4: 2.722  loss_ce_5: 3.452  loss_mask_5: 1.678  loss_dice_5: 2.742  loss_ce_6: 3.515  loss_mask_6: 1.909  loss_dice_6: 2.976  loss_ce_7: 3.477  loss_mask_7: 1.71  loss_dice_7: 2.712  loss_ce_8: 3.524  loss_mask_8: 1.741  loss_dice_8: 2.856    time: 0.4432  last_time: 0.4218  data_time: 0.0218  last_data_time: 0.0209   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:42:42 d2.utils.events]: \u001b[0m eta: 1 day, 20:01:25  iter: 3299  total_loss: 84.83  loss_ce: 3.508  loss_mask: 1.929  loss_dice: 2.855  loss_ce_0: 4.262  loss_mask_0: 1.92  loss_dice_0: 2.897  loss_ce_1: 3.451  loss_mask_1: 1.864  loss_dice_1: 2.625  loss_ce_2: 3.429  loss_mask_2: 1.959  loss_dice_2: 2.779  loss_ce_3: 3.384  loss_mask_3: 1.876  loss_dice_3: 2.807  loss_ce_4: 3.455  loss_mask_4: 2.148  loss_dice_4: 2.857  loss_ce_5: 3.496  loss_mask_5: 2.019  loss_dice_5: 2.813  loss_ce_6: 3.329  loss_mask_6: 1.984  loss_dice_6: 2.893  loss_ce_7: 3.414  loss_mask_7: 2.059  loss_dice_7: 2.854  loss_ce_8: 3.526  loss_mask_8: 1.944  loss_dice_8: 2.77    time: 0.4431  last_time: 0.4448  data_time: 0.0223  last_data_time: 0.0182   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:42:50 d2.utils.events]: \u001b[0m eta: 1 day, 20:00:19  iter: 3319  total_loss: 78.45  loss_ce: 3.257  loss_mask: 1.757  loss_dice: 2.622  loss_ce_0: 4.05  loss_mask_0: 1.843  loss_dice_0: 2.661  loss_ce_1: 3.149  loss_mask_1: 1.899  loss_dice_1: 2.562  loss_ce_2: 3.273  loss_mask_2: 2.066  loss_dice_2: 2.483  loss_ce_3: 3.217  loss_mask_3: 1.843  loss_dice_3: 2.7  loss_ce_4: 3.089  loss_mask_4: 2.194  loss_dice_4: 2.836  loss_ce_5: 3.214  loss_mask_5: 1.8  loss_dice_5: 2.57  loss_ce_6: 3.179  loss_mask_6: 1.96  loss_dice_6: 2.798  loss_ce_7: 3.159  loss_mask_7: 1.966  loss_dice_7: 2.809  loss_ce_8: 3.315  loss_mask_8: 2.041  loss_dice_8: 2.794    time: 0.4430  last_time: 0.4287  data_time: 0.0227  last_data_time: 0.0244   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:42:59 d2.utils.events]: \u001b[0m eta: 1 day, 19:59:21  iter: 3339  total_loss: 75.38  loss_ce: 2.591  loss_mask: 1.752  loss_dice: 2.877  loss_ce_0: 3.72  loss_mask_0: 1.915  loss_dice_0: 2.781  loss_ce_1: 2.555  loss_mask_1: 1.927  loss_dice_1: 2.803  loss_ce_2: 2.585  loss_mask_2: 1.914  loss_dice_2: 2.731  loss_ce_3: 2.65  loss_mask_3: 2.199  loss_dice_3: 2.782  loss_ce_4: 2.41  loss_mask_4: 1.84  loss_dice_4: 2.708  loss_ce_5: 2.654  loss_mask_5: 1.819  loss_dice_5: 2.944  loss_ce_6: 2.475  loss_mask_6: 1.949  loss_dice_6: 2.939  loss_ce_7: 2.593  loss_mask_7: 1.938  loss_dice_7: 3.021  loss_ce_8: 2.634  loss_mask_8: 2.108  loss_dice_8: 2.812    time: 0.4429  last_time: 0.4239  data_time: 0.0224  last_data_time: 0.0204   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:43:07 d2.utils.events]: \u001b[0m eta: 1 day, 19:58:24  iter: 3359  total_loss: 78.09  loss_ce: 3.11  loss_mask: 1.775  loss_dice: 2.775  loss_ce_0: 3.814  loss_mask_0: 1.776  loss_dice_0: 2.873  loss_ce_1: 2.969  loss_mask_1: 1.8  loss_dice_1: 2.63  loss_ce_2: 3.026  loss_mask_2: 1.801  loss_dice_2: 2.752  loss_ce_3: 3.01  loss_mask_3: 1.905  loss_dice_3: 2.759  loss_ce_4: 3.043  loss_mask_4: 2.091  loss_dice_4: 2.854  loss_ce_5: 3.026  loss_mask_5: 2.085  loss_dice_5: 2.952  loss_ce_6: 3.034  loss_mask_6: 1.802  loss_dice_6: 2.892  loss_ce_7: 2.843  loss_mask_7: 1.821  loss_dice_7: 2.734  loss_ce_8: 3.094  loss_mask_8: 1.842  loss_dice_8: 2.829    time: 0.4429  last_time: 0.4319  data_time: 0.0221  last_data_time: 0.0211   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:43:16 d2.utils.events]: \u001b[0m eta: 1 day, 19:57:44  iter: 3379  total_loss: 77  loss_ce: 2.998  loss_mask: 1.746  loss_dice: 2.731  loss_ce_0: 3.961  loss_mask_0: 1.855  loss_dice_0: 2.792  loss_ce_1: 2.853  loss_mask_1: 1.712  loss_dice_1: 2.539  loss_ce_2: 2.981  loss_mask_2: 1.878  loss_dice_2: 2.661  loss_ce_3: 2.961  loss_mask_3: 1.632  loss_dice_3: 2.686  loss_ce_4: 2.983  loss_mask_4: 2.045  loss_dice_4: 2.698  loss_ce_5: 2.975  loss_mask_5: 1.806  loss_dice_5: 2.783  loss_ce_6: 2.795  loss_mask_6: 1.793  loss_dice_6: 2.912  loss_ce_7: 2.931  loss_mask_7: 1.77  loss_dice_7: 2.729  loss_ce_8: 3.064  loss_mask_8: 1.839  loss_dice_8: 2.685    time: 0.4428  last_time: 0.4224  data_time: 0.0220  last_data_time: 0.0202   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:43:25 d2.utils.events]: \u001b[0m eta: 1 day, 19:56:56  iter: 3399  total_loss: 77.41  loss_ce: 2.985  loss_mask: 1.883  loss_dice: 2.62  loss_ce_0: 3.843  loss_mask_0: 1.976  loss_dice_0: 2.5  loss_ce_1: 2.864  loss_mask_1: 1.884  loss_dice_1: 2.336  loss_ce_2: 2.888  loss_mask_2: 2.102  loss_dice_2: 2.486  loss_ce_3: 2.956  loss_mask_3: 2.053  loss_dice_3: 2.615  loss_ce_4: 2.98  loss_mask_4: 2.058  loss_dice_4: 2.407  loss_ce_5: 2.973  loss_mask_5: 2.04  loss_dice_5: 2.563  loss_ce_6: 2.739  loss_mask_6: 2.372  loss_dice_6: 2.656  loss_ce_7: 2.947  loss_mask_7: 2.163  loss_dice_7: 2.645  loss_ce_8: 3.043  loss_mask_8: 1.825  loss_dice_8: 2.511    time: 0.4427  last_time: 0.4305  data_time: 0.0222  last_data_time: 0.0228   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:43:33 d2.utils.events]: \u001b[0m eta: 1 day, 19:56:07  iter: 3419  total_loss: 79.09  loss_ce: 3.1  loss_mask: 2.062  loss_dice: 2.721  loss_ce_0: 4.061  loss_mask_0: 1.982  loss_dice_0: 2.723  loss_ce_1: 3.048  loss_mask_1: 1.75  loss_dice_1: 2.574  loss_ce_2: 3.035  loss_mask_2: 1.791  loss_dice_2: 2.461  loss_ce_3: 3.054  loss_mask_3: 1.903  loss_dice_3: 2.5  loss_ce_4: 3.157  loss_mask_4: 1.81  loss_dice_4: 2.471  loss_ce_5: 3.083  loss_mask_5: 1.864  loss_dice_5: 2.672  loss_ce_6: 2.938  loss_mask_6: 1.809  loss_dice_6: 2.622  loss_ce_7: 3.154  loss_mask_7: 1.86  loss_dice_7: 2.513  loss_ce_8: 3.099  loss_mask_8: 1.801  loss_dice_8: 2.716    time: 0.4427  last_time: 0.4246  data_time: 0.0229  last_data_time: 0.0202   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:43:42 d2.utils.events]: \u001b[0m eta: 1 day, 19:55:13  iter: 3439  total_loss: 80.45  loss_ce: 3.303  loss_mask: 1.876  loss_dice: 2.643  loss_ce_0: 3.892  loss_mask_0: 1.954  loss_dice_0: 2.679  loss_ce_1: 3.296  loss_mask_1: 1.871  loss_dice_1: 2.476  loss_ce_2: 3.245  loss_mask_2: 1.91  loss_dice_2: 2.682  loss_ce_3: 3.134  loss_mask_3: 1.976  loss_dice_3: 2.827  loss_ce_4: 3.271  loss_mask_4: 2.057  loss_dice_4: 2.629  loss_ce_5: 3.23  loss_mask_5: 1.878  loss_dice_5: 2.733  loss_ce_6: 3.16  loss_mask_6: 2.053  loss_dice_6: 2.863  loss_ce_7: 3.191  loss_mask_7: 2.045  loss_dice_7: 2.623  loss_ce_8: 3.126  loss_mask_8: 2.071  loss_dice_8: 2.822    time: 0.4426  last_time: 0.4217  data_time: 0.0227  last_data_time: 0.0191   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:43:51 d2.utils.events]: \u001b[0m eta: 1 day, 19:53:50  iter: 3459  total_loss: 81.6  loss_ce: 2.802  loss_mask: 1.78  loss_dice: 2.799  loss_ce_0: 3.828  loss_mask_0: 1.778  loss_dice_0: 2.914  loss_ce_1: 2.841  loss_mask_1: 1.774  loss_dice_1: 2.668  loss_ce_2: 2.792  loss_mask_2: 1.781  loss_dice_2: 2.692  loss_ce_3: 2.722  loss_mask_3: 1.894  loss_dice_3: 2.905  loss_ce_4: 2.761  loss_mask_4: 2.027  loss_dice_4: 3.043  loss_ce_5: 2.886  loss_mask_5: 1.789  loss_dice_5: 2.696  loss_ce_6: 2.756  loss_mask_6: 1.911  loss_dice_6: 3.03  loss_ce_7: 2.96  loss_mask_7: 1.916  loss_dice_7: 2.735  loss_ce_8: 2.933  loss_mask_8: 1.759  loss_dice_8: 2.792    time: 0.4425  last_time: 0.4274  data_time: 0.0221  last_data_time: 0.0204   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:43:59 d2.utils.events]: \u001b[0m eta: 1 day, 19:51:29  iter: 3479  total_loss: 81.24  loss_ce: 3.2  loss_mask: 1.928  loss_dice: 2.884  loss_ce_0: 3.98  loss_mask_0: 2.023  loss_dice_0: 2.938  loss_ce_1: 3.32  loss_mask_1: 1.857  loss_dice_1: 2.771  loss_ce_2: 3.291  loss_mask_2: 2.073  loss_dice_2: 2.74  loss_ce_3: 3.21  loss_mask_3: 1.913  loss_dice_3: 2.776  loss_ce_4: 3.155  loss_mask_4: 1.966  loss_dice_4: 2.918  loss_ce_5: 3.271  loss_mask_5: 1.821  loss_dice_5: 2.747  loss_ce_6: 3.235  loss_mask_6: 1.887  loss_dice_6: 2.835  loss_ce_7: 3.216  loss_mask_7: 2.153  loss_dice_7: 2.877  loss_ce_8: 3.237  loss_mask_8: 1.938  loss_dice_8: 2.918    time: 0.4424  last_time: 0.4275  data_time: 0.0224  last_data_time: 0.0217   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:44:08 d2.utils.events]: \u001b[0m eta: 1 day, 19:49:24  iter: 3499  total_loss: 77.51  loss_ce: 2.784  loss_mask: 2.265  loss_dice: 2.672  loss_ce_0: 3.667  loss_mask_0: 1.979  loss_dice_0: 2.821  loss_ce_1: 2.869  loss_mask_1: 1.99  loss_dice_1: 2.584  loss_ce_2: 2.819  loss_mask_2: 2.148  loss_dice_2: 2.499  loss_ce_3: 2.819  loss_mask_3: 2.01  loss_dice_3: 2.672  loss_ce_4: 2.841  loss_mask_4: 2.108  loss_dice_4: 2.776  loss_ce_5: 2.803  loss_mask_5: 1.955  loss_dice_5: 2.776  loss_ce_6: 2.805  loss_mask_6: 2.013  loss_dice_6: 2.792  loss_ce_7: 2.778  loss_mask_7: 2.132  loss_dice_7: 2.671  loss_ce_8: 2.968  loss_mask_8: 1.959  loss_dice_8: 2.593    time: 0.4424  last_time: 0.4609  data_time: 0.0223  last_data_time: 0.0464   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:44:16 d2.utils.events]: \u001b[0m eta: 1 day, 19:47:20  iter: 3519  total_loss: 78.81  loss_ce: 2.953  loss_mask: 1.79  loss_dice: 2.954  loss_ce_0: 3.758  loss_mask_0: 1.609  loss_dice_0: 2.722  loss_ce_1: 2.916  loss_mask_1: 1.434  loss_dice_1: 2.821  loss_ce_2: 2.882  loss_mask_2: 1.732  loss_dice_2: 2.768  loss_ce_3: 3.008  loss_mask_3: 1.669  loss_dice_3: 2.976  loss_ce_4: 2.982  loss_mask_4: 1.845  loss_dice_4: 2.758  loss_ce_5: 3.065  loss_mask_5: 1.645  loss_dice_5: 2.768  loss_ce_6: 2.782  loss_mask_6: 1.621  loss_dice_6: 2.944  loss_ce_7: 2.872  loss_mask_7: 1.742  loss_dice_7: 3.062  loss_ce_8: 2.984  loss_mask_8: 1.771  loss_dice_8: 2.884    time: 0.4423  last_time: 0.4217  data_time: 0.0217  last_data_time: 0.0191   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:44:25 d2.utils.events]: \u001b[0m eta: 1 day, 19:46:37  iter: 3539  total_loss: 79.7  loss_ce: 3.265  loss_mask: 1.865  loss_dice: 2.901  loss_ce_0: 3.888  loss_mask_0: 1.795  loss_dice_0: 2.773  loss_ce_1: 3.138  loss_mask_1: 1.866  loss_dice_1: 2.821  loss_ce_2: 3.078  loss_mask_2: 1.793  loss_dice_2: 2.807  loss_ce_3: 3.297  loss_mask_3: 1.995  loss_dice_3: 2.764  loss_ce_4: 3.217  loss_mask_4: 1.785  loss_dice_4: 2.769  loss_ce_5: 3.272  loss_mask_5: 1.949  loss_dice_5: 2.736  loss_ce_6: 2.977  loss_mask_6: 1.885  loss_dice_6: 2.939  loss_ce_7: 3.121  loss_mask_7: 1.923  loss_dice_7: 2.853  loss_ce_8: 3.207  loss_mask_8: 1.892  loss_dice_8: 2.897    time: 0.4422  last_time: 0.4271  data_time: 0.0228  last_data_time: 0.0231   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:44:34 d2.utils.events]: \u001b[0m eta: 1 day, 19:46:17  iter: 3559  total_loss: 81.82  loss_ce: 3.209  loss_mask: 1.915  loss_dice: 2.707  loss_ce_0: 3.986  loss_mask_0: 1.901  loss_dice_0: 2.796  loss_ce_1: 3.113  loss_mask_1: 1.88  loss_dice_1: 2.523  loss_ce_2: 3.171  loss_mask_2: 1.852  loss_dice_2: 2.648  loss_ce_3: 3.227  loss_mask_3: 1.888  loss_dice_3: 2.8  loss_ce_4: 3.253  loss_mask_4: 1.85  loss_dice_4: 2.744  loss_ce_5: 3.274  loss_mask_5: 2.058  loss_dice_5: 2.863  loss_ce_6: 3.112  loss_mask_6: 1.91  loss_dice_6: 2.82  loss_ce_7: 3.134  loss_mask_7: 1.89  loss_dice_7: 2.902  loss_ce_8: 3.368  loss_mask_8: 1.88  loss_dice_8: 2.746    time: 0.4422  last_time: 0.4250  data_time: 0.0242  last_data_time: 0.0201   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:44:42 d2.utils.events]: \u001b[0m eta: 1 day, 19:45:42  iter: 3579  total_loss: 80.79  loss_ce: 3.17  loss_mask: 2.041  loss_dice: 2.878  loss_ce_0: 3.944  loss_mask_0: 1.741  loss_dice_0: 2.824  loss_ce_1: 3.148  loss_mask_1: 1.932  loss_dice_1: 2.75  loss_ce_2: 3.116  loss_mask_2: 2.056  loss_dice_2: 2.857  loss_ce_3: 3.203  loss_mask_3: 1.989  loss_dice_3: 2.726  loss_ce_4: 3.298  loss_mask_4: 2.274  loss_dice_4: 2.895  loss_ce_5: 3.153  loss_mask_5: 2.192  loss_dice_5: 3.101  loss_ce_6: 3.362  loss_mask_6: 2.129  loss_dice_6: 2.826  loss_ce_7: 3.24  loss_mask_7: 2.177  loss_dice_7: 2.911  loss_ce_8: 3.265  loss_mask_8: 2.058  loss_dice_8: 2.904    time: 0.4421  last_time: 0.4341  data_time: 0.0224  last_data_time: 0.0227   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:44:51 d2.utils.events]: \u001b[0m eta: 1 day, 19:44:05  iter: 3599  total_loss: 75.89  loss_ce: 2.888  loss_mask: 1.938  loss_dice: 2.569  loss_ce_0: 3.695  loss_mask_0: 1.975  loss_dice_0: 2.637  loss_ce_1: 2.897  loss_mask_1: 1.579  loss_dice_1: 2.384  loss_ce_2: 3.05  loss_mask_2: 1.864  loss_dice_2: 2.574  loss_ce_3: 2.924  loss_mask_3: 1.749  loss_dice_3: 2.636  loss_ce_4: 2.975  loss_mask_4: 1.943  loss_dice_4: 2.531  loss_ce_5: 2.888  loss_mask_5: 1.831  loss_dice_5: 2.787  loss_ce_6: 2.846  loss_mask_6: 1.777  loss_dice_6: 2.529  loss_ce_7: 3.038  loss_mask_7: 1.709  loss_dice_7: 2.585  loss_ce_8: 2.971  loss_mask_8: 2.04  loss_dice_8: 2.639    time: 0.4421  last_time: 0.4231  data_time: 0.0218  last_data_time: 0.0212   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:45:00 d2.utils.events]: \u001b[0m eta: 1 day, 19:42:22  iter: 3619  total_loss: 83.85  loss_ce: 3.515  loss_mask: 1.838  loss_dice: 2.8  loss_ce_0: 4.213  loss_mask_0: 1.791  loss_dice_0: 2.886  loss_ce_1: 3.672  loss_mask_1: 1.864  loss_dice_1: 2.563  loss_ce_2: 3.666  loss_mask_2: 1.939  loss_dice_2: 2.788  loss_ce_3: 3.562  loss_mask_3: 1.957  loss_dice_3: 2.809  loss_ce_4: 3.631  loss_mask_4: 1.854  loss_dice_4: 2.892  loss_ce_5: 3.683  loss_mask_5: 1.869  loss_dice_5: 2.96  loss_ce_6: 3.581  loss_mask_6: 1.857  loss_dice_6: 2.947  loss_ce_7: 3.643  loss_mask_7: 1.899  loss_dice_7: 2.883  loss_ce_8: 3.621  loss_mask_8: 1.92  loss_dice_8: 2.719    time: 0.4420  last_time: 0.4504  data_time: 0.0237  last_data_time: 0.0247   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:45:08 d2.utils.events]: \u001b[0m eta: 1 day, 19:41:44  iter: 3639  total_loss: 80.36  loss_ce: 3.181  loss_mask: 1.726  loss_dice: 2.883  loss_ce_0: 3.736  loss_mask_0: 2.005  loss_dice_0: 2.768  loss_ce_1: 3.039  loss_mask_1: 1.983  loss_dice_1: 2.514  loss_ce_2: 3.173  loss_mask_2: 1.981  loss_dice_2: 2.834  loss_ce_3: 3.13  loss_mask_3: 1.922  loss_dice_3: 2.713  loss_ce_4: 3.134  loss_mask_4: 1.858  loss_dice_4: 2.917  loss_ce_5: 3.115  loss_mask_5: 1.862  loss_dice_5: 2.865  loss_ce_6: 3.228  loss_mask_6: 2.057  loss_dice_6: 2.78  loss_ce_7: 3.134  loss_mask_7: 1.916  loss_dice_7: 2.755  loss_ce_8: 3.123  loss_mask_8: 2.017  loss_dice_8: 2.815    time: 0.4420  last_time: 0.4207  data_time: 0.0224  last_data_time: 0.0191   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:45:17 d2.utils.events]: \u001b[0m eta: 1 day, 19:40:43  iter: 3659  total_loss: 83.47  loss_ce: 3.115  loss_mask: 1.907  loss_dice: 3.136  loss_ce_0: 3.826  loss_mask_0: 1.741  loss_dice_0: 3.036  loss_ce_1: 3.075  loss_mask_1: 1.676  loss_dice_1: 2.898  loss_ce_2: 3.093  loss_mask_2: 1.739  loss_dice_2: 2.958  loss_ce_3: 3.071  loss_mask_3: 1.599  loss_dice_3: 2.949  loss_ce_4: 3.185  loss_mask_4: 1.752  loss_dice_4: 2.942  loss_ce_5: 3.098  loss_mask_5: 1.769  loss_dice_5: 3.009  loss_ce_6: 3.125  loss_mask_6: 1.909  loss_dice_6: 3.191  loss_ce_7: 3.172  loss_mask_7: 1.905  loss_dice_7: 2.866  loss_ce_8: 3.137  loss_mask_8: 1.986  loss_dice_8: 3.022    time: 0.4419  last_time: 0.4204  data_time: 0.0225  last_data_time: 0.0191   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:45:26 d2.utils.events]: \u001b[0m eta: 1 day, 19:40:27  iter: 3679  total_loss: 79.08  loss_ce: 3.076  loss_mask: 1.746  loss_dice: 2.63  loss_ce_0: 3.809  loss_mask_0: 1.813  loss_dice_0: 2.823  loss_ce_1: 3.278  loss_mask_1: 1.799  loss_dice_1: 2.504  loss_ce_2: 3.301  loss_mask_2: 1.666  loss_dice_2: 2.747  loss_ce_3: 3.109  loss_mask_3: 1.819  loss_dice_3: 2.713  loss_ce_4: 3.318  loss_mask_4: 1.862  loss_dice_4: 2.771  loss_ce_5: 3.192  loss_mask_5: 1.729  loss_dice_5: 2.648  loss_ce_6: 3.185  loss_mask_6: 1.888  loss_dice_6: 2.964  loss_ce_7: 3.25  loss_mask_7: 1.699  loss_dice_7: 2.739  loss_ce_8: 3.183  loss_mask_8: 1.943  loss_dice_8: 2.843    time: 0.4419  last_time: 0.4325  data_time: 0.0229  last_data_time: 0.0236   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:45:34 d2.utils.events]: \u001b[0m eta: 1 day, 19:39:37  iter: 3699  total_loss: 79.29  loss_ce: 3.363  loss_mask: 2.038  loss_dice: 2.587  loss_ce_0: 3.762  loss_mask_0: 1.925  loss_dice_0: 2.565  loss_ce_1: 3.254  loss_mask_1: 1.635  loss_dice_1: 2.273  loss_ce_2: 3.2  loss_mask_2: 1.871  loss_dice_2: 2.558  loss_ce_3: 3.261  loss_mask_3: 1.831  loss_dice_3: 2.478  loss_ce_4: 3.267  loss_mask_4: 2.051  loss_dice_4: 2.502  loss_ce_5: 3.274  loss_mask_5: 1.96  loss_dice_5: 2.736  loss_ce_6: 3.279  loss_mask_6: 1.881  loss_dice_6: 2.741  loss_ce_7: 3.338  loss_mask_7: 1.913  loss_dice_7: 2.639  loss_ce_8: 3.261  loss_mask_8: 2.065  loss_dice_8: 2.845    time: 0.4418  last_time: 0.4324  data_time: 0.0232  last_data_time: 0.0280   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:45:43 d2.utils.events]: \u001b[0m eta: 1 day, 19:38:50  iter: 3719  total_loss: 78.76  loss_ce: 3.007  loss_mask: 2.012  loss_dice: 2.878  loss_ce_0: 3.667  loss_mask_0: 1.844  loss_dice_0: 2.812  loss_ce_1: 2.907  loss_mask_1: 1.864  loss_dice_1: 2.483  loss_ce_2: 2.968  loss_mask_2: 1.965  loss_dice_2: 2.596  loss_ce_3: 2.972  loss_mask_3: 2.14  loss_dice_3: 2.69  loss_ce_4: 3.049  loss_mask_4: 2.008  loss_dice_4: 2.615  loss_ce_5: 2.917  loss_mask_5: 1.872  loss_dice_5: 2.656  loss_ce_6: 2.996  loss_mask_6: 2.03  loss_dice_6: 2.778  loss_ce_7: 2.999  loss_mask_7: 1.813  loss_dice_7: 2.661  loss_ce_8: 2.967  loss_mask_8: 1.863  loss_dice_8: 2.7    time: 0.4418  last_time: 0.4236  data_time: 0.0254  last_data_time: 0.0183   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:45:52 d2.utils.events]: \u001b[0m eta: 1 day, 19:37:52  iter: 3739  total_loss: 79.48  loss_ce: 3.251  loss_mask: 1.69  loss_dice: 2.883  loss_ce_0: 3.661  loss_mask_0: 1.92  loss_dice_0: 2.917  loss_ce_1: 3.131  loss_mask_1: 1.803  loss_dice_1: 2.605  loss_ce_2: 3.206  loss_mask_2: 1.833  loss_dice_2: 2.67  loss_ce_3: 3.111  loss_mask_3: 1.945  loss_dice_3: 2.787  loss_ce_4: 3.173  loss_mask_4: 1.905  loss_dice_4: 2.799  loss_ce_5: 3.005  loss_mask_5: 1.862  loss_dice_5: 2.918  loss_ce_6: 3.248  loss_mask_6: 1.745  loss_dice_6: 2.973  loss_ce_7: 3.01  loss_mask_7: 1.683  loss_dice_7: 2.748  loss_ce_8: 3.165  loss_mask_8: 1.952  loss_dice_8: 2.877    time: 0.4417  last_time: 0.4503  data_time: 0.0218  last_data_time: 0.0208   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:46:00 d2.utils.events]: \u001b[0m eta: 1 day, 19:36:41  iter: 3759  total_loss: 76.34  loss_ce: 3.357  loss_mask: 1.657  loss_dice: 2.882  loss_ce_0: 3.858  loss_mask_0: 1.545  loss_dice_0: 2.926  loss_ce_1: 3.216  loss_mask_1: 1.559  loss_dice_1: 2.769  loss_ce_2: 3.208  loss_mask_2: 1.652  loss_dice_2: 2.868  loss_ce_3: 3.213  loss_mask_3: 1.664  loss_dice_3: 2.757  loss_ce_4: 3.321  loss_mask_4: 2.004  loss_dice_4: 2.832  loss_ce_5: 3.256  loss_mask_5: 1.774  loss_dice_5: 2.978  loss_ce_6: 3.191  loss_mask_6: 1.633  loss_dice_6: 2.947  loss_ce_7: 3.248  loss_mask_7: 1.651  loss_dice_7: 2.962  loss_ce_8: 3.257  loss_mask_8: 1.78  loss_dice_8: 2.865    time: 0.4417  last_time: 0.4330  data_time: 0.0234  last_data_time: 0.0208   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:46:09 d2.utils.events]: \u001b[0m eta: 1 day, 19:35:27  iter: 3779  total_loss: 78.8  loss_ce: 2.982  loss_mask: 1.839  loss_dice: 2.828  loss_ce_0: 3.667  loss_mask_0: 1.873  loss_dice_0: 2.899  loss_ce_1: 2.998  loss_mask_1: 1.839  loss_dice_1: 2.793  loss_ce_2: 2.998  loss_mask_2: 1.717  loss_dice_2: 2.711  loss_ce_3: 2.961  loss_mask_3: 1.855  loss_dice_3: 2.68  loss_ce_4: 3.13  loss_mask_4: 1.857  loss_dice_4: 2.939  loss_ce_5: 3.109  loss_mask_5: 2.15  loss_dice_5: 3.016  loss_ce_6: 2.967  loss_mask_6: 1.994  loss_dice_6: 2.906  loss_ce_7: 3.05  loss_mask_7: 1.863  loss_dice_7: 2.862  loss_ce_8: 2.984  loss_mask_8: 2.033  loss_dice_8: 2.972    time: 0.4417  last_time: 0.4222  data_time: 0.0227  last_data_time: 0.0209   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:46:18 d2.utils.events]: \u001b[0m eta: 1 day, 19:35:08  iter: 3799  total_loss: 83.59  loss_ce: 3.537  loss_mask: 1.886  loss_dice: 2.83  loss_ce_0: 4.005  loss_mask_0: 1.901  loss_dice_0: 2.865  loss_ce_1: 3.439  loss_mask_1: 1.932  loss_dice_1: 2.716  loss_ce_2: 3.522  loss_mask_2: 2.007  loss_dice_2: 2.596  loss_ce_3: 3.543  loss_mask_3: 1.936  loss_dice_3: 2.874  loss_ce_4: 3.451  loss_mask_4: 2.087  loss_dice_4: 2.761  loss_ce_5: 3.593  loss_mask_5: 2  loss_dice_5: 2.899  loss_ce_6: 3.708  loss_mask_6: 1.894  loss_dice_6: 3.204  loss_ce_7: 3.637  loss_mask_7: 1.879  loss_dice_7: 2.867  loss_ce_8: 3.576  loss_mask_8: 1.878  loss_dice_8: 2.815    time: 0.4416  last_time: 0.4307  data_time: 0.0238  last_data_time: 0.0248   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:46:27 d2.utils.events]: \u001b[0m eta: 1 day, 19:34:05  iter: 3819  total_loss: 79.66  loss_ce: 3.064  loss_mask: 1.83  loss_dice: 2.762  loss_ce_0: 3.501  loss_mask_0: 1.879  loss_dice_0: 2.887  loss_ce_1: 2.964  loss_mask_1: 1.749  loss_dice_1: 2.759  loss_ce_2: 3.054  loss_mask_2: 1.841  loss_dice_2: 2.705  loss_ce_3: 3.085  loss_mask_3: 1.655  loss_dice_3: 2.784  loss_ce_4: 2.869  loss_mask_4: 1.86  loss_dice_4: 2.708  loss_ce_5: 2.982  loss_mask_5: 1.876  loss_dice_5: 2.657  loss_ce_6: 3.065  loss_mask_6: 2.033  loss_dice_6: 2.96  loss_ce_7: 3.09  loss_mask_7: 1.844  loss_dice_7: 2.807  loss_ce_8: 2.96  loss_mask_8: 1.867  loss_dice_8: 2.952    time: 0.4416  last_time: 0.4294  data_time: 0.0214  last_data_time: 0.0223   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:46:35 d2.utils.events]: \u001b[0m eta: 1 day, 19:33:26  iter: 3839  total_loss: 80.22  loss_ce: 3.419  loss_mask: 2.026  loss_dice: 2.635  loss_ce_0: 3.802  loss_mask_0: 1.96  loss_dice_0: 2.923  loss_ce_1: 3.318  loss_mask_1: 1.963  loss_dice_1: 2.724  loss_ce_2: 3.318  loss_mask_2: 1.974  loss_dice_2: 2.81  loss_ce_3: 3.432  loss_mask_3: 2.135  loss_dice_3: 2.921  loss_ce_4: 3.353  loss_mask_4: 2.134  loss_dice_4: 2.83  loss_ce_5: 3.463  loss_mask_5: 1.928  loss_dice_5: 2.877  loss_ce_6: 3.296  loss_mask_6: 1.947  loss_dice_6: 2.948  loss_ce_7: 3.318  loss_mask_7: 2.052  loss_dice_7: 2.87  loss_ce_8: 3.253  loss_mask_8: 2.107  loss_dice_8: 2.952    time: 0.4415  last_time: 0.4264  data_time: 0.0234  last_data_time: 0.0223   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:46:44 d2.utils.events]: \u001b[0m eta: 1 day, 19:33:02  iter: 3859  total_loss: 76.37  loss_ce: 3.268  loss_mask: 1.808  loss_dice: 2.566  loss_ce_0: 3.808  loss_mask_0: 1.594  loss_dice_0: 2.522  loss_ce_1: 3.172  loss_mask_1: 1.525  loss_dice_1: 2.516  loss_ce_2: 3.255  loss_mask_2: 1.667  loss_dice_2: 2.527  loss_ce_3: 3.285  loss_mask_3: 1.757  loss_dice_3: 2.676  loss_ce_4: 3.231  loss_mask_4: 1.82  loss_dice_4: 2.655  loss_ce_5: 3.325  loss_mask_5: 1.654  loss_dice_5: 2.551  loss_ce_6: 3.165  loss_mask_6: 1.756  loss_dice_6: 2.733  loss_ce_7: 3.175  loss_mask_7: 1.866  loss_dice_7: 2.496  loss_ce_8: 3.189  loss_mask_8: 1.762  loss_dice_8: 2.636    time: 0.4415  last_time: 0.4250  data_time: 0.0218  last_data_time: 0.0233   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:46:53 d2.utils.events]: \u001b[0m eta: 1 day, 19:32:53  iter: 3879  total_loss: 82.32  loss_ce: 3.326  loss_mask: 1.918  loss_dice: 3.161  loss_ce_0: 3.817  loss_mask_0: 1.812  loss_dice_0: 2.851  loss_ce_1: 3.197  loss_mask_1: 1.593  loss_dice_1: 2.753  loss_ce_2: 3.285  loss_mask_2: 1.89  loss_dice_2: 2.57  loss_ce_3: 3.314  loss_mask_3: 1.745  loss_dice_3: 2.955  loss_ce_4: 3.32  loss_mask_4: 1.821  loss_dice_4: 2.931  loss_ce_5: 3.356  loss_mask_5: 1.651  loss_dice_5: 2.914  loss_ce_6: 3.447  loss_mask_6: 1.733  loss_dice_6: 2.986  loss_ce_7: 3.265  loss_mask_7: 2.031  loss_dice_7: 2.993  loss_ce_8: 3.318  loss_mask_8: 1.802  loss_dice_8: 2.985    time: 0.4415  last_time: 0.4306  data_time: 0.0224  last_data_time: 0.0203   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:47:02 d2.utils.events]: \u001b[0m eta: 1 day, 19:32:23  iter: 3899  total_loss: 77.6  loss_ce: 2.931  loss_mask: 2.125  loss_dice: 2.788  loss_ce_0: 3.499  loss_mask_0: 1.684  loss_dice_0: 2.748  loss_ce_1: 2.796  loss_mask_1: 1.957  loss_dice_1: 2.418  loss_ce_2: 2.859  loss_mask_2: 1.973  loss_dice_2: 2.613  loss_ce_3: 2.916  loss_mask_3: 1.852  loss_dice_3: 2.624  loss_ce_4: 2.855  loss_mask_4: 1.879  loss_dice_4: 2.638  loss_ce_5: 2.865  loss_mask_5: 1.784  loss_dice_5: 2.635  loss_ce_6: 2.934  loss_mask_6: 1.875  loss_dice_6: 2.656  loss_ce_7: 2.828  loss_mask_7: 1.962  loss_dice_7: 2.697  loss_ce_8: 2.84  loss_mask_8: 1.804  loss_dice_8: 2.63    time: 0.4415  last_time: 0.4301  data_time: 0.0243  last_data_time: 0.0213   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:47:10 d2.utils.events]: \u001b[0m eta: 1 day, 19:31:15  iter: 3919  total_loss: 80.38  loss_ce: 3.267  loss_mask: 1.792  loss_dice: 2.514  loss_ce_0: 3.666  loss_mask_0: 1.996  loss_dice_0: 2.587  loss_ce_1: 2.983  loss_mask_1: 1.901  loss_dice_1: 2.363  loss_ce_2: 3.161  loss_mask_2: 1.894  loss_dice_2: 2.43  loss_ce_3: 3.158  loss_mask_3: 1.78  loss_dice_3: 2.571  loss_ce_4: 3.03  loss_mask_4: 1.824  loss_dice_4: 2.559  loss_ce_5: 3.169  loss_mask_5: 1.871  loss_dice_5: 2.753  loss_ce_6: 3.249  loss_mask_6: 2.04  loss_dice_6: 2.655  loss_ce_7: 3.09  loss_mask_7: 2.058  loss_dice_7: 2.555  loss_ce_8: 3.214  loss_mask_8: 1.998  loss_dice_8: 2.798    time: 0.4414  last_time: 0.4282  data_time: 0.0223  last_data_time: 0.0243   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:47:19 d2.utils.events]: \u001b[0m eta: 1 day, 19:30:05  iter: 3939  total_loss: 77.37  loss_ce: 2.947  loss_mask: 1.698  loss_dice: 2.642  loss_ce_0: 3.458  loss_mask_0: 2.013  loss_dice_0: 2.726  loss_ce_1: 2.804  loss_mask_1: 1.622  loss_dice_1: 2.549  loss_ce_2: 2.87  loss_mask_2: 2.071  loss_dice_2: 2.601  loss_ce_3: 3.049  loss_mask_3: 2.048  loss_dice_3: 2.759  loss_ce_4: 2.782  loss_mask_4: 1.885  loss_dice_4: 2.607  loss_ce_5: 2.89  loss_mask_5: 1.808  loss_dice_5: 2.564  loss_ce_6: 2.965  loss_mask_6: 1.991  loss_dice_6: 2.546  loss_ce_7: 2.741  loss_mask_7: 2.085  loss_dice_7: 2.846  loss_ce_8: 3.086  loss_mask_8: 1.928  loss_dice_8: 2.619    time: 0.4414  last_time: 0.4325  data_time: 0.0221  last_data_time: 0.0230   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:47:28 d2.utils.events]: \u001b[0m eta: 1 day, 19:29:45  iter: 3959  total_loss: 79.42  loss_ce: 2.921  loss_mask: 1.967  loss_dice: 2.75  loss_ce_0: 3.575  loss_mask_0: 1.868  loss_dice_0: 2.762  loss_ce_1: 2.927  loss_mask_1: 1.952  loss_dice_1: 2.58  loss_ce_2: 3.134  loss_mask_2: 1.866  loss_dice_2: 2.584  loss_ce_3: 3.014  loss_mask_3: 2.016  loss_dice_3: 2.592  loss_ce_4: 3.058  loss_mask_4: 2.141  loss_dice_4: 2.784  loss_ce_5: 2.974  loss_mask_5: 2.001  loss_dice_5: 2.514  loss_ce_6: 3.04  loss_mask_6: 1.881  loss_dice_6: 2.752  loss_ce_7: 3.135  loss_mask_7: 1.961  loss_dice_7: 2.675  loss_ce_8: 3.101  loss_mask_8: 2.052  loss_dice_8: 2.613    time: 0.4415  last_time: 0.4537  data_time: 0.0271  last_data_time: 0.0281   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:47:37 d2.utils.events]: \u001b[0m eta: 1 day, 19:29:36  iter: 3979  total_loss: 74.05  loss_ce: 3.005  loss_mask: 1.905  loss_dice: 2.729  loss_ce_0: 3.41  loss_mask_0: 1.791  loss_dice_0: 2.537  loss_ce_1: 2.897  loss_mask_1: 1.986  loss_dice_1: 2.615  loss_ce_2: 2.997  loss_mask_2: 1.764  loss_dice_2: 2.542  loss_ce_3: 3.005  loss_mask_3: 1.746  loss_dice_3: 2.592  loss_ce_4: 3.031  loss_mask_4: 1.726  loss_dice_4: 2.645  loss_ce_5: 3.13  loss_mask_5: 1.898  loss_dice_5: 2.815  loss_ce_6: 3.064  loss_mask_6: 1.841  loss_dice_6: 2.634  loss_ce_7: 2.998  loss_mask_7: 1.788  loss_dice_7: 2.622  loss_ce_8: 3.027  loss_mask_8: 1.814  loss_dice_8: 2.676    time: 0.4415  last_time: 0.4397  data_time: 0.0277  last_data_time: 0.0327   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:47:46 d2.evaluation.evaluator]: \u001b[0mStart inference on 69 batches\n",
      "\u001b[32m[07/17 13:47:49 d2.evaluation.evaluator]: \u001b[0mInference done 11/69. Dataloading: 0.0778 s/iter. Inference: 0.1344 s/iter. Eval: 0.0374 s/iter. Total: 0.2496 s/iter. ETA=0:00:14\n",
      "\u001b[32m[07/17 13:47:54 d2.evaluation.evaluator]: \u001b[0mInference done 29/69. Dataloading: 0.1103 s/iter. Inference: 0.1355 s/iter. Eval: 0.0345 s/iter. Total: 0.2804 s/iter. ETA=0:00:11\n",
      "\u001b[32m[07/17 13:48:00 d2.evaluation.evaluator]: \u001b[0mInference done 48/69. Dataloading: 0.1083 s/iter. Inference: 0.1358 s/iter. Eval: 0.0343 s/iter. Total: 0.2785 s/iter. ETA=0:00:05\n",
      "\u001b[32m[07/17 13:48:05 d2.evaluation.evaluator]: \u001b[0mInference done 65/69. Dataloading: 0.1136 s/iter. Inference: 0.1354 s/iter. Eval: 0.0341 s/iter. Total: 0.2833 s/iter. ETA=0:00:01\n",
      "\u001b[32m[07/17 13:48:06 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:18.107628 (0.282932 s / iter per device, on 1 devices)\n",
      "\u001b[32m[07/17 13:48:06 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:08 (0.135230 s / iter per device, on 1 devices)\n",
      "Computing IoU\n",
      "Computing IoU\n",
      "{'IoU_0.5': 0.0, 'IoU_0.75': 0.0}\n",
      "\u001b[32m[07/17 13:48:06 d2.engine.defaults]: \u001b[0mEvaluation results for coco_2017_val in csv format:\n",
      "\u001b[32m[07/17 13:48:06 d2.evaluation.testing]: \u001b[0mcopypaste: IoU_0.5=0.0\n",
      "\u001b[32m[07/17 13:48:06 d2.evaluation.testing]: \u001b[0mcopypaste: IoU_0.75=0.0\n",
      "\u001b[32m[07/17 13:48:06 d2.utils.events]: \u001b[0m eta: 1 day, 19:29:28  iter: 3999  total_loss: 78.92  loss_ce: 3.362  loss_mask: 1.966  loss_dice: 2.847  loss_ce_0: 3.656  loss_mask_0: 1.858  loss_dice_0: 2.696  loss_ce_1: 3.193  loss_mask_1: 1.907  loss_dice_1: 2.668  loss_ce_2: 3.338  loss_mask_2: 1.776  loss_dice_2: 2.691  loss_ce_3: 3.376  loss_mask_3: 1.788  loss_dice_3: 2.822  loss_ce_4: 3.369  loss_mask_4: 2.007  loss_dice_4: 2.866  loss_ce_5: 3.429  loss_mask_5: 1.945  loss_dice_5: 2.856  loss_ce_6: 3.341  loss_mask_6: 1.904  loss_dice_6: 2.851  loss_ce_7: 3.424  loss_mask_7: 1.812  loss_dice_7: 2.671  loss_ce_8: 3.338  loss_mask_8: 1.815  loss_dice_8: 2.803    time: 0.4414  last_time: 0.4247  data_time: 0.0253  last_data_time: 0.0187   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:48:15 d2.utils.events]: \u001b[0m eta: 1 day, 19:29:06  iter: 4019  total_loss: 76.77  loss_ce: 2.892  loss_mask: 1.976  loss_dice: 2.603  loss_ce_0: 3.55  loss_mask_0: 1.877  loss_dice_0: 2.698  loss_ce_1: 2.964  loss_mask_1: 1.787  loss_dice_1: 2.586  loss_ce_2: 3.068  loss_mask_2: 1.759  loss_dice_2: 2.474  loss_ce_3: 3.013  loss_mask_3: 1.903  loss_dice_3: 2.574  loss_ce_4: 2.98  loss_mask_4: 1.807  loss_dice_4: 2.525  loss_ce_5: 3.008  loss_mask_5: 1.879  loss_dice_5: 2.694  loss_ce_6: 3.001  loss_mask_6: 2.054  loss_dice_6: 2.622  loss_ce_7: 2.985  loss_mask_7: 1.917  loss_dice_7: 2.68  loss_ce_8: 2.901  loss_mask_8: 1.971  loss_dice_8: 2.535    time: 0.4416  last_time: 0.4221  data_time: 0.0251  last_data_time: 0.0206   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:48:24 d2.utils.events]: \u001b[0m eta: 1 day, 19:28:49  iter: 4039  total_loss: 75.91  loss_ce: 2.853  loss_mask: 1.811  loss_dice: 2.659  loss_ce_0: 3.38  loss_mask_0: 1.856  loss_dice_0: 2.82  loss_ce_1: 2.802  loss_mask_1: 1.784  loss_dice_1: 2.594  loss_ce_2: 2.844  loss_mask_2: 1.669  loss_dice_2: 2.688  loss_ce_3: 2.894  loss_mask_3: 1.856  loss_dice_3: 2.53  loss_ce_4: 2.883  loss_mask_4: 1.81  loss_dice_4: 2.693  loss_ce_5: 2.898  loss_mask_5: 1.877  loss_dice_5: 2.602  loss_ce_6: 2.885  loss_mask_6: 1.984  loss_dice_6: 2.88  loss_ce_7: 2.856  loss_mask_7: 1.837  loss_dice_7: 2.781  loss_ce_8: 2.845  loss_mask_8: 1.871  loss_dice_8: 2.732    time: 0.4415  last_time: 0.4255  data_time: 0.0220  last_data_time: 0.0199   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:48:32 d2.utils.events]: \u001b[0m eta: 1 day, 19:28:41  iter: 4059  total_loss: 81.03  loss_ce: 3.564  loss_mask: 1.726  loss_dice: 2.754  loss_ce_0: 3.747  loss_mask_0: 1.814  loss_dice_0: 2.748  loss_ce_1: 3.449  loss_mask_1: 1.604  loss_dice_1: 2.445  loss_ce_2: 3.535  loss_mask_2: 1.881  loss_dice_2: 2.59  loss_ce_3: 3.622  loss_mask_3: 1.604  loss_dice_3: 2.419  loss_ce_4: 3.574  loss_mask_4: 1.794  loss_dice_4: 2.548  loss_ce_5: 3.466  loss_mask_5: 1.72  loss_dice_5: 2.475  loss_ce_6: 3.575  loss_mask_6: 1.713  loss_dice_6: 2.721  loss_ce_7: 3.616  loss_mask_7: 1.826  loss_dice_7: 2.413  loss_ce_8: 3.588  loss_mask_8: 1.839  loss_dice_8: 2.571    time: 0.4415  last_time: 0.4262  data_time: 0.0234  last_data_time: 0.0160   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:48:41 d2.utils.events]: \u001b[0m eta: 1 day, 19:28:38  iter: 4079  total_loss: 82.1  loss_ce: 3.52  loss_mask: 1.884  loss_dice: 2.727  loss_ce_0: 3.902  loss_mask_0: 1.82  loss_dice_0: 2.756  loss_ce_1: 3.549  loss_mask_1: 1.739  loss_dice_1: 2.586  loss_ce_2: 3.72  loss_mask_2: 1.841  loss_dice_2: 2.517  loss_ce_3: 3.579  loss_mask_3: 1.839  loss_dice_3: 2.718  loss_ce_4: 3.615  loss_mask_4: 1.831  loss_dice_4: 2.55  loss_ce_5: 3.694  loss_mask_5: 1.819  loss_dice_5: 2.6  loss_ce_6: 3.538  loss_mask_6: 1.845  loss_dice_6: 2.768  loss_ce_7: 3.588  loss_mask_7: 1.905  loss_dice_7: 2.8  loss_ce_8: 3.591  loss_mask_8: 1.885  loss_dice_8: 2.557    time: 0.4415  last_time: 0.4707  data_time: 0.0237  last_data_time: 0.0408   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:48:51 d2.utils.events]: \u001b[0m eta: 1 day, 19:28:32  iter: 4099  total_loss: 80.8  loss_ce: 3.51  loss_mask: 1.775  loss_dice: 2.684  loss_ce_0: 3.846  loss_mask_0: 1.906  loss_dice_0: 2.836  loss_ce_1: 3.546  loss_mask_1: 1.683  loss_dice_1: 2.496  loss_ce_2: 3.531  loss_mask_2: 1.764  loss_dice_2: 2.743  loss_ce_3: 3.588  loss_mask_3: 1.714  loss_dice_3: 2.791  loss_ce_4: 3.598  loss_mask_4: 1.819  loss_dice_4: 2.957  loss_ce_5: 3.578  loss_mask_5: 1.862  loss_dice_5: 2.756  loss_ce_6: 3.521  loss_mask_6: 1.75  loss_dice_6: 2.864  loss_ce_7: 3.472  loss_mask_7: 1.816  loss_dice_7: 2.963  loss_ce_8: 3.534  loss_mask_8: 1.847  loss_dice_8: 2.796    time: 0.4416  last_time: 1.0845  data_time: 0.0245  last_data_time: 0.0556   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:48:59 d2.utils.events]: \u001b[0m eta: 1 day, 19:28:25  iter: 4119  total_loss: 77.4  loss_ce: 3.195  loss_mask: 1.914  loss_dice: 2.507  loss_ce_0: 3.472  loss_mask_0: 1.798  loss_dice_0: 2.52  loss_ce_1: 3.081  loss_mask_1: 1.906  loss_dice_1: 2.344  loss_ce_2: 2.996  loss_mask_2: 1.952  loss_dice_2: 2.5  loss_ce_3: 3.092  loss_mask_3: 2.019  loss_dice_3: 2.526  loss_ce_4: 3.171  loss_mask_4: 1.861  loss_dice_4: 2.61  loss_ce_5: 3.133  loss_mask_5: 2.08  loss_dice_5: 2.609  loss_ce_6: 3.174  loss_mask_6: 1.99  loss_dice_6: 2.644  loss_ce_7: 3.104  loss_mask_7: 1.909  loss_dice_7: 2.509  loss_ce_8: 3.132  loss_mask_8: 2.157  loss_dice_8: 2.404    time: 0.4416  last_time: 0.4234  data_time: 0.0236  last_data_time: 0.0205   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:49:08 d2.utils.events]: \u001b[0m eta: 1 day, 19:28:27  iter: 4139  total_loss: 79.33  loss_ce: 2.906  loss_mask: 2.031  loss_dice: 2.91  loss_ce_0: 3.447  loss_mask_0: 1.934  loss_dice_0: 2.78  loss_ce_1: 3.026  loss_mask_1: 1.706  loss_dice_1: 2.674  loss_ce_2: 2.98  loss_mask_2: 1.933  loss_dice_2: 2.642  loss_ce_3: 3.031  loss_mask_3: 1.887  loss_dice_3: 2.699  loss_ce_4: 2.949  loss_mask_4: 2.04  loss_dice_4: 2.848  loss_ce_5: 3.054  loss_mask_5: 1.722  loss_dice_5: 2.824  loss_ce_6: 2.962  loss_mask_6: 1.941  loss_dice_6: 2.774  loss_ce_7: 2.952  loss_mask_7: 1.996  loss_dice_7: 2.69  loss_ce_8: 3.029  loss_mask_8: 1.964  loss_dice_8: 2.751    time: 0.4415  last_time: 0.4332  data_time: 0.0218  last_data_time: 0.0245   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:49:17 d2.utils.events]: \u001b[0m eta: 1 day, 19:29:11  iter: 4159  total_loss: 82.2  loss_ce: 3.433  loss_mask: 1.695  loss_dice: 2.83  loss_ce_0: 3.759  loss_mask_0: 1.752  loss_dice_0: 2.923  loss_ce_1: 3.489  loss_mask_1: 1.759  loss_dice_1: 2.754  loss_ce_2: 3.47  loss_mask_2: 1.923  loss_dice_2: 2.763  loss_ce_3: 3.49  loss_mask_3: 1.891  loss_dice_3: 2.869  loss_ce_4: 3.478  loss_mask_4: 1.788  loss_dice_4: 2.631  loss_ce_5: 3.558  loss_mask_5: 1.97  loss_dice_5: 2.872  loss_ce_6: 3.567  loss_mask_6: 1.735  loss_dice_6: 2.712  loss_ce_7: 3.395  loss_mask_7: 1.862  loss_dice_7: 2.771  loss_ce_8: 3.507  loss_mask_8: 1.778  loss_dice_8: 2.75    time: 0.4417  last_time: 0.4365  data_time: 0.0243  last_data_time: 0.0227   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:49:26 d2.utils.events]: \u001b[0m eta: 1 day, 19:29:07  iter: 4179  total_loss: 76.86  loss_ce: 3.106  loss_mask: 1.927  loss_dice: 2.57  loss_ce_0: 3.616  loss_mask_0: 1.823  loss_dice_0: 2.569  loss_ce_1: 3.132  loss_mask_1: 1.715  loss_dice_1: 2.557  loss_ce_2: 3.256  loss_mask_2: 1.903  loss_dice_2: 2.369  loss_ce_3: 3.264  loss_mask_3: 1.894  loss_dice_3: 2.366  loss_ce_4: 3.254  loss_mask_4: 1.933  loss_dice_4: 2.606  loss_ce_5: 3.21  loss_mask_5: 1.957  loss_dice_5: 2.61  loss_ce_6: 3.103  loss_mask_6: 2.044  loss_dice_6: 2.694  loss_ce_7: 3.305  loss_mask_7: 2.085  loss_dice_7: 2.547  loss_ce_8: 3.185  loss_mask_8: 1.924  loss_dice_8: 2.449    time: 0.4416  last_time: 0.4255  data_time: 0.0225  last_data_time: 0.0225   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:49:35 d2.utils.events]: \u001b[0m eta: 1 day, 19:29:07  iter: 4199  total_loss: 71.86  loss_ce: 2.673  loss_mask: 1.624  loss_dice: 2.607  loss_ce_0: 3.174  loss_mask_0: 1.806  loss_dice_0: 2.789  loss_ce_1: 2.659  loss_mask_1: 1.493  loss_dice_1: 2.555  loss_ce_2: 2.688  loss_mask_2: 1.715  loss_dice_2: 2.535  loss_ce_3: 2.776  loss_mask_3: 1.495  loss_dice_3: 2.836  loss_ce_4: 2.807  loss_mask_4: 1.641  loss_dice_4: 2.675  loss_ce_5: 2.789  loss_mask_5: 1.739  loss_dice_5: 2.72  loss_ce_6: 2.629  loss_mask_6: 2.065  loss_dice_6: 2.972  loss_ce_7: 2.658  loss_mask_7: 1.877  loss_dice_7: 2.658  loss_ce_8: 2.684  loss_mask_8: 1.909  loss_dice_8: 2.973    time: 0.4416  last_time: 0.4305  data_time: 0.0221  last_data_time: 0.0198   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:49:43 d2.utils.events]: \u001b[0m eta: 1 day, 19:29:10  iter: 4219  total_loss: 84.01  loss_ce: 3.211  loss_mask: 1.894  loss_dice: 2.764  loss_ce_0: 3.743  loss_mask_0: 1.939  loss_dice_0: 2.76  loss_ce_1: 3.247  loss_mask_1: 1.907  loss_dice_1: 2.437  loss_ce_2: 3.228  loss_mask_2: 2.072  loss_dice_2: 2.584  loss_ce_3: 3.154  loss_mask_3: 2.162  loss_dice_3: 2.687  loss_ce_4: 3.294  loss_mask_4: 2.119  loss_dice_4: 2.563  loss_ce_5: 3.245  loss_mask_5: 2.079  loss_dice_5: 2.751  loss_ce_6: 3.289  loss_mask_6: 2.092  loss_dice_6: 2.863  loss_ce_7: 3.135  loss_mask_7: 2.268  loss_dice_7: 2.754  loss_ce_8: 3.257  loss_mask_8: 2.12  loss_dice_8: 2.803    time: 0.4415  last_time: 0.4255  data_time: 0.0227  last_data_time: 0.0211   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:49:52 d2.utils.events]: \u001b[0m eta: 1 day, 19:29:04  iter: 4239  total_loss: 78.05  loss_ce: 2.941  loss_mask: 1.993  loss_dice: 2.913  loss_ce_0: 3.273  loss_mask_0: 1.791  loss_dice_0: 2.842  loss_ce_1: 3.033  loss_mask_1: 1.853  loss_dice_1: 2.667  loss_ce_2: 3.043  loss_mask_2: 1.772  loss_dice_2: 2.652  loss_ce_3: 3.106  loss_mask_3: 1.783  loss_dice_3: 2.643  loss_ce_4: 3.091  loss_mask_4: 1.922  loss_dice_4: 2.743  loss_ce_5: 3.148  loss_mask_5: 1.904  loss_dice_5: 2.758  loss_ce_6: 3.114  loss_mask_6: 1.851  loss_dice_6: 2.946  loss_ce_7: 2.99  loss_mask_7: 2.013  loss_dice_7: 2.781  loss_ce_8: 3.17  loss_mask_8: 1.901  loss_dice_8: 2.761    time: 0.4415  last_time: 0.4220  data_time: 0.0220  last_data_time: 0.0188   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:50:02 d2.utils.events]: \u001b[0m eta: 1 day, 19:29:51  iter: 4259  total_loss: 76.59  loss_ce: 3.005  loss_mask: 1.675  loss_dice: 2.913  loss_ce_0: 3.542  loss_mask_0: 1.613  loss_dice_0: 2.786  loss_ce_1: 3.001  loss_mask_1: 1.778  loss_dice_1: 2.906  loss_ce_2: 3.061  loss_mask_2: 1.677  loss_dice_2: 2.693  loss_ce_3: 3.078  loss_mask_3: 1.778  loss_dice_3: 2.841  loss_ce_4: 3.052  loss_mask_4: 1.802  loss_dice_4: 2.894  loss_ce_5: 3.07  loss_mask_5: 1.709  loss_dice_5: 2.892  loss_ce_6: 3.076  loss_mask_6: 1.815  loss_dice_6: 3.021  loss_ce_7: 3.18  loss_mask_7: 1.783  loss_dice_7: 2.896  loss_ce_8: 3.13  loss_mask_8: 1.888  loss_dice_8: 2.893    time: 0.4417  last_time: 0.4537  data_time: 0.0243  last_data_time: 0.0402   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:50:10 d2.utils.events]: \u001b[0m eta: 1 day, 19:30:04  iter: 4279  total_loss: 79.57  loss_ce: 3.119  loss_mask: 1.732  loss_dice: 3.126  loss_ce_0: 3.434  loss_mask_0: 1.646  loss_dice_0: 3.115  loss_ce_1: 3.176  loss_mask_1: 1.578  loss_dice_1: 3.009  loss_ce_2: 3.287  loss_mask_2: 1.759  loss_dice_2: 2.912  loss_ce_3: 3.349  loss_mask_3: 1.715  loss_dice_3: 2.845  loss_ce_4: 3.208  loss_mask_4: 1.825  loss_dice_4: 2.873  loss_ce_5: 3.234  loss_mask_5: 1.837  loss_dice_5: 2.998  loss_ce_6: 3.172  loss_mask_6: 1.776  loss_dice_6: 3.115  loss_ce_7: 3.24  loss_mask_7: 1.642  loss_dice_7: 2.854  loss_ce_8: 3.275  loss_mask_8: 1.714  loss_dice_8: 3.046    time: 0.4417  last_time: 0.4315  data_time: 0.0220  last_data_time: 0.0207   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:50:19 d2.utils.events]: \u001b[0m eta: 1 day, 19:29:34  iter: 4299  total_loss: 73.82  loss_ce: 2.701  loss_mask: 1.822  loss_dice: 2.745  loss_ce_0: 3.227  loss_mask_0: 1.888  loss_dice_0: 2.836  loss_ce_1: 2.549  loss_mask_1: 1.735  loss_dice_1: 2.552  loss_ce_2: 2.695  loss_mask_2: 1.849  loss_dice_2: 2.542  loss_ce_3: 2.621  loss_mask_3: 1.789  loss_dice_3: 2.532  loss_ce_4: 2.744  loss_mask_4: 1.996  loss_dice_4: 2.657  loss_ce_5: 2.645  loss_mask_5: 1.83  loss_dice_5: 2.832  loss_ce_6: 2.66  loss_mask_6: 1.808  loss_dice_6: 2.762  loss_ce_7: 2.599  loss_mask_7: 1.856  loss_dice_7: 2.644  loss_ce_8: 2.648  loss_mask_8: 2.045  loss_dice_8: 2.763    time: 0.4416  last_time: 0.4251  data_time: 0.0221  last_data_time: 0.0212   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:50:28 d2.utils.events]: \u001b[0m eta: 1 day, 19:30:15  iter: 4319  total_loss: 82.39  loss_ce: 3.709  loss_mask: 2.076  loss_dice: 2.742  loss_ce_0: 3.86  loss_mask_0: 1.747  loss_dice_0: 2.745  loss_ce_1: 3.617  loss_mask_1: 1.929  loss_dice_1: 2.684  loss_ce_2: 3.772  loss_mask_2: 1.893  loss_dice_2: 2.668  loss_ce_3: 3.716  loss_mask_3: 1.837  loss_dice_3: 2.643  loss_ce_4: 3.632  loss_mask_4: 1.845  loss_dice_4: 2.625  loss_ce_5: 3.581  loss_mask_5: 1.907  loss_dice_5: 2.735  loss_ce_6: 3.8  loss_mask_6: 1.666  loss_dice_6: 2.586  loss_ce_7: 3.632  loss_mask_7: 1.947  loss_dice_7: 2.74  loss_ce_8: 3.752  loss_mask_8: 1.868  loss_dice_8: 2.771    time: 0.4417  last_time: 0.4262  data_time: 0.0265  last_data_time: 0.0214   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:50:51 d2.utils.events]: \u001b[0m eta: 1 day, 19:30:43  iter: 4339  total_loss: 74.17  loss_ce: 2.912  loss_mask: 1.721  loss_dice: 2.614  loss_ce_0: 3.42  loss_mask_0: 1.788  loss_dice_0: 2.706  loss_ce_1: 2.978  loss_mask_1: 1.682  loss_dice_1: 2.523  loss_ce_2: 3.017  loss_mask_2: 1.75  loss_dice_2: 2.685  loss_ce_3: 3.028  loss_mask_3: 1.832  loss_dice_3: 2.569  loss_ce_4: 2.958  loss_mask_4: 1.656  loss_dice_4: 2.435  loss_ce_5: 3.019  loss_mask_5: 1.794  loss_dice_5: 2.464  loss_ce_6: 3.015  loss_mask_6: 1.811  loss_dice_6: 2.829  loss_ce_7: 2.891  loss_mask_7: 1.806  loss_dice_7: 2.546  loss_ce_8: 2.886  loss_mask_8: 1.906  loss_dice_8: 2.69    time: 0.4450  last_time: 0.4309  data_time: 0.0238  last_data_time: 0.0250   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:51:00 d2.utils.events]: \u001b[0m eta: 1 day, 19:30:42  iter: 4359  total_loss: 79.85  loss_ce: 2.97  loss_mask: 1.891  loss_dice: 2.832  loss_ce_0: 3.502  loss_mask_0: 1.96  loss_dice_0: 2.872  loss_ce_1: 3.101  loss_mask_1: 1.941  loss_dice_1: 2.687  loss_ce_2: 3.119  loss_mask_2: 1.862  loss_dice_2: 2.682  loss_ce_3: 3.053  loss_mask_3: 1.925  loss_dice_3: 2.712  loss_ce_4: 3.083  loss_mask_4: 2.096  loss_dice_4: 2.934  loss_ce_5: 3.082  loss_mask_5: 1.877  loss_dice_5: 2.835  loss_ce_6: 3.154  loss_mask_6: 1.862  loss_dice_6: 2.8  loss_ce_7: 3.099  loss_mask_7: 2.022  loss_dice_7: 2.804  loss_ce_8: 3.174  loss_mask_8: 1.871  loss_dice_8: 2.78    time: 0.4449  last_time: 0.4952  data_time: 0.0221  last_data_time: 0.0197   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:51:10 d2.utils.events]: \u001b[0m eta: 1 day, 19:30:42  iter: 4379  total_loss: 78.76  loss_ce: 3.102  loss_mask: 1.911  loss_dice: 2.792  loss_ce_0: 3.659  loss_mask_0: 1.95  loss_dice_0: 2.66  loss_ce_1: 3.13  loss_mask_1: 1.815  loss_dice_1: 2.566  loss_ce_2: 3.155  loss_mask_2: 1.879  loss_dice_2: 2.593  loss_ce_3: 3.168  loss_mask_3: 1.899  loss_dice_3: 2.676  loss_ce_4: 3.157  loss_mask_4: 1.9  loss_dice_4: 2.823  loss_ce_5: 3.149  loss_mask_5: 2.095  loss_dice_5: 2.656  loss_ce_6: 3.111  loss_mask_6: 2.081  loss_dice_6: 2.788  loss_ce_7: 3.232  loss_mask_7: 2.031  loss_dice_7: 2.71  loss_ce_8: 3.152  loss_mask_8: 2.081  loss_dice_8: 2.631    time: 0.4452  last_time: 0.4298  data_time: 0.0248  last_data_time: 0.0228   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:51:20 d2.utils.events]: \u001b[0m eta: 1 day, 19:31:15  iter: 4399  total_loss: 83.68  loss_ce: 3.483  loss_mask: 2.026  loss_dice: 2.774  loss_ce_0: 3.802  loss_mask_0: 1.908  loss_dice_0: 2.662  loss_ce_1: 3.466  loss_mask_1: 1.899  loss_dice_1: 2.495  loss_ce_2: 3.603  loss_mask_2: 2.046  loss_dice_2: 2.546  loss_ce_3: 3.4  loss_mask_3: 1.992  loss_dice_3: 2.669  loss_ce_4: 3.51  loss_mask_4: 2.113  loss_dice_4: 2.712  loss_ce_5: 3.502  loss_mask_5: 1.952  loss_dice_5: 2.792  loss_ce_6: 3.553  loss_mask_6: 2.124  loss_dice_6: 2.763  loss_ce_7: 3.517  loss_mask_7: 2.023  loss_dice_7: 2.703  loss_ce_8: 3.51  loss_mask_8: 2.018  loss_dice_8: 2.678    time: 0.4454  last_time: 0.4352  data_time: 0.0246  last_data_time: 0.0287   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:51:30 d2.utils.events]: \u001b[0m eta: 1 day, 19:31:09  iter: 4419  total_loss: 80.46  loss_ce: 3.308  loss_mask: 1.95  loss_dice: 2.843  loss_ce_0: 3.549  loss_mask_0: 2.025  loss_dice_0: 2.785  loss_ce_1: 3.199  loss_mask_1: 2.038  loss_dice_1: 2.849  loss_ce_2: 3.286  loss_mask_2: 1.952  loss_dice_2: 2.786  loss_ce_3: 3.256  loss_mask_3: 2.088  loss_dice_3: 2.909  loss_ce_4: 3.328  loss_mask_4: 2.031  loss_dice_4: 2.849  loss_ce_5: 3.307  loss_mask_5: 1.951  loss_dice_5: 3  loss_ce_6: 3.339  loss_mask_6: 1.873  loss_dice_6: 2.885  loss_ce_7: 3.314  loss_mask_7: 1.896  loss_dice_7: 2.9  loss_ce_8: 3.374  loss_mask_8: 1.987  loss_dice_8: 2.811    time: 0.4457  last_time: 0.4241  data_time: 0.0220  last_data_time: 0.0210   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:51:55 d2.utils.events]: \u001b[0m eta: 1 day, 19:31:14  iter: 4439  total_loss: 83.31  loss_ce: 3.691  loss_mask: 1.999  loss_dice: 2.561  loss_ce_0: 3.873  loss_mask_0: 1.882  loss_dice_0: 2.613  loss_ce_1: 3.665  loss_mask_1: 1.856  loss_dice_1: 2.529  loss_ce_2: 3.703  loss_mask_2: 1.834  loss_dice_2: 2.52  loss_ce_3: 3.763  loss_mask_3: 1.907  loss_dice_3: 2.556  loss_ce_4: 3.815  loss_mask_4: 1.837  loss_dice_4: 2.601  loss_ce_5: 3.838  loss_mask_5: 1.954  loss_dice_5: 2.754  loss_ce_6: 3.751  loss_mask_6: 1.906  loss_dice_6: 2.762  loss_ce_7: 3.791  loss_mask_7: 2.189  loss_dice_7: 2.74  loss_ce_8: 3.75  loss_mask_8: 2.056  loss_dice_8: 2.757    time: 0.4492  last_time: 0.4355  data_time: 0.1019  last_data_time: 0.0270   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:52:03 d2.utils.events]: \u001b[0m eta: 1 day, 19:30:51  iter: 4459  total_loss: 81.51  loss_ce: 3.362  loss_mask: 1.799  loss_dice: 2.691  loss_ce_0: 3.579  loss_mask_0: 1.963  loss_dice_0: 2.801  loss_ce_1: 3.072  loss_mask_1: 1.985  loss_dice_1: 2.576  loss_ce_2: 3.318  loss_mask_2: 1.89  loss_dice_2: 2.592  loss_ce_3: 3.234  loss_mask_3: 1.917  loss_dice_3: 2.614  loss_ce_4: 3.245  loss_mask_4: 2.083  loss_dice_4: 2.823  loss_ce_5: 3.141  loss_mask_5: 2.006  loss_dice_5: 2.707  loss_ce_6: 3.437  loss_mask_6: 1.921  loss_dice_6: 2.74  loss_ce_7: 3.465  loss_mask_7: 1.922  loss_dice_7: 2.76  loss_ce_8: 3.28  loss_mask_8: 1.863  loss_dice_8: 2.749    time: 0.4491  last_time: 0.4236  data_time: 0.0222  last_data_time: 0.0198   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:52:12 d2.utils.events]: \u001b[0m eta: 1 day, 19:30:56  iter: 4479  total_loss: 77.04  loss_ce: 3.415  loss_mask: 1.786  loss_dice: 2.575  loss_ce_0: 3.654  loss_mask_0: 1.68  loss_dice_0: 2.712  loss_ce_1: 3.353  loss_mask_1: 1.647  loss_dice_1: 2.659  loss_ce_2: 3.516  loss_mask_2: 1.621  loss_dice_2: 2.64  loss_ce_3: 3.443  loss_mask_3: 1.634  loss_dice_3: 2.699  loss_ce_4: 3.357  loss_mask_4: 1.74  loss_dice_4: 2.79  loss_ce_5: 3.492  loss_mask_5: 1.763  loss_dice_5: 2.583  loss_ce_6: 3.359  loss_mask_6: 1.684  loss_dice_6: 2.793  loss_ce_7: 3.433  loss_mask_7: 1.702  loss_dice_7: 2.684  loss_ce_8: 3.4  loss_mask_8: 1.856  loss_dice_8: 2.755    time: 0.4491  last_time: 0.4200  data_time: 0.0230  last_data_time: 0.0180   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:52:21 d2.utils.events]: \u001b[0m eta: 1 day, 19:30:54  iter: 4499  total_loss: 75.25  loss_ce: 2.795  loss_mask: 2.082  loss_dice: 2.78  loss_ce_0: 3.199  loss_mask_0: 1.829  loss_dice_0: 2.706  loss_ce_1: 2.875  loss_mask_1: 1.931  loss_dice_1: 2.504  loss_ce_2: 2.856  loss_mask_2: 1.927  loss_dice_2: 2.564  loss_ce_3: 2.892  loss_mask_3: 1.924  loss_dice_3: 2.598  loss_ce_4: 3.018  loss_mask_4: 1.996  loss_dice_4: 2.711  loss_ce_5: 2.925  loss_mask_5: 2.063  loss_dice_5: 2.665  loss_ce_6: 2.866  loss_mask_6: 1.87  loss_dice_6: 2.771  loss_ce_7: 2.795  loss_mask_7: 1.98  loss_dice_7: 2.782  loss_ce_8: 2.786  loss_mask_8: 2.057  loss_dice_8: 2.753    time: 0.4490  last_time: 0.5391  data_time: 0.0228  last_data_time: 0.0198   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:52:29 d2.utils.events]: \u001b[0m eta: 1 day, 19:30:55  iter: 4519  total_loss: 79.37  loss_ce: 3.218  loss_mask: 1.813  loss_dice: 2.618  loss_ce_0: 3.632  loss_mask_0: 1.824  loss_dice_0: 2.667  loss_ce_1: 3.235  loss_mask_1: 1.731  loss_dice_1: 2.48  loss_ce_2: 3.324  loss_mask_2: 1.892  loss_dice_2: 2.513  loss_ce_3: 3.221  loss_mask_3: 1.83  loss_dice_3: 2.574  loss_ce_4: 3.215  loss_mask_4: 1.947  loss_dice_4: 2.758  loss_ce_5: 3.22  loss_mask_5: 1.7  loss_dice_5: 2.565  loss_ce_6: 3.246  loss_mask_6: 1.865  loss_dice_6: 2.669  loss_ce_7: 3.14  loss_mask_7: 1.827  loss_dice_7: 2.769  loss_ce_8: 3.126  loss_mask_8: 1.866  loss_dice_8: 2.579    time: 0.4489  last_time: 0.4300  data_time: 0.0236  last_data_time: 0.0269   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:52:38 d2.utils.events]: \u001b[0m eta: 1 day, 19:30:53  iter: 4539  total_loss: 78.08  loss_ce: 3.272  loss_mask: 1.759  loss_dice: 2.824  loss_ce_0: 3.419  loss_mask_0: 1.842  loss_dice_0: 2.823  loss_ce_1: 3.027  loss_mask_1: 1.804  loss_dice_1: 2.668  loss_ce_2: 3.144  loss_mask_2: 1.868  loss_dice_2: 2.661  loss_ce_3: 3.033  loss_mask_3: 1.876  loss_dice_3: 2.621  loss_ce_4: 3.162  loss_mask_4: 1.734  loss_dice_4: 2.699  loss_ce_5: 3.118  loss_mask_5: 1.943  loss_dice_5: 2.874  loss_ce_6: 3.127  loss_mask_6: 1.737  loss_dice_6: 2.702  loss_ce_7: 3.113  loss_mask_7: 1.834  loss_dice_7: 2.843  loss_ce_8: 3.022  loss_mask_8: 2.003  loss_dice_8: 2.954    time: 0.4489  last_time: 0.4261  data_time: 0.0251  last_data_time: 0.0219   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:52:47 d2.utils.events]: \u001b[0m eta: 1 day, 19:30:44  iter: 4559  total_loss: 76.71  loss_ce: 3.147  loss_mask: 1.834  loss_dice: 2.671  loss_ce_0: 3.552  loss_mask_0: 1.855  loss_dice_0: 2.699  loss_ce_1: 3.075  loss_mask_1: 1.738  loss_dice_1: 2.522  loss_ce_2: 3.104  loss_mask_2: 1.68  loss_dice_2: 2.587  loss_ce_3: 3.025  loss_mask_3: 1.889  loss_dice_3: 2.755  loss_ce_4: 3.06  loss_mask_4: 1.873  loss_dice_4: 2.715  loss_ce_5: 3.122  loss_mask_5: 1.914  loss_dice_5: 2.761  loss_ce_6: 3.1  loss_mask_6: 2.208  loss_dice_6: 2.829  loss_ce_7: 3.115  loss_mask_7: 1.981  loss_dice_7: 2.729  loss_ce_8: 3.104  loss_mask_8: 1.898  loss_dice_8: 2.693    time: 0.4488  last_time: 0.4318  data_time: 0.0219  last_data_time: 0.0245   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:52:55 d2.utils.events]: \u001b[0m eta: 1 day, 19:30:36  iter: 4579  total_loss: 79.48  loss_ce: 3.492  loss_mask: 1.843  loss_dice: 2.716  loss_ce_0: 3.72  loss_mask_0: 1.947  loss_dice_0: 2.541  loss_ce_1: 3.448  loss_mask_1: 1.794  loss_dice_1: 2.423  loss_ce_2: 3.473  loss_mask_2: 1.827  loss_dice_2: 2.423  loss_ce_3: 3.456  loss_mask_3: 1.854  loss_dice_3: 2.639  loss_ce_4: 3.463  loss_mask_4: 1.89  loss_dice_4: 2.402  loss_ce_5: 3.524  loss_mask_5: 1.908  loss_dice_5: 2.681  loss_ce_6: 3.363  loss_mask_6: 2.011  loss_dice_6: 2.587  loss_ce_7: 3.539  loss_mask_7: 2.056  loss_dice_7: 2.453  loss_ce_8: 3.453  loss_mask_8: 1.992  loss_dice_8: 2.564    time: 0.4487  last_time: 0.4328  data_time: 0.0230  last_data_time: 0.0255   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:53:04 d2.utils.events]: \u001b[0m eta: 1 day, 19:30:38  iter: 4599  total_loss: 82.06  loss_ce: 3.396  loss_mask: 1.748  loss_dice: 2.733  loss_ce_0: 3.619  loss_mask_0: 1.669  loss_dice_0: 2.593  loss_ce_1: 3.329  loss_mask_1: 1.647  loss_dice_1: 2.576  loss_ce_2: 3.414  loss_mask_2: 1.76  loss_dice_2: 2.616  loss_ce_3: 3.351  loss_mask_3: 1.669  loss_dice_3: 2.608  loss_ce_4: 3.442  loss_mask_4: 2.016  loss_dice_4: 2.724  loss_ce_5: 3.459  loss_mask_5: 1.811  loss_dice_5: 2.856  loss_ce_6: 3.454  loss_mask_6: 1.753  loss_dice_6: 2.776  loss_ce_7: 3.452  loss_mask_7: 1.639  loss_dice_7: 2.65  loss_ce_8: 3.452  loss_mask_8: 1.891  loss_dice_8: 2.646    time: 0.4486  last_time: 0.4319  data_time: 0.0228  last_data_time: 0.0259   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:53:13 d2.utils.events]: \u001b[0m eta: 1 day, 19:30:26  iter: 4619  total_loss: 78.11  loss_ce: 3.296  loss_mask: 1.969  loss_dice: 2.674  loss_ce_0: 3.567  loss_mask_0: 1.868  loss_dice_0: 2.579  loss_ce_1: 3.353  loss_mask_1: 1.605  loss_dice_1: 2.621  loss_ce_2: 3.192  loss_mask_2: 1.719  loss_dice_2: 2.484  loss_ce_3: 3.259  loss_mask_3: 1.645  loss_dice_3: 2.555  loss_ce_4: 3.263  loss_mask_4: 1.518  loss_dice_4: 2.425  loss_ce_5: 3.338  loss_mask_5: 1.741  loss_dice_5: 2.722  loss_ce_6: 3.274  loss_mask_6: 1.776  loss_dice_6: 2.56  loss_ce_7: 3.188  loss_mask_7: 1.847  loss_dice_7: 2.602  loss_ce_8: 3.205  loss_mask_8: 1.72  loss_dice_8: 2.431    time: 0.4485  last_time: 0.4312  data_time: 0.0221  last_data_time: 0.0226   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:53:21 d2.utils.events]: \u001b[0m eta: 1 day, 19:30:03  iter: 4639  total_loss: 77.7  loss_ce: 3.208  loss_mask: 1.922  loss_dice: 2.617  loss_ce_0: 3.47  loss_mask_0: 1.853  loss_dice_0: 2.782  loss_ce_1: 3.043  loss_mask_1: 1.694  loss_dice_1: 2.656  loss_ce_2: 3.06  loss_mask_2: 1.849  loss_dice_2: 2.628  loss_ce_3: 3.062  loss_mask_3: 1.868  loss_dice_3: 2.621  loss_ce_4: 3.063  loss_mask_4: 1.858  loss_dice_4: 2.687  loss_ce_5: 2.956  loss_mask_5: 1.947  loss_dice_5: 2.785  loss_ce_6: 3.06  loss_mask_6: 1.958  loss_dice_6: 2.676  loss_ce_7: 3.143  loss_mask_7: 1.885  loss_dice_7: 2.754  loss_ce_8: 3.042  loss_mask_8: 1.77  loss_dice_8: 2.768    time: 0.4485  last_time: 0.4297  data_time: 0.0225  last_data_time: 0.0252   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:53:30 d2.utils.events]: \u001b[0m eta: 1 day, 19:29:39  iter: 4659  total_loss: 77.65  loss_ce: 2.989  loss_mask: 2.049  loss_dice: 2.613  loss_ce_0: 3.27  loss_mask_0: 2.005  loss_dice_0: 2.648  loss_ce_1: 3.089  loss_mask_1: 1.892  loss_dice_1: 2.634  loss_ce_2: 2.912  loss_mask_2: 1.917  loss_dice_2: 2.438  loss_ce_3: 2.998  loss_mask_3: 1.922  loss_dice_3: 2.449  loss_ce_4: 2.95  loss_mask_4: 2.102  loss_dice_4: 2.726  loss_ce_5: 3.068  loss_mask_5: 2.034  loss_dice_5: 2.552  loss_ce_6: 2.96  loss_mask_6: 2.076  loss_dice_6: 2.586  loss_ce_7: 2.992  loss_mask_7: 2.008  loss_dice_7: 2.702  loss_ce_8: 3.074  loss_mask_8: 2.045  loss_dice_8: 2.64    time: 0.4484  last_time: 0.4478  data_time: 0.0216  last_data_time: 0.0200   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:53:38 d2.utils.events]: \u001b[0m eta: 1 day, 19:29:18  iter: 4679  total_loss: 74.66  loss_ce: 2.487  loss_mask: 1.906  loss_dice: 2.598  loss_ce_0: 2.893  loss_mask_0: 1.868  loss_dice_0: 2.815  loss_ce_1: 2.424  loss_mask_1: 1.807  loss_dice_1: 2.641  loss_ce_2: 2.348  loss_mask_2: 1.974  loss_dice_2: 2.723  loss_ce_3: 2.41  loss_mask_3: 1.926  loss_dice_3: 2.659  loss_ce_4: 2.481  loss_mask_4: 2.007  loss_dice_4: 2.843  loss_ce_5: 2.32  loss_mask_5: 1.974  loss_dice_5: 2.661  loss_ce_6: 2.394  loss_mask_6: 1.958  loss_dice_6: 2.917  loss_ce_7: 2.437  loss_mask_7: 2  loss_dice_7: 2.748  loss_ce_8: 2.448  loss_mask_8: 1.938  loss_dice_8: 2.781    time: 0.4483  last_time: 0.4231  data_time: 0.0209  last_data_time: 0.0201   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:53:47 d2.utils.events]: \u001b[0m eta: 1 day, 19:29:07  iter: 4699  total_loss: 80.95  loss_ce: 3.033  loss_mask: 2.266  loss_dice: 2.715  loss_ce_0: 3.234  loss_mask_0: 1.946  loss_dice_0: 2.709  loss_ce_1: 3.12  loss_mask_1: 1.949  loss_dice_1: 2.607  loss_ce_2: 2.996  loss_mask_2: 2.077  loss_dice_2: 2.597  loss_ce_3: 3.063  loss_mask_3: 2.076  loss_dice_3: 2.667  loss_ce_4: 3.059  loss_mask_4: 2.115  loss_dice_4: 2.654  loss_ce_5: 3.028  loss_mask_5: 2.345  loss_dice_5: 2.785  loss_ce_6: 2.962  loss_mask_6: 2.119  loss_dice_6: 2.915  loss_ce_7: 3.032  loss_mask_7: 2.116  loss_dice_7: 2.839  loss_ce_8: 3.086  loss_mask_8: 1.958  loss_dice_8: 2.75    time: 0.4482  last_time: 0.4481  data_time: 0.0223  last_data_time: 0.0226   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:53:56 d2.utils.events]: \u001b[0m eta: 1 day, 19:28:49  iter: 4719  total_loss: 75.84  loss_ce: 2.983  loss_mask: 1.845  loss_dice: 2.5  loss_ce_0: 3.283  loss_mask_0: 1.761  loss_dice_0: 2.671  loss_ce_1: 2.995  loss_mask_1: 1.77  loss_dice_1: 2.556  loss_ce_2: 2.894  loss_mask_2: 1.748  loss_dice_2: 2.457  loss_ce_3: 2.971  loss_mask_3: 1.793  loss_dice_3: 2.582  loss_ce_4: 3.019  loss_mask_4: 1.8  loss_dice_4: 2.568  loss_ce_5: 3.139  loss_mask_5: 1.794  loss_dice_5: 2.555  loss_ce_6: 2.92  loss_mask_6: 1.937  loss_dice_6: 2.774  loss_ce_7: 3.02  loss_mask_7: 1.826  loss_dice_7: 2.565  loss_ce_8: 3.093  loss_mask_8: 1.778  loss_dice_8: 2.623    time: 0.4481  last_time: 0.4301  data_time: 0.0223  last_data_time: 0.0219   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:54:05 d2.utils.events]: \u001b[0m eta: 1 day, 19:28:52  iter: 4739  total_loss: 72.88  loss_ce: 2.867  loss_mask: 2.032  loss_dice: 2.486  loss_ce_0: 3.118  loss_mask_0: 1.856  loss_dice_0: 2.385  loss_ce_1: 2.794  loss_mask_1: 1.686  loss_dice_1: 2.409  loss_ce_2: 2.861  loss_mask_2: 1.846  loss_dice_2: 2.42  loss_ce_3: 2.897  loss_mask_3: 1.924  loss_dice_3: 2.515  loss_ce_4: 2.88  loss_mask_4: 1.934  loss_dice_4: 2.477  loss_ce_5: 2.835  loss_mask_5: 1.898  loss_dice_5: 2.423  loss_ce_6: 2.851  loss_mask_6: 2.249  loss_dice_6: 2.605  loss_ce_7: 2.816  loss_mask_7: 1.91  loss_dice_7: 2.436  loss_ce_8: 2.796  loss_mask_8: 2.176  loss_dice_8: 2.806    time: 0.4483  last_time: 0.4215  data_time: 0.0554  last_data_time: 0.0194   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:54:14 d2.utils.events]: \u001b[0m eta: 1 day, 19:28:56  iter: 4759  total_loss: 80.92  loss_ce: 3.517  loss_mask: 1.866  loss_dice: 2.722  loss_ce_0: 3.631  loss_mask_0: 1.785  loss_dice_0: 2.824  loss_ce_1: 3.411  loss_mask_1: 1.735  loss_dice_1: 2.695  loss_ce_2: 3.413  loss_mask_2: 2.072  loss_dice_2: 2.786  loss_ce_3: 3.369  loss_mask_3: 1.744  loss_dice_3: 2.706  loss_ce_4: 3.492  loss_mask_4: 1.939  loss_dice_4: 2.73  loss_ce_5: 3.526  loss_mask_5: 1.847  loss_dice_5: 2.634  loss_ce_6: 3.464  loss_mask_6: 1.998  loss_dice_6: 2.921  loss_ce_7: 3.55  loss_mask_7: 1.864  loss_dice_7: 2.802  loss_ce_8: 3.417  loss_mask_8: 1.772  loss_dice_8: 2.911    time: 0.4482  last_time: 0.4425  data_time: 0.0240  last_data_time: 0.0363   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:54:23 d2.utils.events]: \u001b[0m eta: 1 day, 19:29:01  iter: 4779  total_loss: 74.27  loss_ce: 3.287  loss_mask: 1.635  loss_dice: 2.77  loss_ce_0: 3.525  loss_mask_0: 1.657  loss_dice_0: 2.838  loss_ce_1: 3.246  loss_mask_1: 1.553  loss_dice_1: 2.466  loss_ce_2: 3.207  loss_mask_2: 1.634  loss_dice_2: 2.527  loss_ce_3: 3.252  loss_mask_3: 1.571  loss_dice_3: 2.577  loss_ce_4: 3.274  loss_mask_4: 1.6  loss_dice_4: 2.639  loss_ce_5: 3.3  loss_mask_5: 1.645  loss_dice_5: 2.724  loss_ce_6: 3.16  loss_mask_6: 1.669  loss_dice_6: 2.625  loss_ce_7: 3.2  loss_mask_7: 1.584  loss_dice_7: 2.725  loss_ce_8: 3.215  loss_mask_8: 1.795  loss_dice_8: 2.821    time: 0.4481  last_time: 0.4401  data_time: 0.0220  last_data_time: 0.0257   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:54:32 d2.utils.events]: \u001b[0m eta: 1 day, 19:28:32  iter: 4799  total_loss: 78.04  loss_ce: 3.195  loss_mask: 1.952  loss_dice: 2.662  loss_ce_0: 3.351  loss_mask_0: 1.92  loss_dice_0: 2.551  loss_ce_1: 3.13  loss_mask_1: 1.931  loss_dice_1: 2.377  loss_ce_2: 3.114  loss_mask_2: 1.934  loss_dice_2: 2.314  loss_ce_3: 3.302  loss_mask_3: 1.917  loss_dice_3: 2.468  loss_ce_4: 3.212  loss_mask_4: 1.888  loss_dice_4: 2.695  loss_ce_5: 3.149  loss_mask_5: 1.764  loss_dice_5: 2.574  loss_ce_6: 3.26  loss_mask_6: 1.824  loss_dice_6: 2.658  loss_ce_7: 3.221  loss_mask_7: 2.025  loss_dice_7: 2.507  loss_ce_8: 3.103  loss_mask_8: 1.799  loss_dice_8: 2.648    time: 0.4483  last_time: 0.4342  data_time: 0.0234  last_data_time: 0.0216   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:54:41 d2.utils.events]: \u001b[0m eta: 1 day, 19:28:58  iter: 4819  total_loss: 83.35  loss_ce: 3.564  loss_mask: 1.811  loss_dice: 2.876  loss_ce_0: 3.689  loss_mask_0: 1.812  loss_dice_0: 2.88  loss_ce_1: 3.472  loss_mask_1: 1.893  loss_dice_1: 2.739  loss_ce_2: 3.5  loss_mask_2: 1.775  loss_dice_2: 2.694  loss_ce_3: 3.509  loss_mask_3: 1.717  loss_dice_3: 2.661  loss_ce_4: 3.466  loss_mask_4: 1.846  loss_dice_4: 2.814  loss_ce_5: 3.591  loss_mask_5: 1.774  loss_dice_5: 2.747  loss_ce_6: 3.498  loss_mask_6: 1.899  loss_dice_6: 2.829  loss_ce_7: 3.522  loss_mask_7: 1.941  loss_dice_7: 2.936  loss_ce_8: 3.429  loss_mask_8: 1.994  loss_dice_8: 2.838    time: 0.4482  last_time: 0.4291  data_time: 0.0232  last_data_time: 0.0243   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:54:54 d2.utils.events]: \u001b[0m eta: 1 day, 19:28:49  iter: 4839  total_loss: 78.12  loss_ce: 3.345  loss_mask: 1.881  loss_dice: 2.619  loss_ce_0: 3.434  loss_mask_0: 1.746  loss_dice_0: 2.723  loss_ce_1: 3.284  loss_mask_1: 1.869  loss_dice_1: 2.599  loss_ce_2: 3.33  loss_mask_2: 1.842  loss_dice_2: 2.559  loss_ce_3: 3.342  loss_mask_3: 1.895  loss_dice_3: 2.69  loss_ce_4: 3.347  loss_mask_4: 1.932  loss_dice_4: 2.692  loss_ce_5: 3.408  loss_mask_5: 1.869  loss_dice_5: 2.829  loss_ce_6: 3.44  loss_mask_6: 1.888  loss_dice_6: 2.779  loss_ce_7: 3.432  loss_mask_7: 2.108  loss_dice_7: 2.813  loss_ce_8: 3.384  loss_mask_8: 1.908  loss_dice_8: 2.719    time: 0.4491  last_time: 0.4267  data_time: 0.0237  last_data_time: 0.0222   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:55:03 d2.utils.events]: \u001b[0m eta: 1 day, 19:28:40  iter: 4859  total_loss: 79.39  loss_ce: 3.031  loss_mask: 2.028  loss_dice: 2.762  loss_ce_0: 3.337  loss_mask_0: 1.831  loss_dice_0: 2.902  loss_ce_1: 2.881  loss_mask_1: 1.879  loss_dice_1: 2.771  loss_ce_2: 2.976  loss_mask_2: 1.997  loss_dice_2: 2.707  loss_ce_3: 3.013  loss_mask_3: 1.926  loss_dice_3: 2.656  loss_ce_4: 3.088  loss_mask_4: 2.04  loss_dice_4: 2.861  loss_ce_5: 3.003  loss_mask_5: 1.833  loss_dice_5: 3.004  loss_ce_6: 3.123  loss_mask_6: 1.722  loss_dice_6: 2.799  loss_ce_7: 2.9  loss_mask_7: 1.957  loss_dice_7: 2.912  loss_ce_8: 3.058  loss_mask_8: 1.96  loss_dice_8: 2.852    time: 0.4490  last_time: 0.4364  data_time: 0.0214  last_data_time: 0.0219   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:55:15 d2.utils.events]: \u001b[0m eta: 1 day, 19:28:27  iter: 4879  total_loss: 79.9  loss_ce: 3.137  loss_mask: 2.007  loss_dice: 2.7  loss_ce_0: 3.403  loss_mask_0: 1.841  loss_dice_0: 2.665  loss_ce_1: 3.216  loss_mask_1: 1.798  loss_dice_1: 2.447  loss_ce_2: 3.337  loss_mask_2: 1.914  loss_dice_2: 2.571  loss_ce_3: 3.373  loss_mask_3: 1.753  loss_dice_3: 2.654  loss_ce_4: 3.239  loss_mask_4: 1.818  loss_dice_4: 2.711  loss_ce_5: 3.385  loss_mask_5: 1.767  loss_dice_5: 2.719  loss_ce_6: 3.243  loss_mask_6: 1.849  loss_dice_6: 2.838  loss_ce_7: 3.309  loss_mask_7: 1.824  loss_dice_7: 2.772  loss_ce_8: 3.204  loss_mask_8: 1.851  loss_dice_8: 2.735    time: 0.4498  last_time: 0.4293  data_time: 0.0246  last_data_time: 0.0208   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:55:24 d2.utils.events]: \u001b[0m eta: 1 day, 19:28:09  iter: 4899  total_loss: 75.46  loss_ce: 2.943  loss_mask: 2.051  loss_dice: 2.635  loss_ce_0: 3.138  loss_mask_0: 1.98  loss_dice_0: 2.726  loss_ce_1: 2.789  loss_mask_1: 2.022  loss_dice_1: 2.574  loss_ce_2: 2.837  loss_mask_2: 1.955  loss_dice_2: 2.673  loss_ce_3: 2.983  loss_mask_3: 1.965  loss_dice_3: 2.615  loss_ce_4: 3.008  loss_mask_4: 1.949  loss_dice_4: 2.673  loss_ce_5: 2.968  loss_mask_5: 1.998  loss_dice_5: 2.643  loss_ce_6: 2.924  loss_mask_6: 1.837  loss_dice_6: 2.587  loss_ce_7: 2.924  loss_mask_7: 2.034  loss_dice_7: 2.593  loss_ce_8: 2.904  loss_mask_8: 1.732  loss_dice_8: 2.66    time: 0.4497  last_time: 0.4443  data_time: 0.0215  last_data_time: 0.0199   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:55:33 d2.utils.events]: \u001b[0m eta: 1 day, 19:27:50  iter: 4919  total_loss: 75.48  loss_ce: 2.957  loss_mask: 1.748  loss_dice: 2.655  loss_ce_0: 3.186  loss_mask_0: 1.748  loss_dice_0: 2.805  loss_ce_1: 2.944  loss_mask_1: 1.736  loss_dice_1: 2.698  loss_ce_2: 2.875  loss_mask_2: 1.71  loss_dice_2: 2.767  loss_ce_3: 2.916  loss_mask_3: 1.743  loss_dice_3: 2.642  loss_ce_4: 2.921  loss_mask_4: 1.929  loss_dice_4: 2.89  loss_ce_5: 2.973  loss_mask_5: 1.96  loss_dice_5: 2.832  loss_ce_6: 2.929  loss_mask_6: 1.905  loss_dice_6: 2.865  loss_ce_7: 2.959  loss_mask_7: 1.761  loss_dice_7: 2.649  loss_ce_8: 2.868  loss_mask_8: 1.811  loss_dice_8: 2.765    time: 0.4496  last_time: 0.4280  data_time: 0.0225  last_data_time: 0.0198   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:55:41 d2.utils.events]: \u001b[0m eta: 1 day, 19:27:31  iter: 4939  total_loss: 79.22  loss_ce: 2.975  loss_mask: 1.746  loss_dice: 2.924  loss_ce_0: 3.208  loss_mask_0: 1.654  loss_dice_0: 2.972  loss_ce_1: 2.885  loss_mask_1: 1.8  loss_dice_1: 2.817  loss_ce_2: 2.934  loss_mask_2: 1.723  loss_dice_2: 2.786  loss_ce_3: 3.008  loss_mask_3: 1.679  loss_dice_3: 2.786  loss_ce_4: 2.97  loss_mask_4: 1.778  loss_dice_4: 3.09  loss_ce_5: 2.986  loss_mask_5: 1.718  loss_dice_5: 2.986  loss_ce_6: 3.015  loss_mask_6: 1.884  loss_dice_6: 2.916  loss_ce_7: 2.975  loss_mask_7: 1.652  loss_dice_7: 2.96  loss_ce_8: 2.913  loss_mask_8: 1.703  loss_dice_8: 2.985    time: 0.4495  last_time: 0.4292  data_time: 0.0223  last_data_time: 0.0247   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:55:50 d2.utils.events]: \u001b[0m eta: 1 day, 19:27:12  iter: 4959  total_loss: 79.78  loss_ce: 3.158  loss_mask: 1.929  loss_dice: 2.784  loss_ce_0: 3.352  loss_mask_0: 1.705  loss_dice_0: 2.77  loss_ce_1: 3.171  loss_mask_1: 1.85  loss_dice_1: 2.731  loss_ce_2: 3.135  loss_mask_2: 1.939  loss_dice_2: 2.63  loss_ce_3: 3.159  loss_mask_3: 1.746  loss_dice_3: 2.697  loss_ce_4: 3.21  loss_mask_4: 1.738  loss_dice_4: 2.738  loss_ce_5: 3.201  loss_mask_5: 1.869  loss_dice_5: 2.787  loss_ce_6: 3.133  loss_mask_6: 1.847  loss_dice_6: 2.84  loss_ce_7: 3.329  loss_mask_7: 1.801  loss_dice_7: 2.69  loss_ce_8: 3.213  loss_mask_8: 1.818  loss_dice_8: 2.831    time: 0.4495  last_time: 0.4394  data_time: 0.0236  last_data_time: 0.0373   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:55:58 d2.utils.events]: \u001b[0m eta: 1 day, 19:26:57  iter: 4979  total_loss: 78.24  loss_ce: 3.102  loss_mask: 2.087  loss_dice: 2.636  loss_ce_0: 3.207  loss_mask_0: 1.933  loss_dice_0: 2.584  loss_ce_1: 3.171  loss_mask_1: 1.851  loss_dice_1: 2.616  loss_ce_2: 3.146  loss_mask_2: 1.851  loss_dice_2: 2.548  loss_ce_3: 3.165  loss_mask_3: 1.95  loss_dice_3: 2.586  loss_ce_4: 3.188  loss_mask_4: 1.816  loss_dice_4: 2.538  loss_ce_5: 3.135  loss_mask_5: 1.996  loss_dice_5: 2.789  loss_ce_6: 3.209  loss_mask_6: 1.973  loss_dice_6: 2.449  loss_ce_7: 3.159  loss_mask_7: 2.04  loss_dice_7: 2.875  loss_ce_8: 3.155  loss_mask_8: 1.919  loss_dice_8: 2.608    time: 0.4494  last_time: 0.4363  data_time: 0.0219  last_data_time: 0.0241   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:56:08 d2.utils.events]: \u001b[0m eta: 1 day, 19:25:47  iter: 4999  total_loss: 77.09  loss_ce: 3.077  loss_mask: 2.11  loss_dice: 2.754  loss_ce_0: 3.389  loss_mask_0: 1.929  loss_dice_0: 2.556  loss_ce_1: 3.163  loss_mask_1: 1.963  loss_dice_1: 2.492  loss_ce_2: 3.301  loss_mask_2: 2.216  loss_dice_2: 2.516  loss_ce_3: 3.245  loss_mask_3: 1.995  loss_dice_3: 2.457  loss_ce_4: 3.243  loss_mask_4: 1.85  loss_dice_4: 2.354  loss_ce_5: 3.207  loss_mask_5: 2.059  loss_dice_5: 2.684  loss_ce_6: 3.147  loss_mask_6: 2.029  loss_dice_6: 2.676  loss_ce_7: 3.092  loss_mask_7: 1.83  loss_dice_7: 2.49  loss_ce_8: 3.144  loss_mask_8: 1.984  loss_dice_8: 2.675    time: 0.4493  last_time: 0.4242  data_time: 0.0218  last_data_time: 0.0215   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:56:17 d2.utils.events]: \u001b[0m eta: 1 day, 19:25:06  iter: 5019  total_loss: 78.88  loss_ce: 3.057  loss_mask: 1.918  loss_dice: 2.5  loss_ce_0: 3.22  loss_mask_0: 1.889  loss_dice_0: 2.637  loss_ce_1: 2.961  loss_mask_1: 1.86  loss_dice_1: 2.47  loss_ce_2: 3.077  loss_mask_2: 1.905  loss_dice_2: 2.375  loss_ce_3: 3.143  loss_mask_3: 1.894  loss_dice_3: 2.525  loss_ce_4: 2.994  loss_mask_4: 2.087  loss_dice_4: 2.53  loss_ce_5: 3.111  loss_mask_5: 1.946  loss_dice_5: 2.74  loss_ce_6: 3.016  loss_mask_6: 1.952  loss_dice_6: 2.597  loss_ce_7: 3.115  loss_mask_7: 1.846  loss_dice_7: 2.363  loss_ce_8: 3.19  loss_mask_8: 2.013  loss_dice_8: 2.642    time: 0.4492  last_time: 0.4208  data_time: 0.0212  last_data_time: 0.0172   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:56:25 d2.utils.events]: \u001b[0m eta: 1 day, 19:24:28  iter: 5039  total_loss: 75.54  loss_ce: 2.97  loss_mask: 2.094  loss_dice: 2.636  loss_ce_0: 3.215  loss_mask_0: 2.027  loss_dice_0: 2.843  loss_ce_1: 2.818  loss_mask_1: 2.016  loss_dice_1: 2.776  loss_ce_2: 2.907  loss_mask_2: 2.164  loss_dice_2: 2.717  loss_ce_3: 2.941  loss_mask_3: 2.142  loss_dice_3: 2.68  loss_ce_4: 2.85  loss_mask_4: 2.22  loss_dice_4: 2.701  loss_ce_5: 2.957  loss_mask_5: 2.214  loss_dice_5: 2.727  loss_ce_6: 2.996  loss_mask_6: 2.034  loss_dice_6: 2.737  loss_ce_7: 3.056  loss_mask_7: 2.089  loss_dice_7: 2.681  loss_ce_8: 3.076  loss_mask_8: 1.979  loss_dice_8: 2.668    time: 0.4491  last_time: 0.4217  data_time: 0.0211  last_data_time: 0.0191   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:56:34 d2.utils.events]: \u001b[0m eta: 1 day, 19:24:32  iter: 5059  total_loss: 80.9  loss_ce: 3.544  loss_mask: 1.731  loss_dice: 2.751  loss_ce_0: 3.454  loss_mask_0: 1.689  loss_dice_0: 2.852  loss_ce_1: 3.424  loss_mask_1: 1.774  loss_dice_1: 2.677  loss_ce_2: 3.504  loss_mask_2: 1.892  loss_dice_2: 2.689  loss_ce_3: 3.507  loss_mask_3: 1.729  loss_dice_3: 2.733  loss_ce_4: 3.451  loss_mask_4: 1.914  loss_dice_4: 2.833  loss_ce_5: 3.495  loss_mask_5: 1.801  loss_dice_5: 2.894  loss_ce_6: 3.45  loss_mask_6: 1.763  loss_dice_6: 2.727  loss_ce_7: 3.488  loss_mask_7: 1.817  loss_dice_7: 2.782  loss_ce_8: 3.43  loss_mask_8: 1.915  loss_dice_8: 2.717    time: 0.4491  last_time: 0.4286  data_time: 0.0240  last_data_time: 0.0223   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:56:42 d2.utils.events]: \u001b[0m eta: 1 day, 19:24:03  iter: 5079  total_loss: 79.71  loss_ce: 3.205  loss_mask: 2.064  loss_dice: 3.006  loss_ce_0: 3.202  loss_mask_0: 1.913  loss_dice_0: 2.909  loss_ce_1: 3.105  loss_mask_1: 1.733  loss_dice_1: 2.838  loss_ce_2: 3.155  loss_mask_2: 1.777  loss_dice_2: 2.881  loss_ce_3: 3.188  loss_mask_3: 1.76  loss_dice_3: 2.825  loss_ce_4: 3.221  loss_mask_4: 1.812  loss_dice_4: 2.935  loss_ce_5: 3.1  loss_mask_5: 1.893  loss_dice_5: 3.143  loss_ce_6: 3.165  loss_mask_6: 1.927  loss_dice_6: 3.058  loss_ce_7: 3.18  loss_mask_7: 1.687  loss_dice_7: 2.962  loss_ce_8: 3.299  loss_mask_8: 1.785  loss_dice_8: 3.004    time: 0.4490  last_time: 0.4347  data_time: 0.0217  last_data_time: 0.0223   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:56:51 d2.utils.events]: \u001b[0m eta: 1 day, 19:23:10  iter: 5099  total_loss: 76.74  loss_ce: 3.097  loss_mask: 2.006  loss_dice: 2.835  loss_ce_0: 3.291  loss_mask_0: 1.782  loss_dice_0: 2.844  loss_ce_1: 3.006  loss_mask_1: 1.889  loss_dice_1: 2.627  loss_ce_2: 3.087  loss_mask_2: 1.868  loss_dice_2: 2.668  loss_ce_3: 3.134  loss_mask_3: 1.916  loss_dice_3: 2.554  loss_ce_4: 3.189  loss_mask_4: 1.976  loss_dice_4: 2.741  loss_ce_5: 3.262  loss_mask_5: 1.994  loss_dice_5: 2.708  loss_ce_6: 3.207  loss_mask_6: 1.879  loss_dice_6: 2.768  loss_ce_7: 3.19  loss_mask_7: 2.032  loss_dice_7: 2.636  loss_ce_8: 3.165  loss_mask_8: 1.932  loss_dice_8: 2.841    time: 0.4489  last_time: 0.4280  data_time: 0.0215  last_data_time: 0.0233   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:57:00 d2.utils.events]: \u001b[0m eta: 1 day, 19:23:03  iter: 5119  total_loss: 81.4  loss_ce: 3.485  loss_mask: 1.839  loss_dice: 2.97  loss_ce_0: 3.503  loss_mask_0: 1.62  loss_dice_0: 2.874  loss_ce_1: 3.579  loss_mask_1: 1.622  loss_dice_1: 2.72  loss_ce_2: 3.586  loss_mask_2: 1.679  loss_dice_2: 2.522  loss_ce_3: 3.565  loss_mask_3: 1.661  loss_dice_3: 2.81  loss_ce_4: 3.474  loss_mask_4: 1.913  loss_dice_4: 2.785  loss_ce_5: 3.633  loss_mask_5: 1.71  loss_dice_5: 2.993  loss_ce_6: 3.466  loss_mask_6: 1.975  loss_dice_6: 2.914  loss_ce_7: 3.635  loss_mask_7: 1.791  loss_dice_7: 2.854  loss_ce_8: 3.556  loss_mask_8: 1.962  loss_dice_8: 3.022    time: 0.4489  last_time: 0.4371  data_time: 0.0235  last_data_time: 0.0204   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:57:08 d2.utils.events]: \u001b[0m eta: 1 day, 19:22:40  iter: 5139  total_loss: 73.21  loss_ce: 2.589  loss_mask: 2.324  loss_dice: 2.626  loss_ce_0: 2.927  loss_mask_0: 1.869  loss_dice_0: 2.59  loss_ce_1: 2.565  loss_mask_1: 2.022  loss_dice_1: 2.485  loss_ce_2: 2.605  loss_mask_2: 2.015  loss_dice_2: 2.544  loss_ce_3: 2.595  loss_mask_3: 1.817  loss_dice_3: 2.391  loss_ce_4: 2.543  loss_mask_4: 2.053  loss_dice_4: 2.512  loss_ce_5: 2.608  loss_mask_5: 2.005  loss_dice_5: 2.693  loss_ce_6: 2.428  loss_mask_6: 2.195  loss_dice_6: 2.687  loss_ce_7: 2.465  loss_mask_7: 2.334  loss_dice_7: 2.723  loss_ce_8: 2.473  loss_mask_8: 2.119  loss_dice_8: 2.644    time: 0.4488  last_time: 0.4206  data_time: 0.0213  last_data_time: 0.0187   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:57:17 d2.utils.events]: \u001b[0m eta: 1 day, 19:22:02  iter: 5159  total_loss: 81.91  loss_ce: 3.414  loss_mask: 1.755  loss_dice: 2.75  loss_ce_0: 3.439  loss_mask_0: 1.701  loss_dice_0: 2.63  loss_ce_1: 3.271  loss_mask_1: 1.706  loss_dice_1: 2.568  loss_ce_2: 3.301  loss_mask_2: 1.885  loss_dice_2: 2.564  loss_ce_3: 3.417  loss_mask_3: 1.815  loss_dice_3: 2.659  loss_ce_4: 3.367  loss_mask_4: 1.743  loss_dice_4: 2.845  loss_ce_5: 3.379  loss_mask_5: 1.862  loss_dice_5: 2.802  loss_ce_6: 3.487  loss_mask_6: 1.824  loss_dice_6: 2.864  loss_ce_7: 3.557  loss_mask_7: 1.923  loss_dice_7: 2.806  loss_ce_8: 3.408  loss_mask_8: 1.781  loss_dice_8: 2.813    time: 0.4487  last_time: 0.4262  data_time: 0.0216  last_data_time: 0.0212   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:57:26 d2.utils.events]: \u001b[0m eta: 1 day, 19:21:49  iter: 5179  total_loss: 77.49  loss_ce: 3.188  loss_mask: 1.987  loss_dice: 2.617  loss_ce_0: 3.275  loss_mask_0: 1.895  loss_dice_0: 2.59  loss_ce_1: 3.038  loss_mask_1: 1.863  loss_dice_1: 2.541  loss_ce_2: 3.229  loss_mask_2: 1.823  loss_dice_2: 2.365  loss_ce_3: 3.269  loss_mask_3: 2.026  loss_dice_3: 2.514  loss_ce_4: 3.117  loss_mask_4: 2.016  loss_dice_4: 2.801  loss_ce_5: 3.206  loss_mask_5: 1.872  loss_dice_5: 2.621  loss_ce_6: 3.213  loss_mask_6: 1.867  loss_dice_6: 2.559  loss_ce_7: 3.197  loss_mask_7: 1.927  loss_dice_7: 2.691  loss_ce_8: 3.103  loss_mask_8: 1.804  loss_dice_8: 2.711    time: 0.4487  last_time: 0.4325  data_time: 0.0222  last_data_time: 0.0230   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:57:34 d2.utils.events]: \u001b[0m eta: 1 day, 19:21:45  iter: 5199  total_loss: 69.78  loss_ce: 2.615  loss_mask: 1.787  loss_dice: 2.404  loss_ce_0: 2.815  loss_mask_0: 1.997  loss_dice_0: 2.36  loss_ce_1: 2.569  loss_mask_1: 1.77  loss_dice_1: 2.317  loss_ce_2: 2.711  loss_mask_2: 1.919  loss_dice_2: 2.167  loss_ce_3: 2.654  loss_mask_3: 1.823  loss_dice_3: 2.266  loss_ce_4: 2.66  loss_mask_4: 1.995  loss_dice_4: 2.363  loss_ce_5: 2.612  loss_mask_5: 1.704  loss_dice_5: 2.314  loss_ce_6: 2.647  loss_mask_6: 1.786  loss_dice_6: 2.446  loss_ce_7: 2.568  loss_mask_7: 1.738  loss_dice_7: 2.138  loss_ce_8: 2.476  loss_mask_8: 1.884  loss_dice_8: 2.471    time: 0.4486  last_time: 0.4300  data_time: 0.0216  last_data_time: 0.0243   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:57:43 d2.utils.events]: \u001b[0m eta: 1 day, 19:21:36  iter: 5219  total_loss: 83.83  loss_ce: 3.691  loss_mask: 2.052  loss_dice: 2.735  loss_ce_0: 3.796  loss_mask_0: 1.749  loss_dice_0: 2.695  loss_ce_1: 3.726  loss_mask_1: 1.72  loss_dice_1: 2.449  loss_ce_2: 3.682  loss_mask_2: 1.832  loss_dice_2: 2.419  loss_ce_3: 3.76  loss_mask_3: 1.887  loss_dice_3: 2.566  loss_ce_4: 3.672  loss_mask_4: 1.832  loss_dice_4: 2.639  loss_ce_5: 3.707  loss_mask_5: 1.78  loss_dice_5: 2.633  loss_ce_6: 3.737  loss_mask_6: 1.763  loss_dice_6: 2.626  loss_ce_7: 3.678  loss_mask_7: 1.782  loss_dice_7: 2.615  loss_ce_8: 3.679  loss_mask_8: 1.853  loss_dice_8: 2.717    time: 0.4485  last_time: 0.4302  data_time: 0.0238  last_data_time: 0.0245   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:57:52 d2.utils.events]: \u001b[0m eta: 1 day, 19:21:16  iter: 5239  total_loss: 79.86  loss_ce: 3.538  loss_mask: 1.958  loss_dice: 2.764  loss_ce_0: 3.563  loss_mask_0: 1.818  loss_dice_0: 2.683  loss_ce_1: 3.424  loss_mask_1: 1.735  loss_dice_1: 2.564  loss_ce_2: 3.557  loss_mask_2: 1.815  loss_dice_2: 2.426  loss_ce_3: 3.417  loss_mask_3: 1.827  loss_dice_3: 2.545  loss_ce_4: 3.453  loss_mask_4: 1.782  loss_dice_4: 2.537  loss_ce_5: 3.499  loss_mask_5: 1.817  loss_dice_5: 2.735  loss_ce_6: 3.504  loss_mask_6: 1.774  loss_dice_6: 2.845  loss_ce_7: 3.592  loss_mask_7: 1.764  loss_dice_7: 2.564  loss_ce_8: 3.512  loss_mask_8: 1.794  loss_dice_8: 2.647    time: 0.4485  last_time: 0.4255  data_time: 0.0234  last_data_time: 0.0193   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:58:04 d2.utils.events]: \u001b[0m eta: 1 day, 19:21:07  iter: 5259  total_loss: 77.89  loss_ce: 3.362  loss_mask: 2.119  loss_dice: 2.58  loss_ce_0: 3.494  loss_mask_0: 1.88  loss_dice_0: 2.577  loss_ce_1: 3.383  loss_mask_1: 1.862  loss_dice_1: 2.458  loss_ce_2: 3.51  loss_mask_2: 1.874  loss_dice_2: 2.326  loss_ce_3: 3.523  loss_mask_3: 1.905  loss_dice_3: 2.469  loss_ce_4: 3.508  loss_mask_4: 1.945  loss_dice_4: 2.408  loss_ce_5: 3.463  loss_mask_5: 1.881  loss_dice_5: 2.533  loss_ce_6: 3.484  loss_mask_6: 1.965  loss_dice_6: 2.378  loss_ce_7: 3.443  loss_mask_7: 1.95  loss_dice_7: 2.466  loss_ce_8: 3.453  loss_mask_8: 1.865  loss_dice_8: 2.419    time: 0.4492  last_time: 0.4225  data_time: 0.0247  last_data_time: 0.0194   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:58:24 d2.utils.events]: \u001b[0m eta: 1 day, 19:21:24  iter: 5279  total_loss: 82.37  loss_ce: 3.17  loss_mask: 1.782  loss_dice: 2.706  loss_ce_0: 3.285  loss_mask_0: 1.794  loss_dice_0: 2.732  loss_ce_1: 3.307  loss_mask_1: 1.822  loss_dice_1: 2.682  loss_ce_2: 3.251  loss_mask_2: 1.857  loss_dice_2: 2.629  loss_ce_3: 3.201  loss_mask_3: 1.965  loss_dice_3: 2.651  loss_ce_4: 3.276  loss_mask_4: 1.838  loss_dice_4: 2.737  loss_ce_5: 3.106  loss_mask_5: 1.799  loss_dice_5: 2.754  loss_ce_6: 3.201  loss_mask_6: 1.784  loss_dice_6: 2.638  loss_ce_7: 3.21  loss_mask_7: 1.795  loss_dice_7: 2.669  loss_ce_8: 3.09  loss_mask_8: 1.717  loss_dice_8: 2.76    time: 0.4512  last_time: 0.4383  data_time: 0.0667  last_data_time: 0.0321   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:58:33 d2.utils.events]: \u001b[0m eta: 1 day, 19:22:29  iter: 5299  total_loss: 80.43  loss_ce: 3.418  loss_mask: 1.949  loss_dice: 2.877  loss_ce_0: 3.421  loss_mask_0: 1.74  loss_dice_0: 2.853  loss_ce_1: 3.408  loss_mask_1: 1.705  loss_dice_1: 2.69  loss_ce_2: 3.455  loss_mask_2: 1.564  loss_dice_2: 2.639  loss_ce_3: 3.395  loss_mask_3: 1.723  loss_dice_3: 2.736  loss_ce_4: 3.431  loss_mask_4: 1.793  loss_dice_4: 2.776  loss_ce_5: 3.438  loss_mask_5: 1.719  loss_dice_5: 2.699  loss_ce_6: 3.506  loss_mask_6: 1.971  loss_dice_6: 2.778  loss_ce_7: 3.411  loss_mask_7: 1.827  loss_dice_7: 2.763  loss_ce_8: 3.387  loss_mask_8: 1.913  loss_dice_8: 2.843    time: 0.4512  last_time: 0.4365  data_time: 0.0242  last_data_time: 0.0204   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:58:42 d2.utils.events]: \u001b[0m eta: 1 day, 19:22:21  iter: 5319  total_loss: 72.05  loss_ce: 3.314  loss_mask: 1.832  loss_dice: 2.501  loss_ce_0: 3.193  loss_mask_0: 1.87  loss_dice_0: 2.651  loss_ce_1: 2.946  loss_mask_1: 2.009  loss_dice_1: 2.511  loss_ce_2: 3.139  loss_mask_2: 1.96  loss_dice_2: 2.298  loss_ce_3: 3.133  loss_mask_3: 1.895  loss_dice_3: 2.5  loss_ce_4: 3.094  loss_mask_4: 2.097  loss_dice_4: 2.449  loss_ce_5: 3.147  loss_mask_5: 1.944  loss_dice_5: 2.482  loss_ce_6: 3.15  loss_mask_6: 1.917  loss_dice_6: 2.485  loss_ce_7: 3.207  loss_mask_7: 1.806  loss_dice_7: 2.416  loss_ce_8: 3.062  loss_mask_8: 1.943  loss_dice_8: 2.511    time: 0.4512  last_time: 0.4257  data_time: 0.0228  last_data_time: 0.0227   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:58:51 d2.utils.events]: \u001b[0m eta: 1 day, 19:22:21  iter: 5339  total_loss: 76.72  loss_ce: 2.969  loss_mask: 1.667  loss_dice: 2.615  loss_ce_0: 3.038  loss_mask_0: 1.832  loss_dice_0: 2.675  loss_ce_1: 2.93  loss_mask_1: 1.996  loss_dice_1: 2.664  loss_ce_2: 2.914  loss_mask_2: 1.779  loss_dice_2: 2.459  loss_ce_3: 2.938  loss_mask_3: 1.83  loss_dice_3: 2.676  loss_ce_4: 2.919  loss_mask_4: 1.795  loss_dice_4: 2.609  loss_ce_5: 2.985  loss_mask_5: 1.665  loss_dice_5: 2.597  loss_ce_6: 2.905  loss_mask_6: 2.001  loss_dice_6: 2.653  loss_ce_7: 2.971  loss_mask_7: 1.87  loss_dice_7: 2.647  loss_ce_8: 2.884  loss_mask_8: 1.697  loss_dice_8: 2.536    time: 0.4511  last_time: 0.4312  data_time: 0.0217  last_data_time: 0.0216   lr: 1e-05  max_mem: 6728M\n",
      "\u001b[32m[07/17 13:58:59 d2.utils.events]: \u001b[0m eta: 1 day, 19:21:56  iter: 5359  total_loss: 74.46  loss_ce: 2.983  loss_mask: 1.803  loss_dice: 2.629  loss_ce_0: 3.001  loss_mask_0: 1.709  loss_dice_0: 2.624  loss_ce_1: 2.759  loss_mask_1: 1.788  loss_dice_1: 2.487  loss_ce_2: 2.938  loss_mask_2: 1.742  loss_dice_2: 2.524  loss_ce_3: 2.924  loss_mask_3: 1.89  loss_dice_3: 2.648  loss_ce_4: 2.905  loss_mask_4: 1.827  loss_dice_4: 2.601  loss_ce_5: 2.874  loss_mask_5: 1.826  loss_dice_5: 2.782  loss_ce_6: 2.86  loss_mask_6: 1.799  loss_dice_6: 2.564  loss_ce_7: 2.966  loss_mask_7: 1.865  loss_dice_7: 2.661  loss_ce_8: 2.966  loss_mask_8: 1.94  loss_dice_8: 2.661    time: 0.4510  last_time: 0.4369  data_time: 0.0210  last_data_time: 0.0223   lr: 1e-05  max_mem: 6728M\n"
     ]
    }
   ],
   "source": [
    "cfg = get_cfg()\n",
    "add_deeplab_config(cfg)\n",
    "mask2former.add_maskformer2_config(cfg)\n",
    "cfg.merge_from_file(CONFIG)\n",
    "\n",
    "launch(get_trainer, 1, args=(cfg,))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
