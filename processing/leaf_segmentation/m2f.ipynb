{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stefan.steinheber/.conda/envs/pytorch/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import lib.Mask2Former as m2f\n",
    "import lib.Mask2Former.mask2former as mask2former\n",
    "import os\n",
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from detectron2.engine import (launch)\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.projects.deeplab import add_deeplab_config\n",
    "from detectron2.data import build_detection_train_loader\n",
    "from lib.Mask2Former.train_net import Trainer\n",
    "import numpy as np\n",
    "from detectron2.structures import Boxes, Instances, BitMasks\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from detectron2.evaluation import DatasetEvaluator, DatasetEvaluators\n",
    "from detectron2.data import build_detection_test_loader\n",
    "from detectron2.utils import comm\n",
    "from detectron2.structures import BoxMode, pairwise_iou\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATA_SOURCE = \"combined\"\n",
    "DATA_LOCATION = \"_data\"\n",
    "DATA_DIR = \"coco\"\n",
    "os.environ[\"DETECTRON2_DATASETS\"] = os.path.join(DATA_LOCATION, DATA_DIR)\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert the dataset to COCO format\n",
    "The following commands convert the existing PNG mask-based dataset to the coco annotations required for training Mask2Former"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "tags": []
   },
   "source": [
    "!cd {DATA_LOCATION} && python mask_to_coco.py --images {DATA_SOURCE}/val/images/ --masks {DATA_SOURCE}/val/leaf_instances/ --output {DATA_DIR}/annotations/instances_val2017.json --fixed-category-id 58 --fixed-category-name \"potted plant\" --default-categories\n",
    "!cd {DATA_LOCATION} && python mask_to_coco.py --images {DATA_SOURCE}/train/images/ --masks {DATA_SOURCE}/train/leaf_instances/ --output {DATA_DIR}/annotations/instances_train2017.json --fixed-category-id 58 --fixed-category-name \"potted plant\" --default-categories"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "tags": []
   },
   "source": [
    "!cd {DATA_LOCATION} && mkdir {DATA_DIR}/train2017\n",
    "!cd {DATA_LOCATION} && cp {DATA_SOURCE}/train/images/* {DATA_DIR}/train2017\n",
    "!cd {DATA_LOCATION} && mkdir {DATA_DIR}/val2017\n",
    "!cd {DATA_LOCATION} && cp {DATA_SOURCE}/val/images/* {DATA_DIR}/val2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "CONFIG = \"lib/Mask2Former/configs/coco/instance-segmentation/swin/maskformer2_swin_base_IN21k_384_bs16_50ep.yaml\"\n",
    "#CONFIG = \"configs/mask2former.yaml\"\n",
    "NUM_GPUS = 1\n",
    "BATCH_SIZE = 8\n",
    "LEARNING_RATE = 0.001\n",
    "DATASET_DIR = \"_data/urban_street_combined\"\n",
    "DATASET_DIR_VAL = \"_data/combined/val\"\n",
    "IMAGES_DIR_NAME = \"images\"\n",
    "IMAGE_DIR = os.path.join(DATASET_DIR, IMAGES_DIR_NAME)\n",
    "INSTANCES_DIR_NAME = \"leaf_instances\"\n",
    "INSTANCES_DIR = os.path.join(DATASET_DIR, INSTANCES_DIR_NAME)\n",
    "IMAGE_DIR_VAL = os.path.join(DATASET_DIR_VAL, IMAGES_DIR_NAME)\n",
    "INSTANCES_DIR_VAL = os.path.join(DATASET_DIR_VAL, INSTANCES_DIR_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LeavesDataset(Dataset):\n",
    "    def __init__(self, image_dir, label_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.transform = transform\n",
    "        self.image_files = os.listdir(image_dir)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_path = os.path.join(self.image_dir, self.image_files[index])\n",
    "        label_path = os.path.join(self.label_dir, self.image_files[index])\n",
    "\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        label = Image.open(label_path).convert(\"L\")\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            #label = self.transform(label).squeeze()\n",
    "\n",
    "        # Convert label to tensor\n",
    "        label = torch.from_numpy(np.array(label))\n",
    "\n",
    "        # Create instances\n",
    "        instances = Instances(image.shape[1:])\n",
    "\n",
    "        # Create gt_boxes\n",
    "        boxes = []\n",
    "        gt_classes = []\n",
    "        gt_masks = []\n",
    "        unique_labels = torch.unique(label)\n",
    "        if len(unique_labels) > 1:\n",
    "            if 255 in unique_labels: \n",
    "                print(\"Invalid label in file\", image_path)\n",
    "            for obj_class in unique_labels:\n",
    "                if obj_class > 0:\n",
    "                    mask = label == obj_class\n",
    "                    coords = torch.nonzero(mask)\n",
    "                    xmin, ymin = coords.min(dim=0).values\n",
    "                    xmax, ymax = coords.max(dim=0).values\n",
    "                    boxes.append([xmin, ymin, xmax, ymax])\n",
    "                    gt_classes.append(obj_class.item())\n",
    "                    gt_masks.append(mask)\n",
    "\n",
    "            instances.gt_boxes = Boxes(torch.tensor(boxes))\n",
    "            instances.gt_classes = torch.tensor(gt_classes, dtype=torch.long)\n",
    "\n",
    "            # Resize masks to match the image size\n",
    "            resized_masks = []\n",
    "            for mask in gt_masks:\n",
    "                resized_mask = F.interpolate(mask.unsqueeze(0).unsqueeze(0).float(), size=image.shape[1:], mode='nearest').squeeze().to(torch.bool)\n",
    "                resized_masks.append(resized_mask)\n",
    "\n",
    "            if len(resized_masks) > 0:\n",
    "                instances.gt_masks = torch.stack(resized_masks)\n",
    "            else:\n",
    "                print(\"Masks empty, class lenght is\", len(gt_classes))\n",
    "                instances.gt_masks = torch.Tensor()\n",
    "\n",
    "            return {\n",
    "                \"image\": image,\n",
    "                \"height\": image.shape[1],\n",
    "                \"width\": image.shape[2],\n",
    "                \"instances\": instances,\n",
    "            }\n",
    "        \n",
    "        return {\n",
    "            \"image\": image,\n",
    "            \"height\": image.shape[1],\n",
    "            \"width\": image.shape[2]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeavesEvaluator(DatasetEvaluator):\n",
    "    def __init__(self, dataset_name):\n",
    "        self.dataset_name = dataset_name\n",
    "        self._cpu_device = torch.device(\"cpu\")\n",
    "\n",
    "    def reset(self):\n",
    "        self._predictions = []\n",
    "        self._targets = []\n",
    "\n",
    "    def process(self, inputs, outputs):\n",
    "        for input, output in zip(inputs, outputs):\n",
    "            prediction = {\"image_id\": input[\"image_id\"]}\n",
    "\n",
    "            if \"instances\" in output:\n",
    "                instances = output[\"instances\"].to(self._cpu_device)\n",
    "                prediction[\"instances\"] = instances\n",
    "\n",
    "            self._predictions.append(prediction)\n",
    "            self._targets.append(input[\"instances\"].to(self._cpu_device))\n",
    "\n",
    "    def evaluate(self):\n",
    "        if comm.is_main_process():\n",
    "            self._evaluate()\n",
    "\n",
    "        if comm.is_main_process():\n",
    "            return copy.deepcopy(self._results)\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def _evaluate(self):\n",
    "        self._results = {}\n",
    "        iou_thresholds = [0.5, 0.75]\n",
    "        for iou_threshold in iou_thresholds:\n",
    "            self._results[f\"IoU_{iou_threshold}\"] = self._compute_iou(iou_threshold)\n",
    "\n",
    "    def _compute_iou(self, iou_threshold):\n",
    "        num_instances = len(self._predictions)\n",
    "        iou_sum = 0.0\n",
    "\n",
    "        for pred, target in zip(self._predictions, self._targets):\n",
    "            pred_boxes = pred[\"instances\"].pred_boxes.tensor\n",
    "            target_boxes = target.gt_boxes.tensor\n",
    "\n",
    "            if len(pred_boxes) == 0 or len(target_boxes) == 0:\n",
    "                continue\n",
    "\n",
    "            # Convert the boxes to the format expected by the pairwise_iou function\n",
    "            pred_boxes = BoxMode.convert(pred_boxes, BoxMode.XYXY_ABS, BoxMode.XYXY_ABS)\n",
    "            target_boxes = BoxMode.convert(target_boxes, BoxMode.XYXY_ABS, BoxMode.XYXY_ABS)\n",
    "\n",
    "            # Compute IoU between predicted and target boxes\n",
    "            iou_matrix = pairwise_iou(Boxes(pred_boxes), Boxes(target_boxes))\n",
    "            max_iou, _ = iou_matrix.max(dim=1)\n",
    "\n",
    "            # Count the number of predicted boxes with IoU above the threshold\n",
    "            num_above_threshold = (max_iou > iou_threshold).sum().item()\n",
    "            iou_sum += num_above_threshold\n",
    "\n",
    "        avg_iou = iou_sum / num_instances\n",
    "        return avg_iou\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    images = []\n",
    "    instances = []\n",
    "    extras = {}\n",
    "\n",
    "    for item in batch:\n",
    "        images.append(item[\"image\"])\n",
    "        \n",
    "        item_instances = item[\"instances\"]\n",
    "        item_instances[\"gt_boxes\"] = torch.tensor(item_instances[\"gt_boxes\"])\n",
    "        item_instances[\"gt_classes\"] = torch.tensor(item_instances[\"gt_classes\"], dtype=torch.long)\n",
    "        item_instances[\"gt_masks\"] = torch.tensor(item_instances[\"gt_masks\"])\n",
    "        instances.append(item_instances)\n",
    "        \n",
    "        extras[\"height\"] = item[\"height\"]\n",
    "        extras[\"width\"] = item[\"width\"]\n",
    "\n",
    "    batched_inputs = [\n",
    "        {\"image\": image, \"instances\": instance, **extras}\n",
    "        for image, instance in zip(images, instances)\n",
    "    ]\n",
    "\n",
    "    return batched_inputs\n",
    "\n",
    "class LeavesTrainer(Trainer):\n",
    "    @classmethod\n",
    "    def build_train_loader(cls, _):\n",
    "        # Define your data transforms\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((800, 800)),\n",
    "            transforms.ToTensor(),\n",
    "            #transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "\n",
    "        # Create the dataset\n",
    "        dataset = LeavesDataset(IMAGE_DIR, INSTANCES_DIR, transform=transform, )\n",
    "        \n",
    "        # Create the DataLoader\n",
    "        dataloader = build_detection_train_loader(dataset, mapper=None, total_batch_size=1)\n",
    "        return dataloader\n",
    "    \n",
    "    @classmethod\n",
    "    def build_test_loader(cls, cfg, dataset_name):\n",
    "        # Define your data transforms\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((800, 800)),\n",
    "            transforms.ToTensor(),\n",
    "            #transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "\n",
    "        # Create the dataset\n",
    "        dataset = LeavesDataset(IMAGE_DIR, INSTANCES_DIR, transform=transform, )\n",
    "        \n",
    "        # Create the DataLoader\n",
    "        dataloader = build_detection_train_loader(dataset, mapper=None, total_batch_size=1)\n",
    "        return dataloader\n",
    "        \n",
    "    @classmethod\n",
    "    def build_evaluator(cls, cfg, dataset_name, output_folder=None):\n",
    "        return DatasetEvaluators([LeavesEvaluator(dataset_name)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_trainer(cfg):\n",
    "    trainer = LeavesTrainer(cfg)\n",
    "    #trainer.resume_or_load(resume=args.resume)\n",
    "    return trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stefan.steinheber/.conda/envs/pytorch/lib/python3.12/site-packages/torch/functional.py:512: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /opt/conda/conda-bld/pytorch_1716905971214/work/aten/src/ATen/native/TensorShape.cpp:3587.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[07/17 08:37:49 d2.engine.defaults]: \u001b[0mModel:\n",
      "MaskFormer(\n",
      "  (backbone): D2SwinTransformer(\n",
      "    (patch_embed): PatchEmbed(\n",
      "      (proj): Conv2d(3, 128, kernel_size=(4, 4), stride=(4, 4))\n",
      "      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (pos_drop): Dropout(p=0.0, inplace=False)\n",
      "    (layers): ModuleList(\n",
      "      (0): BasicLayer(\n",
      "        (blocks): ModuleList(\n",
      "          (0): SwinTransformerBlock(\n",
      "            (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): Identity()\n",
      "            (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (1): SwinTransformerBlock(\n",
      "            (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.013)\n",
      "            (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (downsample): PatchMerging(\n",
      "          (reduction): Linear(in_features=512, out_features=256, bias=False)\n",
      "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicLayer(\n",
      "        (blocks): ModuleList(\n",
      "          (0): SwinTransformerBlock(\n",
      "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.026)\n",
      "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (1): SwinTransformerBlock(\n",
      "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.039)\n",
      "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (downsample): PatchMerging(\n",
      "          (reduction): Linear(in_features=1024, out_features=512, bias=False)\n",
      "          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (2): BasicLayer(\n",
      "        (blocks): ModuleList(\n",
      "          (0): SwinTransformerBlock(\n",
      "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.052)\n",
      "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (1): SwinTransformerBlock(\n",
      "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.065)\n",
      "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (2): SwinTransformerBlock(\n",
      "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.078)\n",
      "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (3): SwinTransformerBlock(\n",
      "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.091)\n",
      "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (4): SwinTransformerBlock(\n",
      "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.104)\n",
      "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (5): SwinTransformerBlock(\n",
      "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.117)\n",
      "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (6): SwinTransformerBlock(\n",
      "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.130)\n",
      "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (7): SwinTransformerBlock(\n",
      "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.143)\n",
      "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (8): SwinTransformerBlock(\n",
      "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.157)\n",
      "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (9): SwinTransformerBlock(\n",
      "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.170)\n",
      "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (10): SwinTransformerBlock(\n",
      "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.183)\n",
      "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (11): SwinTransformerBlock(\n",
      "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.196)\n",
      "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (12): SwinTransformerBlock(\n",
      "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.209)\n",
      "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (13): SwinTransformerBlock(\n",
      "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.222)\n",
      "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (14): SwinTransformerBlock(\n",
      "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.235)\n",
      "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (15): SwinTransformerBlock(\n",
      "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.248)\n",
      "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (16): SwinTransformerBlock(\n",
      "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.261)\n",
      "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (17): SwinTransformerBlock(\n",
      "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.274)\n",
      "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (downsample): PatchMerging(\n",
      "          (reduction): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "          (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (3): BasicLayer(\n",
      "        (blocks): ModuleList(\n",
      "          (0): SwinTransformerBlock(\n",
      "            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.287)\n",
      "            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (1): SwinTransformerBlock(\n",
      "            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.300)\n",
      "            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (norm0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    (norm3): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (sem_seg_head): MaskFormerHead(\n",
      "    (pixel_decoder): MSDeformAttnPixelDecoder(\n",
      "      (input_proj): ModuleList(\n",
      "        (0): Sequential(\n",
      "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "        )\n",
      "        (1): Sequential(\n",
      "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "        )\n",
      "        (2): Sequential(\n",
      "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "        )\n",
      "      )\n",
      "      (transformer): MSDeformAttnTransformerEncoderOnly(\n",
      "        (encoder): MSDeformAttnTransformerEncoder(\n",
      "          (layers): ModuleList(\n",
      "            (0-5): 6 x MSDeformAttnTransformerEncoderLayer(\n",
      "              (self_attn): MSDeformAttn(\n",
      "                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)\n",
      "                (attention_weights): Linear(in_features=256, out_features=96, bias=True)\n",
      "                (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "                (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "              )\n",
      "              (dropout1): Dropout(p=0.0, inplace=False)\n",
      "              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "              (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "              (dropout2): Dropout(p=0.0, inplace=False)\n",
      "              (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "              (dropout3): Dropout(p=0.0, inplace=False)\n",
      "              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (pe_layer): Positional encoding PositionEmbeddingSine\n",
      "          num_pos_feats: 128\n",
      "          temperature: 10000\n",
      "          normalize: True\n",
      "          scale: 6.283185307179586\n",
      "      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (adapter_1): Conv2d(\n",
      "        128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "      )\n",
      "      (layer_1): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "      )\n",
      "    )\n",
      "    (predictor): MultiScaleMaskedTransformerDecoder(\n",
      "      (pe_layer): Positional encoding PositionEmbeddingSine\n",
      "          num_pos_feats: 128\n",
      "          temperature: 10000\n",
      "          normalize: True\n",
      "          scale: 6.283185307179586\n",
      "      (transformer_self_attention_layers): ModuleList(\n",
      "        (0-8): 9 x SelfAttentionLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (transformer_cross_attention_layers): ModuleList(\n",
      "        (0-8): 9 x CrossAttentionLayer(\n",
      "          (multihead_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (transformer_ffn_layers): ModuleList(\n",
      "        (0-8): 9 x FFNLayer(\n",
      "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (query_feat): Embedding(100, 256)\n",
      "      (query_embed): Embedding(100, 256)\n",
      "      (level_embed): Embedding(3, 256)\n",
      "      (input_proj): ModuleList(\n",
      "        (0-2): 3 x Sequential()\n",
      "      )\n",
      "      (class_embed): Linear(in_features=256, out_features=81, bias=True)\n",
      "      (mask_embed): MLP(\n",
      "        (layers): ModuleList(\n",
      "          (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (criterion): Criterion SetCriterion\n",
      "      matcher: Matcher HungarianMatcher\n",
      "          cost_class: 2.0\n",
      "          cost_mask: 5.0\n",
      "          cost_dice: 5.0\n",
      "      losses: ['labels', 'masks']\n",
      "      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}\n",
      "      num_classes: 80\n",
      "      eos_coef: 0.1\n",
      "      num_points: 12544\n",
      "      oversample_ratio: 3.0\n",
      "      importance_sample_ratio: 0.75\n",
      ")\n",
      "relative_position_bias_table\n",
      "relative_position_bias_table\n",
      "relative_position_bias_table\n",
      "relative_position_bias_table\n",
      "relative_position_bias_table\n",
      "relative_position_bias_table\n",
      "relative_position_bias_table\n",
      "relative_position_bias_table\n",
      "relative_position_bias_table\n",
      "relative_position_bias_table\n",
      "relative_position_bias_table\n",
      "relative_position_bias_table\n",
      "relative_position_bias_table\n",
      "relative_position_bias_table\n",
      "relative_position_bias_table\n",
      "relative_position_bias_table\n",
      "relative_position_bias_table\n",
      "relative_position_bias_table\n",
      "relative_position_bias_table\n",
      "relative_position_bias_table\n",
      "relative_position_bias_table\n",
      "relative_position_bias_table\n",
      "relative_position_bias_table\n",
      "relative_position_bias_table\n",
      "\u001b[32m[07/17 08:37:49 d2.data.build]: \u001b[0mMaking batched data loader with batch_size=1\n",
      "\u001b[32m[07/17 08:37:49 d2.engine.train_loop]: \u001b[0mStarting training from iteration 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stefan.steinheber/.conda/envs/pytorch/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[07/17 08:37:58 d2.utils.events]: \u001b[0m eta: 1 day, 20:35:25  iter: 19  total_loss: 143.7  loss_ce: 5.316  loss_mask: 3.449  loss_dice: 4.574  loss_ce_0: 8.993  loss_mask_0: 1.977  loss_dice_0: 4.417  loss_ce_1: 7.127  loss_mask_1: 2.708  loss_dice_1: 4.47  loss_ce_2: 5.454  loss_mask_2: 2.263  loss_dice_2: 4.633  loss_ce_3: 5.553  loss_mask_3: 2.855  loss_dice_3: 4.592  loss_ce_4: 5.654  loss_mask_4: 3.343  loss_dice_4: 4.61  loss_ce_5: 5.011  loss_mask_5: 3.494  loss_dice_5: 4.646  loss_ce_6: 5.334  loss_mask_6: 4.353  loss_dice_6: 4.599  loss_ce_7: 5.443  loss_mask_7: 4.509  loss_dice_7: 4.63  loss_ce_8: 5.073  loss_mask_8: 3.689  loss_dice_8: 4.609    time: 0.4398  last_time: 0.4314  data_time: 0.0248  last_data_time: 0.0211   lr: 1e-05  max_mem: 6714M\n",
      "\u001b[32m[07/17 08:38:07 d2.utils.events]: \u001b[0m eta: 1 day, 20:52:52  iter: 39  total_loss: 105  loss_ce: 3.538  loss_mask: 2.061  loss_dice: 4.556  loss_ce_0: 8.802  loss_mask_0: 1.755  loss_dice_0: 4.356  loss_ce_1: 3.592  loss_mask_1: 1.652  loss_dice_1: 4.359  loss_ce_2: 3.514  loss_mask_2: 1.588  loss_dice_2: 4.474  loss_ce_3: 3.511  loss_mask_3: 1.839  loss_dice_3: 4.527  loss_ce_4: 3.557  loss_mask_4: 1.928  loss_dice_4: 4.584  loss_ce_5: 3.505  loss_mask_5: 1.937  loss_dice_5: 4.547  loss_ce_6: 3.574  loss_mask_6: 1.988  loss_dice_6: 4.596  loss_ce_7: 3.503  loss_mask_7: 1.938  loss_dice_7: 4.552  loss_ce_8: 3.647  loss_mask_8: 2.025  loss_dice_8: 4.574    time: 0.4415  last_time: 0.4265  data_time: 0.0242  last_data_time: 0.0189   lr: 1e-05  max_mem: 6714M\n",
      "\u001b[32m[07/17 08:38:16 d2.utils.events]: \u001b[0m eta: 1 day, 20:52:01  iter: 59  total_loss: 104.8  loss_ce: 3.484  loss_mask: 1.43  loss_dice: 4.671  loss_ce_0: 8.956  loss_mask_0: 1.254  loss_dice_0: 4.57  loss_ce_1: 3.449  loss_mask_1: 1.363  loss_dice_1: 4.572  loss_ce_2: 3.361  loss_mask_2: 1.329  loss_dice_2: 4.597  loss_ce_3: 3.37  loss_mask_3: 1.252  loss_dice_3: 4.627  loss_ce_4: 3.483  loss_mask_4: 1.337  loss_dice_4: 4.625  loss_ce_5: 3.512  loss_mask_5: 1.335  loss_dice_5: 4.674  loss_ce_6: 3.512  loss_mask_6: 1.33  loss_dice_6: 4.696  loss_ce_7: 3.496  loss_mask_7: 1.458  loss_dice_7: 4.678  loss_ce_8: 3.52  loss_mask_8: 1.499  loss_dice_8: 4.66    time: 0.4410  last_time: 0.4375  data_time: 0.0231  last_data_time: 0.0211   lr: 1e-05  max_mem: 6714M\n",
      "\u001b[32m[07/17 08:38:25 d2.utils.events]: \u001b[0m eta: 1 day, 20:53:41  iter: 79  total_loss: 103.3  loss_ce: 3.7  loss_mask: 1.889  loss_dice: 4.606  loss_ce_0: 8.925  loss_mask_0: 1.491  loss_dice_0: 4.474  loss_ce_1: 3.422  loss_mask_1: 1.535  loss_dice_1: 4.483  loss_ce_2: 3.333  loss_mask_2: 1.504  loss_dice_2: 4.459  loss_ce_3: 3.387  loss_mask_3: 1.56  loss_dice_3: 4.491  loss_ce_4: 3.626  loss_mask_4: 1.663  loss_dice_4: 4.503  loss_ce_5: 3.711  loss_mask_5: 1.761  loss_dice_5: 4.583  loss_ce_6: 3.692  loss_mask_6: 1.883  loss_dice_6: 4.581  loss_ce_7: 3.724  loss_mask_7: 1.818  loss_dice_7: 4.602  loss_ce_8: 3.675  loss_mask_8: 1.9  loss_dice_8: 4.593    time: 0.4404  last_time: 0.4475  data_time: 0.0250  last_data_time: 0.0377   lr: 1e-05  max_mem: 6714M\n",
      "\u001b[32m[07/17 08:38:34 d2.utils.events]: \u001b[0m eta: 1 day, 20:51:43  iter: 99  total_loss: 99.98  loss_ce: 3.396  loss_mask: 1.924  loss_dice: 4.47  loss_ce_0: 8.882  loss_mask_0: 1.651  loss_dice_0: 4.286  loss_ce_1: 3.162  loss_mask_1: 1.659  loss_dice_1: 4.39  loss_ce_2: 3.008  loss_mask_2: 1.666  loss_dice_2: 4.361  loss_ce_3: 3.072  loss_mask_3: 1.672  loss_dice_3: 4.369  loss_ce_4: 3.133  loss_mask_4: 1.671  loss_dice_4: 4.283  loss_ce_5: 3.205  loss_mask_5: 1.725  loss_dice_5: 4.413  loss_ce_6: 3.365  loss_mask_6: 1.873  loss_dice_6: 4.465  loss_ce_7: 3.392  loss_mask_7: 1.983  loss_dice_7: 4.508  loss_ce_8: 3.451  loss_mask_8: 2.029  loss_dice_8: 4.54    time: 0.4402  last_time: 0.4511  data_time: 0.0238  last_data_time: 0.0261   lr: 1e-05  max_mem: 6714M\n",
      "\u001b[32m[07/17 08:38:42 d2.utils.events]: \u001b[0m eta: 1 day, 20:47:45  iter: 119  total_loss: 98.37  loss_ce: 3.526  loss_mask: 1.441  loss_dice: 4.567  loss_ce_0: 8.797  loss_mask_0: 1.576  loss_dice_0: 4.416  loss_ce_1: 3.284  loss_mask_1: 1.344  loss_dice_1: 4.514  loss_ce_2: 3.122  loss_mask_2: 1.478  loss_dice_2: 4.453  loss_ce_3: 3.156  loss_mask_3: 1.437  loss_dice_3: 4.551  loss_ce_4: 3.311  loss_mask_4: 1.29  loss_dice_4: 4.508  loss_ce_5: 3.37  loss_mask_5: 1.409  loss_dice_5: 4.576  loss_ce_6: 3.466  loss_mask_6: 1.511  loss_dice_6: 4.524  loss_ce_7: 3.465  loss_mask_7: 1.602  loss_dice_7: 4.541  loss_ce_8: 3.515  loss_mask_8: 1.566  loss_dice_8: 4.586    time: 0.4398  last_time: 0.4291  data_time: 0.0242  last_data_time: 0.0214   lr: 1e-05  max_mem: 6714M\n",
      "\u001b[32m[07/17 08:38:51 d2.utils.events]: \u001b[0m eta: 1 day, 20:44:20  iter: 139  total_loss: 94.78  loss_ce: 2.964  loss_mask: 1.514  loss_dice: 4.382  loss_ce_0: 8.774  loss_mask_0: 1.992  loss_dice_0: 4.204  loss_ce_1: 2.558  loss_mask_1: 1.943  loss_dice_1: 4.179  loss_ce_2: 2.673  loss_mask_2: 2.068  loss_dice_2: 4.228  loss_ce_3: 2.693  loss_mask_3: 1.794  loss_dice_3: 4.348  loss_ce_4: 2.85  loss_mask_4: 1.614  loss_dice_4: 4.394  loss_ce_5: 2.854  loss_mask_5: 1.602  loss_dice_5: 4.458  loss_ce_6: 2.567  loss_mask_6: 1.679  loss_dice_6: 4.391  loss_ce_7: 2.843  loss_mask_7: 1.594  loss_dice_7: 4.423  loss_ce_8: 2.801  loss_mask_8: 1.641  loss_dice_8: 4.397    time: 0.4392  last_time: 0.4353  data_time: 0.0220  last_data_time: 0.0215   lr: 1e-05  max_mem: 6714M\n",
      "\u001b[32m[07/17 08:39:00 d2.utils.events]: \u001b[0m eta: 1 day, 20:44:11  iter: 159  total_loss: 96.2  loss_ce: 3.277  loss_mask: 1.48  loss_dice: 4.475  loss_ce_0: 8.765  loss_mask_0: 1.684  loss_dice_0: 4.254  loss_ce_1: 2.761  loss_mask_1: 1.659  loss_dice_1: 4.332  loss_ce_2: 2.788  loss_mask_2: 1.761  loss_dice_2: 4.376  loss_ce_3: 3.077  loss_mask_3: 1.623  loss_dice_3: 4.42  loss_ce_4: 3.379  loss_mask_4: 1.499  loss_dice_4: 4.353  loss_ce_5: 3.344  loss_mask_5: 1.609  loss_dice_5: 4.388  loss_ce_6: 3.494  loss_mask_6: 1.56  loss_dice_6: 4.475  loss_ce_7: 3.02  loss_mask_7: 1.458  loss_dice_7: 4.4  loss_ce_8: 2.919  loss_mask_8: 1.592  loss_dice_8: 4.422    time: 0.4387  last_time: 0.4389  data_time: 0.0225  last_data_time: 0.0290   lr: 1e-05  max_mem: 6714M\n",
      "\u001b[32m[07/17 08:39:09 d2.utils.events]: \u001b[0m eta: 1 day, 20:45:03  iter: 179  total_loss: 98.73  loss_ce: 3.283  loss_mask: 1.728  loss_dice: 4.431  loss_ce_0: 8.729  loss_mask_0: 1.733  loss_dice_0: 4.042  loss_ce_1: 2.865  loss_mask_1: 1.756  loss_dice_1: 4.048  loss_ce_2: 2.988  loss_mask_2: 1.728  loss_dice_2: 4.103  loss_ce_3: 3.323  loss_mask_3: 1.765  loss_dice_3: 4.198  loss_ce_4: 3.624  loss_mask_4: 1.635  loss_dice_4: 4.359  loss_ce_5: 3.716  loss_mask_5: 1.626  loss_dice_5: 4.412  loss_ce_6: 3.444  loss_mask_6: 1.654  loss_dice_6: 4.554  loss_ce_7: 3.386  loss_mask_7: 1.695  loss_dice_7: 4.375  loss_ce_8: 3.331  loss_mask_8: 1.752  loss_dice_8: 4.417    time: 0.4386  last_time: 0.4344  data_time: 0.0230  last_data_time: 0.0221   lr: 1e-05  max_mem: 6714M\n",
      "\u001b[32m[07/17 08:39:17 d2.utils.events]: \u001b[0m eta: 1 day, 20:43:54  iter: 199  total_loss: 95.28  loss_ce: 2.761  loss_mask: 2.015  loss_dice: 4.188  loss_ce_0: 8.723  loss_mask_0: 1.917  loss_dice_0: 3.898  loss_ce_1: 2.639  loss_mask_1: 1.898  loss_dice_1: 3.994  loss_ce_2: 2.856  loss_mask_2: 1.885  loss_dice_2: 4.07  loss_ce_3: 2.867  loss_mask_3: 1.909  loss_dice_3: 4.101  loss_ce_4: 2.932  loss_mask_4: 1.908  loss_dice_4: 4.094  loss_ce_5: 2.797  loss_mask_5: 1.951  loss_dice_5: 4.188  loss_ce_6: 2.801  loss_mask_6: 1.965  loss_dice_6: 4.353  loss_ce_7: 2.793  loss_mask_7: 1.894  loss_dice_7: 4.366  loss_ce_8: 2.812  loss_mask_8: 1.95  loss_dice_8: 4.117    time: 0.4383  last_time: 0.4410  data_time: 0.0221  last_data_time: 0.0281   lr: 1e-05  max_mem: 6714M\n",
      "\u001b[32m[07/17 08:39:26 d2.utils.events]: \u001b[0m eta: 1 day, 20:42:32  iter: 219  total_loss: 97.31  loss_ce: 2.791  loss_mask: 1.781  loss_dice: 4.106  loss_ce_0: 8.676  loss_mask_0: 1.756  loss_dice_0: 3.956  loss_ce_1: 2.709  loss_mask_1: 1.707  loss_dice_1: 4.128  loss_ce_2: 3.017  loss_mask_2: 1.984  loss_dice_2: 4.065  loss_ce_3: 2.977  loss_mask_3: 1.745  loss_dice_3: 4.026  loss_ce_4: 3.048  loss_mask_4: 1.788  loss_dice_4: 3.977  loss_ce_5: 3.198  loss_mask_5: 1.798  loss_dice_5: 4.174  loss_ce_6: 2.986  loss_mask_6: 1.752  loss_dice_6: 4.14  loss_ce_7: 2.914  loss_mask_7: 2.052  loss_dice_7: 4.148  loss_ce_8: 3.023  loss_mask_8: 1.78  loss_dice_8: 4.135    time: 0.4382  last_time: 0.4266  data_time: 0.0223  last_data_time: 0.0194   lr: 1e-05  max_mem: 6714M\n",
      "\u001b[32m[07/17 08:39:35 d2.utils.events]: \u001b[0m eta: 1 day, 20:41:50  iter: 239  total_loss: 95.22  loss_ce: 3.075  loss_mask: 1.837  loss_dice: 4.144  loss_ce_0: 8.502  loss_mask_0: 1.885  loss_dice_0: 3.863  loss_ce_1: 3.085  loss_mask_1: 1.95  loss_dice_1: 3.841  loss_ce_2: 3.306  loss_mask_2: 1.928  loss_dice_2: 3.875  loss_ce_3: 3.221  loss_mask_3: 1.95  loss_dice_3: 3.903  loss_ce_4: 3.084  loss_mask_4: 1.822  loss_dice_4: 3.917  loss_ce_5: 3.094  loss_mask_5: 1.899  loss_dice_5: 3.932  loss_ce_6: 2.964  loss_mask_6: 1.928  loss_dice_6: 4.162  loss_ce_7: 3.114  loss_mask_7: 1.812  loss_dice_7: 4.032  loss_ce_8: 3.331  loss_mask_8: 2.044  loss_dice_8: 4.03    time: 0.4379  last_time: 0.4355  data_time: 0.0230  last_data_time: 0.0222   lr: 1e-05  max_mem: 6714M\n",
      "\u001b[32m[07/17 08:39:44 d2.utils.events]: \u001b[0m eta: 1 day, 20:41:07  iter: 259  total_loss: 97.74  loss_ce: 3.431  loss_mask: 2.124  loss_dice: 3.868  loss_ce_0: 8.582  loss_mask_0: 2.025  loss_dice_0: 3.743  loss_ce_1: 3.291  loss_mask_1: 2.078  loss_dice_1: 3.739  loss_ce_2: 3.397  loss_mask_2: 2.146  loss_dice_2: 3.808  loss_ce_3: 3.408  loss_mask_3: 2.107  loss_dice_3: 3.762  loss_ce_4: 3.505  loss_mask_4: 2.09  loss_dice_4: 3.765  loss_ce_5: 3.395  loss_mask_5: 2.24  loss_dice_5: 3.854  loss_ce_6: 3.369  loss_mask_6: 2.066  loss_dice_6: 3.985  loss_ce_7: 3.325  loss_mask_7: 1.977  loss_dice_7: 3.847  loss_ce_8: 3.563  loss_mask_8: 2.148  loss_dice_8: 4.02    time: 0.4380  last_time: 0.4406  data_time: 0.0236  last_data_time: 0.0246   lr: 1e-05  max_mem: 6714M\n",
      "\u001b[32m[07/17 08:39:52 d2.utils.events]: \u001b[0m eta: 1 day, 20:40:45  iter: 279  total_loss: 98.9  loss_ce: 3.524  loss_mask: 1.918  loss_dice: 3.811  loss_ce_0: 8.488  loss_mask_0: 1.942  loss_dice_0: 3.743  loss_ce_1: 3.412  loss_mask_1: 1.911  loss_dice_1: 3.653  loss_ce_2: 3.43  loss_mask_2: 2.043  loss_dice_2: 3.808  loss_ce_3: 3.49  loss_mask_3: 1.939  loss_dice_3: 3.797  loss_ce_4: 3.531  loss_mask_4: 1.934  loss_dice_4: 3.821  loss_ce_5: 3.585  loss_mask_5: 1.931  loss_dice_5: 3.717  loss_ce_6: 3.653  loss_mask_6: 1.999  loss_dice_6: 3.857  loss_ce_7: 3.544  loss_mask_7: 1.967  loss_dice_7: 3.915  loss_ce_8: 3.572  loss_mask_8: 1.915  loss_dice_8: 3.677    time: 0.4380  last_time: 0.4401  data_time: 0.0245  last_data_time: 0.0299   lr: 1e-05  max_mem: 6714M\n",
      "\u001b[32m[07/17 08:40:01 d2.utils.events]: \u001b[0m eta: 1 day, 20:40:36  iter: 299  total_loss: 93.07  loss_ce: 2.879  loss_mask: 2.015  loss_dice: 3.598  loss_ce_0: 8.547  loss_mask_0: 2.047  loss_dice_0: 3.499  loss_ce_1: 2.585  loss_mask_1: 1.941  loss_dice_1: 3.602  loss_ce_2: 2.733  loss_mask_2: 1.991  loss_dice_2: 3.587  loss_ce_3: 2.842  loss_mask_3: 1.94  loss_dice_3: 3.717  loss_ce_4: 2.847  loss_mask_4: 1.982  loss_dice_4: 3.765  loss_ce_5: 2.662  loss_mask_5: 2.004  loss_dice_5: 3.698  loss_ce_6: 2.936  loss_mask_6: 1.918  loss_dice_6: 3.534  loss_ce_7: 2.719  loss_mask_7: 2.082  loss_dice_7: 3.86  loss_ce_8: 3.012  loss_mask_8: 1.934  loss_dice_8: 3.587    time: 0.4379  last_time: 0.4432  data_time: 0.0220  last_data_time: 0.0283   lr: 1e-05  max_mem: 6714M\n",
      "\u001b[32m[07/17 08:40:10 d2.utils.events]: \u001b[0m eta: 1 day, 20:40:22  iter: 319  total_loss: 97.97  loss_ce: 3.409  loss_mask: 2.208  loss_dice: 3.756  loss_ce_0: 8.492  loss_mask_0: 2.158  loss_dice_0: 3.569  loss_ce_1: 3.573  loss_mask_1: 2.158  loss_dice_1: 3.554  loss_ce_2: 3.486  loss_mask_2: 2.165  loss_dice_2: 3.626  loss_ce_3: 3.402  loss_mask_3: 2.173  loss_dice_3: 3.616  loss_ce_4: 3.535  loss_mask_4: 2.148  loss_dice_4: 3.615  loss_ce_5: 3.511  loss_mask_5: 2.175  loss_dice_5: 3.64  loss_ce_6: 3.425  loss_mask_6: 2.047  loss_dice_6: 3.561  loss_ce_7: 3.359  loss_mask_7: 2.283  loss_dice_7: 3.683  loss_ce_8: 3.347  loss_mask_8: 2.144  loss_dice_8: 3.646    time: 0.4378  last_time: 0.4405  data_time: 0.0232  last_data_time: 0.0290   lr: 1e-05  max_mem: 6714M\n",
      "\u001b[32m[07/17 08:40:19 d2.utils.events]: \u001b[0m eta: 1 day, 20:39:44  iter: 339  total_loss: 95.02  loss_ce: 3.273  loss_mask: 1.834  loss_dice: 4.048  loss_ce_0: 8.474  loss_mask_0: 1.907  loss_dice_0: 3.81  loss_ce_1: 3.185  loss_mask_1: 1.827  loss_dice_1: 3.752  loss_ce_2: 3.223  loss_mask_2: 1.878  loss_dice_2: 3.736  loss_ce_3: 3.273  loss_mask_3: 1.949  loss_dice_3: 3.783  loss_ce_4: 3.142  loss_mask_4: 1.96  loss_dice_4: 3.867  loss_ce_5: 3.16  loss_mask_5: 1.933  loss_dice_5: 3.851  loss_ce_6: 3.11  loss_mask_6: 1.833  loss_dice_6: 4.047  loss_ce_7: 3.233  loss_mask_7: 1.913  loss_dice_7: 4.031  loss_ce_8: 3.223  loss_mask_8: 1.884  loss_dice_8: 4.038    time: 0.4378  last_time: 0.4402  data_time: 0.0240  last_data_time: 0.0218   lr: 1e-05  max_mem: 6714M\n",
      "\u001b[32m[07/17 08:40:27 d2.utils.events]: \u001b[0m eta: 1 day, 20:38:33  iter: 359  total_loss: 90.64  loss_ce: 2.882  loss_mask: 1.995  loss_dice: 3.79  loss_ce_0: 8.364  loss_mask_0: 2.189  loss_dice_0: 3.612  loss_ce_1: 2.659  loss_mask_1: 2.041  loss_dice_1: 3.553  loss_ce_2: 2.648  loss_mask_2: 2.076  loss_dice_2: 3.449  loss_ce_3: 2.603  loss_mask_3: 2.059  loss_dice_3: 3.493  loss_ce_4: 2.676  loss_mask_4: 2.19  loss_dice_4: 3.872  loss_ce_5: 2.652  loss_mask_5: 2.057  loss_dice_5: 3.666  loss_ce_6: 2.718  loss_mask_6: 2.051  loss_dice_6: 3.705  loss_ce_7: 2.749  loss_mask_7: 2.049  loss_dice_7: 3.73  loss_ce_8: 2.784  loss_mask_8: 1.993  loss_dice_8: 3.654    time: 0.4376  last_time: 0.4313  data_time: 0.0218  last_data_time: 0.0178   lr: 1e-05  max_mem: 6714M\n",
      "\u001b[32m[07/17 08:40:36 d2.utils.events]: \u001b[0m eta: 1 day, 20:37:29  iter: 379  total_loss: 95.46  loss_ce: 3.514  loss_mask: 1.994  loss_dice: 3.762  loss_ce_0: 8.455  loss_mask_0: 1.98  loss_dice_0: 3.604  loss_ce_1: 3.21  loss_mask_1: 2.041  loss_dice_1: 3.49  loss_ce_2: 3.308  loss_mask_2: 1.981  loss_dice_2: 3.557  loss_ce_3: 3.422  loss_mask_3: 2.026  loss_dice_3: 3.59  loss_ce_4: 3.51  loss_mask_4: 2.117  loss_dice_4: 3.639  loss_ce_5: 3.588  loss_mask_5: 2.007  loss_dice_5: 3.553  loss_ce_6: 3.229  loss_mask_6: 2.041  loss_dice_6: 3.638  loss_ce_7: 3.296  loss_mask_7: 2.118  loss_dice_7: 3.712  loss_ce_8: 3.479  loss_mask_8: 1.996  loss_dice_8: 3.561    time: 0.4375  last_time: 0.4366  data_time: 0.0228  last_data_time: 0.0268   lr: 1e-05  max_mem: 6714M\n",
      "\u001b[32m[07/17 08:40:45 d2.utils.events]: \u001b[0m eta: 1 day, 20:38:16  iter: 399  total_loss: 95.65  loss_ce: 3.347  loss_mask: 2.168  loss_dice: 3.508  loss_ce_0: 8.378  loss_mask_0: 2.2  loss_dice_0: 3.351  loss_ce_1: 3.399  loss_mask_1: 2.15  loss_dice_1: 3.418  loss_ce_2: 3.501  loss_mask_2: 2.21  loss_dice_2: 3.414  loss_ce_3: 3.635  loss_mask_3: 2.172  loss_dice_3: 3.275  loss_ce_4: 3.513  loss_mask_4: 2.194  loss_dice_4: 3.405  loss_ce_5: 3.523  loss_mask_5: 2.13  loss_dice_5: 3.287  loss_ce_6: 3.363  loss_mask_6: 2.299  loss_dice_6: 3.281  loss_ce_7: 3.371  loss_mask_7: 2.025  loss_dice_7: 3.617  loss_ce_8: 3.303  loss_mask_8: 2.299  loss_dice_8: 3.598    time: 0.4379  last_time: 0.4478  data_time: 0.0246  last_data_time: 0.0325   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 08:40:54 d2.utils.events]: \u001b[0m eta: 1 day, 20:37:02  iter: 419  total_loss: 90.95  loss_ce: 3.02  loss_mask: 2.097  loss_dice: 3.44  loss_ce_0: 8.395  loss_mask_0: 2.175  loss_dice_0: 3.284  loss_ce_1: 3.189  loss_mask_1: 2.158  loss_dice_1: 3.202  loss_ce_2: 3.161  loss_mask_2: 2.055  loss_dice_2: 3.137  loss_ce_3: 3.072  loss_mask_3: 2.231  loss_dice_3: 3.275  loss_ce_4: 2.989  loss_mask_4: 2.261  loss_dice_4: 3.463  loss_ce_5: 2.818  loss_mask_5: 2.126  loss_dice_5: 3.329  loss_ce_6: 3.018  loss_mask_6: 2.162  loss_dice_6: 3.325  loss_ce_7: 2.936  loss_mask_7: 2.173  loss_dice_7: 3.387  loss_ce_8: 2.845  loss_mask_8: 2.483  loss_dice_8: 3.461    time: 0.4380  last_time: 0.4331  data_time: 0.0224  last_data_time: 0.0196   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 08:41:03 d2.utils.events]: \u001b[0m eta: 1 day, 20:36:39  iter: 439  total_loss: 93.65  loss_ce: 3.481  loss_mask: 2.015  loss_dice: 3.45  loss_ce_0: 8.262  loss_mask_0: 2.008  loss_dice_0: 3.404  loss_ce_1: 3.359  loss_mask_1: 1.861  loss_dice_1: 3.419  loss_ce_2: 3.542  loss_mask_2: 1.924  loss_dice_2: 3.251  loss_ce_3: 3.635  loss_mask_3: 1.96  loss_dice_3: 3.454  loss_ce_4: 3.491  loss_mask_4: 1.968  loss_dice_4: 3.333  loss_ce_5: 3.56  loss_mask_5: 1.923  loss_dice_5: 3.323  loss_ce_6: 3.566  loss_mask_6: 1.956  loss_dice_6: 3.363  loss_ce_7: 3.517  loss_mask_7: 2.027  loss_dice_7: 3.441  loss_ce_8: 3.562  loss_mask_8: 2.004  loss_dice_8: 3.434    time: 0.4380  last_time: 0.4383  data_time: 0.0237  last_data_time: 0.0234   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 08:41:11 d2.utils.events]: \u001b[0m eta: 1 day, 20:36:16  iter: 459  total_loss: 91.69  loss_ce: 3.648  loss_mask: 2.143  loss_dice: 3.178  loss_ce_0: 8.259  loss_mask_0: 1.997  loss_dice_0: 3.214  loss_ce_1: 3.624  loss_mask_1: 2.066  loss_dice_1: 3.058  loss_ce_2: 3.624  loss_mask_2: 2.067  loss_dice_2: 3.06  loss_ce_3: 3.64  loss_mask_3: 2.035  loss_dice_3: 3.083  loss_ce_4: 3.582  loss_mask_4: 2.088  loss_dice_4: 3.062  loss_ce_5: 3.517  loss_mask_5: 2.03  loss_dice_5: 3.052  loss_ce_6: 3.487  loss_mask_6: 2.091  loss_dice_6: 3.24  loss_ce_7: 3.581  loss_mask_7: 2.047  loss_dice_7: 3.392  loss_ce_8: 3.645  loss_mask_8: 2.163  loss_dice_8: 3.279    time: 0.4380  last_time: 0.4447  data_time: 0.0239  last_data_time: 0.0294   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 08:41:20 d2.utils.events]: \u001b[0m eta: 1 day, 20:36:36  iter: 479  total_loss: 91.8  loss_ce: 3.163  loss_mask: 2.321  loss_dice: 3.457  loss_ce_0: 8.25  loss_mask_0: 2.128  loss_dice_0: 3.375  loss_ce_1: 3.26  loss_mask_1: 2.029  loss_dice_1: 3.366  loss_ce_2: 3.176  loss_mask_2: 2.027  loss_dice_2: 3.311  loss_ce_3: 3.302  loss_mask_3: 1.899  loss_dice_3: 3.105  loss_ce_4: 3.161  loss_mask_4: 1.978  loss_dice_4: 3.251  loss_ce_5: 3.258  loss_mask_5: 2.092  loss_dice_5: 3.363  loss_ce_6: 3.183  loss_mask_6: 2.181  loss_dice_6: 3.513  loss_ce_7: 3.354  loss_mask_7: 2.063  loss_dice_7: 3.466  loss_ce_8: 3.091  loss_mask_8: 1.949  loss_dice_8: 3.329    time: 0.4380  last_time: 0.4327  data_time: 0.0234  last_data_time: 0.0225   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 08:41:29 d2.utils.events]: \u001b[0m eta: 1 day, 20:36:12  iter: 499  total_loss: 89.62  loss_ce: 3.248  loss_mask: 1.98  loss_dice: 3.196  loss_ce_0: 8.272  loss_mask_0: 1.953  loss_dice_0: 3.281  loss_ce_1: 2.984  loss_mask_1: 1.963  loss_dice_1: 3.232  loss_ce_2: 3.244  loss_mask_2: 1.849  loss_dice_2: 3.154  loss_ce_3: 3.207  loss_mask_3: 1.986  loss_dice_3: 3.048  loss_ce_4: 3.234  loss_mask_4: 1.988  loss_dice_4: 3.14  loss_ce_5: 3.083  loss_mask_5: 2.28  loss_dice_5: 3.329  loss_ce_6: 3.204  loss_mask_6: 2.126  loss_dice_6: 3.278  loss_ce_7: 3.28  loss_mask_7: 1.978  loss_dice_7: 3.252  loss_ce_8: 3.264  loss_mask_8: 1.994  loss_dice_8: 3    time: 0.4380  last_time: 0.4552  data_time: 0.0235  last_data_time: 0.0399   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 08:41:38 d2.utils.events]: \u001b[0m eta: 1 day, 20:35:47  iter: 519  total_loss: 86.38  loss_ce: 2.893  loss_mask: 2.047  loss_dice: 3.233  loss_ce_0: 8.151  loss_mask_0: 1.957  loss_dice_0: 3.346  loss_ce_1: 2.986  loss_mask_1: 1.904  loss_dice_1: 3.096  loss_ce_2: 3.07  loss_mask_2: 1.902  loss_dice_2: 3.148  loss_ce_3: 3.017  loss_mask_3: 1.984  loss_dice_3: 3.202  loss_ce_4: 2.977  loss_mask_4: 2.007  loss_dice_4: 3.207  loss_ce_5: 2.853  loss_mask_5: 2.071  loss_dice_5: 3.388  loss_ce_6: 2.97  loss_mask_6: 1.952  loss_dice_6: 3.243  loss_ce_7: 2.947  loss_mask_7: 1.957  loss_dice_7: 3.288  loss_ce_8: 2.92  loss_mask_8: 2.106  loss_dice_8: 3.501    time: 0.4379  last_time: 0.4370  data_time: 0.0220  last_data_time: 0.0199   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 08:41:46 d2.utils.events]: \u001b[0m eta: 1 day, 20:35:26  iter: 539  total_loss: 90.98  loss_ce: 2.987  loss_mask: 1.991  loss_dice: 3.386  loss_ce_0: 8.049  loss_mask_0: 1.872  loss_dice_0: 3.454  loss_ce_1: 3.123  loss_mask_1: 1.854  loss_dice_1: 3.207  loss_ce_2: 3.289  loss_mask_2: 1.922  loss_dice_2: 3.282  loss_ce_3: 3.166  loss_mask_3: 1.974  loss_dice_3: 3.149  loss_ce_4: 3.155  loss_mask_4: 1.994  loss_dice_4: 3.318  loss_ce_5: 3  loss_mask_5: 1.98  loss_dice_5: 3.421  loss_ce_6: 3.064  loss_mask_6: 2.032  loss_dice_6: 3.333  loss_ce_7: 3.195  loss_mask_7: 1.889  loss_dice_7: 3.374  loss_ce_8: 3.092  loss_mask_8: 2.142  loss_dice_8: 3.373    time: 0.4378  last_time: 0.4235  data_time: 0.0220  last_data_time: 0.0173   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 08:41:55 d2.utils.events]: \u001b[0m eta: 1 day, 20:35:18  iter: 559  total_loss: 89.98  loss_ce: 3.054  loss_mask: 2.23  loss_dice: 3.305  loss_ce_0: 8.194  loss_mask_0: 1.985  loss_dice_0: 3.281  loss_ce_1: 3.175  loss_mask_1: 2.09  loss_dice_1: 3.06  loss_ce_2: 3.373  loss_mask_2: 1.902  loss_dice_2: 3.047  loss_ce_3: 3.269  loss_mask_3: 2.052  loss_dice_3: 3.132  loss_ce_4: 3.256  loss_mask_4: 2.083  loss_dice_4: 2.998  loss_ce_5: 2.986  loss_mask_5: 2.118  loss_dice_5: 3.075  loss_ce_6: 3.17  loss_mask_6: 2.043  loss_dice_6: 3.117  loss_ce_7: 3.237  loss_mask_7: 2.006  loss_dice_7: 3.142  loss_ce_8: 3.187  loss_mask_8: 1.998  loss_dice_8: 3.147    time: 0.4378  last_time: 0.4417  data_time: 0.0230  last_data_time: 0.0310   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 08:42:04 d2.utils.events]: \u001b[0m eta: 1 day, 20:35:34  iter: 579  total_loss: 91.48  loss_ce: 3.476  loss_mask: 1.953  loss_dice: 3.166  loss_ce_0: 8.092  loss_mask_0: 1.84  loss_dice_0: 3.394  loss_ce_1: 3.511  loss_mask_1: 1.78  loss_dice_1: 3.159  loss_ce_2: 3.69  loss_mask_2: 1.86  loss_dice_2: 2.939  loss_ce_3: 3.542  loss_mask_3: 1.827  loss_dice_3: 3.148  loss_ce_4: 3.577  loss_mask_4: 1.899  loss_dice_4: 3.097  loss_ce_5: 3.4  loss_mask_5: 1.999  loss_dice_5: 3.126  loss_ce_6: 3.519  loss_mask_6: 1.91  loss_dice_6: 3.199  loss_ce_7: 3.596  loss_mask_7: 1.79  loss_dice_7: 3.15  loss_ce_8: 3.591  loss_mask_8: 2.001  loss_dice_8: 3.183    time: 0.4379  last_time: 0.4377  data_time: 0.0258  last_data_time: 0.0285   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 08:42:13 d2.utils.events]: \u001b[0m eta: 1 day, 20:35:25  iter: 599  total_loss: 90.92  loss_ce: 3.716  loss_mask: 2.168  loss_dice: 3.042  loss_ce_0: 8.049  loss_mask_0: 1.9  loss_dice_0: 3.157  loss_ce_1: 3.708  loss_mask_1: 1.864  loss_dice_1: 3.032  loss_ce_2: 3.734  loss_mask_2: 1.9  loss_dice_2: 2.872  loss_ce_3: 3.825  loss_mask_3: 2.054  loss_dice_3: 2.969  loss_ce_4: 3.835  loss_mask_4: 2.014  loss_dice_4: 2.905  loss_ce_5: 3.74  loss_mask_5: 2.104  loss_dice_5: 3.06  loss_ce_6: 3.76  loss_mask_6: 1.882  loss_dice_6: 3.112  loss_ce_7: 3.718  loss_mask_7: 1.952  loss_dice_7: 3.048  loss_ce_8: 3.63  loss_mask_8: 2.081  loss_dice_8: 3.176    time: 0.4379  last_time: 0.4386  data_time: 0.0245  last_data_time: 0.0234   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 08:42:21 d2.utils.events]: \u001b[0m eta: 1 day, 20:35:17  iter: 619  total_loss: 87.38  loss_ce: 3.256  loss_mask: 2.005  loss_dice: 3.037  loss_ce_0: 8.038  loss_mask_0: 1.923  loss_dice_0: 3.077  loss_ce_1: 3.114  loss_mask_1: 1.968  loss_dice_1: 2.929  loss_ce_2: 3.384  loss_mask_2: 1.816  loss_dice_2: 2.688  loss_ce_3: 3.276  loss_mask_3: 1.99  loss_dice_3: 2.902  loss_ce_4: 3.314  loss_mask_4: 2.004  loss_dice_4: 2.779  loss_ce_5: 3.397  loss_mask_5: 1.901  loss_dice_5: 2.918  loss_ce_6: 3.333  loss_mask_6: 2.047  loss_dice_6: 3.166  loss_ce_7: 3.378  loss_mask_7: 2.003  loss_dice_7: 3.001  loss_ce_8: 3.138  loss_mask_8: 2.163  loss_dice_8: 3.066    time: 0.4379  last_time: 0.4404  data_time: 0.0228  last_data_time: 0.0210   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 08:42:30 d2.utils.events]: \u001b[0m eta: 1 day, 20:35:26  iter: 639  total_loss: 91.99  loss_ce: 3.588  loss_mask: 2  loss_dice: 3.087  loss_ce_0: 8.021  loss_mask_0: 1.901  loss_dice_0: 3.215  loss_ce_1: 3.79  loss_mask_1: 1.745  loss_dice_1: 2.973  loss_ce_2: 3.774  loss_mask_2: 1.841  loss_dice_2: 2.947  loss_ce_3: 3.553  loss_mask_3: 1.881  loss_dice_3: 3.007  loss_ce_4: 3.636  loss_mask_4: 2.02  loss_dice_4: 2.945  loss_ce_5: 3.783  loss_mask_5: 1.935  loss_dice_5: 2.987  loss_ce_6: 3.654  loss_mask_6: 2.046  loss_dice_6: 3.056  loss_ce_7: 3.798  loss_mask_7: 1.914  loss_dice_7: 3.021  loss_ce_8: 3.571  loss_mask_8: 2.035  loss_dice_8: 3.064    time: 0.4379  last_time: 0.4370  data_time: 0.0242  last_data_time: 0.0243   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 08:42:39 d2.utils.events]: \u001b[0m eta: 1 day, 20:34:59  iter: 659  total_loss: 89.24  loss_ce: 3.241  loss_mask: 1.896  loss_dice: 3.051  loss_ce_0: 8.094  loss_mask_0: 1.958  loss_dice_0: 3.176  loss_ce_1: 3.392  loss_mask_1: 1.884  loss_dice_1: 2.906  loss_ce_2: 3.311  loss_mask_2: 2.02  loss_dice_2: 2.982  loss_ce_3: 3.394  loss_mask_3: 2.117  loss_dice_3: 3.102  loss_ce_4: 3.317  loss_mask_4: 1.98  loss_dice_4: 2.987  loss_ce_5: 3.266  loss_mask_5: 2.169  loss_dice_5: 3.099  loss_ce_6: 3.31  loss_mask_6: 2.265  loss_dice_6: 3.245  loss_ce_7: 3.123  loss_mask_7: 2.068  loss_dice_7: 3.033  loss_ce_8: 3.193  loss_mask_8: 1.909  loss_dice_8: 3.125    time: 0.4379  last_time: 0.4321  data_time: 0.0235  last_data_time: 0.0239   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 08:42:48 d2.utils.events]: \u001b[0m eta: 1 day, 20:34:51  iter: 679  total_loss: 88.55  loss_ce: 3.422  loss_mask: 1.939  loss_dice: 3.044  loss_ce_0: 7.951  loss_mask_0: 1.837  loss_dice_0: 3.09  loss_ce_1: 3.25  loss_mask_1: 1.894  loss_dice_1: 2.988  loss_ce_2: 3.333  loss_mask_2: 1.838  loss_dice_2: 2.923  loss_ce_3: 3.21  loss_mask_3: 2.042  loss_dice_3: 2.903  loss_ce_4: 3.354  loss_mask_4: 2.088  loss_dice_4: 2.944  loss_ce_5: 3.361  loss_mask_5: 1.975  loss_dice_5: 3.127  loss_ce_6: 3.223  loss_mask_6: 2.274  loss_dice_6: 3.19  loss_ce_7: 3.28  loss_mask_7: 2.025  loss_dice_7: 3.145  loss_ce_8: 3.31  loss_mask_8: 2.007  loss_dice_8: 2.994    time: 0.4380  last_time: 0.4478  data_time: 0.0253  last_data_time: 0.0376   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 08:42:57 d2.utils.events]: \u001b[0m eta: 1 day, 20:34:42  iter: 699  total_loss: 88.11  loss_ce: 3.001  loss_mask: 1.851  loss_dice: 2.939  loss_ce_0: 7.881  loss_mask_0: 1.887  loss_dice_0: 3.357  loss_ce_1: 3.06  loss_mask_1: 1.845  loss_dice_1: 2.969  loss_ce_2: 3.119  loss_mask_2: 1.861  loss_dice_2: 2.999  loss_ce_3: 3.172  loss_mask_3: 1.793  loss_dice_3: 3.008  loss_ce_4: 3.229  loss_mask_4: 1.75  loss_dice_4: 2.988  loss_ce_5: 3.161  loss_mask_5: 1.806  loss_dice_5: 3.029  loss_ce_6: 3.173  loss_mask_6: 1.97  loss_dice_6: 3.032  loss_ce_7: 3.218  loss_mask_7: 1.914  loss_dice_7: 3.06  loss_ce_8: 3.089  loss_mask_8: 1.997  loss_dice_8: 3.149    time: 0.4380  last_time: 0.4392  data_time: 0.0239  last_data_time: 0.0236   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 08:43:05 d2.utils.events]: \u001b[0m eta: 1 day, 20:34:22  iter: 719  total_loss: 88.69  loss_ce: 3.32  loss_mask: 2.066  loss_dice: 3.112  loss_ce_0: 7.898  loss_mask_0: 2.066  loss_dice_0: 3.089  loss_ce_1: 3.231  loss_mask_1: 2.074  loss_dice_1: 2.877  loss_ce_2: 3.401  loss_mask_2: 1.932  loss_dice_2: 2.801  loss_ce_3: 3.332  loss_mask_3: 1.994  loss_dice_3: 2.915  loss_ce_4: 3.369  loss_mask_4: 2.045  loss_dice_4: 3.053  loss_ce_5: 3.359  loss_mask_5: 1.96  loss_dice_5: 3.007  loss_ce_6: 3.431  loss_mask_6: 2.008  loss_dice_6: 3.129  loss_ce_7: 3.402  loss_mask_7: 1.978  loss_dice_7: 3.082  loss_ce_8: 3.335  loss_mask_8: 2.023  loss_dice_8: 3.142    time: 0.4380  last_time: 0.4306  data_time: 0.0233  last_data_time: 0.0232   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 08:43:14 d2.utils.events]: \u001b[0m eta: 1 day, 20:34:14  iter: 739  total_loss: 90.99  loss_ce: 3.392  loss_mask: 2.222  loss_dice: 3.422  loss_ce_0: 7.782  loss_mask_0: 1.887  loss_dice_0: 3.314  loss_ce_1: 3.573  loss_mask_1: 1.956  loss_dice_1: 3.034  loss_ce_2: 3.616  loss_mask_2: 1.988  loss_dice_2: 3.133  loss_ce_3: 3.66  loss_mask_3: 2.133  loss_dice_3: 3.131  loss_ce_4: 3.631  loss_mask_4: 2.017  loss_dice_4: 3.053  loss_ce_5: 3.64  loss_mask_5: 2.018  loss_dice_5: 3.111  loss_ce_6: 3.601  loss_mask_6: 2.026  loss_dice_6: 3.287  loss_ce_7: 3.659  loss_mask_7: 2.059  loss_dice_7: 3.187  loss_ce_8: 3.474  loss_mask_8: 2.175  loss_dice_8: 3.327    time: 0.4380  last_time: 0.4333  data_time: 0.0235  last_data_time: 0.0235   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 08:43:23 d2.utils.events]: \u001b[0m eta: 1 day, 20:34:27  iter: 759  total_loss: 87.95  loss_ce: 3.667  loss_mask: 2.117  loss_dice: 2.88  loss_ce_0: 7.894  loss_mask_0: 1.946  loss_dice_0: 2.903  loss_ce_1: 3.594  loss_mask_1: 2.176  loss_dice_1: 2.843  loss_ce_2: 3.425  loss_mask_2: 2.349  loss_dice_2: 2.716  loss_ce_3: 3.341  loss_mask_3: 2.218  loss_dice_3: 2.817  loss_ce_4: 3.402  loss_mask_4: 2.26  loss_dice_4: 2.743  loss_ce_5: 3.483  loss_mask_5: 2.255  loss_dice_5: 2.711  loss_ce_6: 3.553  loss_mask_6: 2.357  loss_dice_6: 2.784  loss_ce_7: 3.441  loss_mask_7: 2.085  loss_dice_7: 2.855  loss_ce_8: 3.475  loss_mask_8: 2.259  loss_dice_8: 2.948    time: 0.4380  last_time: 0.4282  data_time: 0.0235  last_data_time: 0.0185   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 08:43:32 d2.utils.events]: \u001b[0m eta: 1 day, 20:33:53  iter: 779  total_loss: 85.47  loss_ce: 3.277  loss_mask: 2.251  loss_dice: 3.03  loss_ce_0: 7.793  loss_mask_0: 2.052  loss_dice_0: 2.994  loss_ce_1: 3.139  loss_mask_1: 2.103  loss_dice_1: 2.631  loss_ce_2: 3.243  loss_mask_2: 2.072  loss_dice_2: 2.74  loss_ce_3: 3.383  loss_mask_3: 2.205  loss_dice_3: 2.676  loss_ce_4: 3.312  loss_mask_4: 2.146  loss_dice_4: 2.618  loss_ce_5: 3.276  loss_mask_5: 2.094  loss_dice_5: 2.664  loss_ce_6: 3.187  loss_mask_6: 2.313  loss_dice_6: 2.954  loss_ce_7: 3.215  loss_mask_7: 2.296  loss_dice_7: 2.758  loss_ce_8: 3.122  loss_mask_8: 2.115  loss_dice_8: 2.846    time: 0.4379  last_time: 0.4345  data_time: 0.0225  last_data_time: 0.0207   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 08:43:40 d2.utils.events]: \u001b[0m eta: 1 day, 20:33:45  iter: 799  total_loss: 85.11  loss_ce: 3.335  loss_mask: 2.003  loss_dice: 3.028  loss_ce_0: 7.801  loss_mask_0: 1.969  loss_dice_0: 2.945  loss_ce_1: 3.441  loss_mask_1: 1.841  loss_dice_1: 2.77  loss_ce_2: 3.341  loss_mask_2: 1.974  loss_dice_2: 2.789  loss_ce_3: 3.424  loss_mask_3: 1.862  loss_dice_3: 2.827  loss_ce_4: 3.452  loss_mask_4: 2.231  loss_dice_4: 2.809  loss_ce_5: 3.429  loss_mask_5: 2.132  loss_dice_5: 2.907  loss_ce_6: 3.396  loss_mask_6: 2.082  loss_dice_6: 3.108  loss_ce_7: 3.279  loss_mask_7: 2.234  loss_dice_7: 3.122  loss_ce_8: 3.314  loss_mask_8: 2.046  loss_dice_8: 2.922    time: 0.4379  last_time: 0.4352  data_time: 0.0230  last_data_time: 0.0245   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 08:43:49 d2.utils.events]: \u001b[0m eta: 1 day, 20:33:17  iter: 819  total_loss: 84.97  loss_ce: 2.947  loss_mask: 2.052  loss_dice: 3.318  loss_ce_0: 7.771  loss_mask_0: 2.059  loss_dice_0: 2.855  loss_ce_1: 2.986  loss_mask_1: 2.085  loss_dice_1: 2.972  loss_ce_2: 3.127  loss_mask_2: 1.901  loss_dice_2: 2.668  loss_ce_3: 3.051  loss_mask_3: 1.958  loss_dice_3: 2.799  loss_ce_4: 2.915  loss_mask_4: 2.173  loss_dice_4: 3.096  loss_ce_5: 3.005  loss_mask_5: 2.297  loss_dice_5: 3.162  loss_ce_6: 2.849  loss_mask_6: 2.227  loss_dice_6: 3.068  loss_ce_7: 3.064  loss_mask_7: 2.084  loss_dice_7: 3.156  loss_ce_8: 3.083  loss_mask_8: 1.863  loss_dice_8: 2.891    time: 0.4378  last_time: 0.4371  data_time: 0.0230  last_data_time: 0.0240   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 08:43:58 d2.utils.events]: \u001b[0m eta: 1 day, 20:32:43  iter: 839  total_loss: 82.37  loss_ce: 2.764  loss_mask: 1.889  loss_dice: 2.642  loss_ce_0: 7.616  loss_mask_0: 1.833  loss_dice_0: 2.888  loss_ce_1: 2.702  loss_mask_1: 1.899  loss_dice_1: 2.826  loss_ce_2: 2.834  loss_mask_2: 2.016  loss_dice_2: 2.783  loss_ce_3: 2.913  loss_mask_3: 1.896  loss_dice_3: 2.703  loss_ce_4: 3.055  loss_mask_4: 1.873  loss_dice_4: 2.659  loss_ce_5: 2.85  loss_mask_5: 1.956  loss_dice_5: 2.758  loss_ce_6: 2.843  loss_mask_6: 2.042  loss_dice_6: 2.841  loss_ce_7: 2.829  loss_mask_7: 1.974  loss_dice_7: 2.792  loss_ce_8: 2.841  loss_mask_8: 2.057  loss_dice_8: 2.696    time: 0.4377  last_time: 0.4402  data_time: 0.0220  last_data_time: 0.0234   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 08:44:07 d2.utils.events]: \u001b[0m eta: 1 day, 20:32:35  iter: 859  total_loss: 83.59  loss_ce: 3.18  loss_mask: 1.881  loss_dice: 2.848  loss_ce_0: 7.748  loss_mask_0: 1.799  loss_dice_0: 2.995  loss_ce_1: 2.959  loss_mask_1: 1.942  loss_dice_1: 2.86  loss_ce_2: 3.043  loss_mask_2: 1.992  loss_dice_2: 2.756  loss_ce_3: 3.167  loss_mask_3: 1.937  loss_dice_3: 2.858  loss_ce_4: 3.055  loss_mask_4: 2.073  loss_dice_4: 2.835  loss_ce_5: 3.001  loss_mask_5: 1.978  loss_dice_5: 2.929  loss_ce_6: 3.159  loss_mask_6: 1.867  loss_dice_6: 2.957  loss_ce_7: 3.105  loss_mask_7: 1.951  loss_dice_7: 2.911  loss_ce_8: 3.169  loss_mask_8: 1.951  loss_dice_8: 2.889    time: 0.4377  last_time: 0.4359  data_time: 0.0221  last_data_time: 0.0233   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 08:44:15 d2.utils.events]: \u001b[0m eta: 1 day, 20:32:23  iter: 879  total_loss: 84.35  loss_ce: 2.923  loss_mask: 2.184  loss_dice: 3.085  loss_ce_0: 7.625  loss_mask_0: 1.93  loss_dice_0: 3.214  loss_ce_1: 2.979  loss_mask_1: 1.811  loss_dice_1: 2.901  loss_ce_2: 2.955  loss_mask_2: 1.869  loss_dice_2: 2.904  loss_ce_3: 2.968  loss_mask_3: 2.089  loss_dice_3: 2.839  loss_ce_4: 3.039  loss_mask_4: 2.055  loss_dice_4: 2.914  loss_ce_5: 3.068  loss_mask_5: 2.265  loss_dice_5: 2.974  loss_ce_6: 2.847  loss_mask_6: 1.926  loss_dice_6: 2.951  loss_ce_7: 2.97  loss_mask_7: 1.93  loss_dice_7: 2.811  loss_ce_8: 2.965  loss_mask_8: 2.015  loss_dice_8: 3.071    time: 0.4377  last_time: 0.4335  data_time: 0.0217  last_data_time: 0.0216   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 08:44:24 d2.utils.events]: \u001b[0m eta: 1 day, 20:32:09  iter: 899  total_loss: 87.65  loss_ce: 3.381  loss_mask: 2.052  loss_dice: 3.09  loss_ce_0: 7.586  loss_mask_0: 2.006  loss_dice_0: 3.218  loss_ce_1: 3.247  loss_mask_1: 1.959  loss_dice_1: 3.023  loss_ce_2: 3.389  loss_mask_2: 2.103  loss_dice_2: 3.075  loss_ce_3: 3.365  loss_mask_3: 2.238  loss_dice_3: 3.143  loss_ce_4: 3.085  loss_mask_4: 1.887  loss_dice_4: 2.926  loss_ce_5: 3.242  loss_mask_5: 1.968  loss_dice_5: 3.132  loss_ce_6: 3.379  loss_mask_6: 1.947  loss_dice_6: 2.94  loss_ce_7: 3.325  loss_mask_7: 1.911  loss_dice_7: 2.968  loss_ce_8: 3.452  loss_mask_8: 2.174  loss_dice_8: 3.008    time: 0.4377  last_time: 0.4312  data_time: 0.0240  last_data_time: 0.0228   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 08:44:33 d2.utils.events]: \u001b[0m eta: 1 day, 20:31:49  iter: 919  total_loss: 89.09  loss_ce: 3.273  loss_mask: 1.997  loss_dice: 3.32  loss_ce_0: 7.516  loss_mask_0: 1.806  loss_dice_0: 3.34  loss_ce_1: 3.175  loss_mask_1: 1.96  loss_dice_1: 3.238  loss_ce_2: 3.312  loss_mask_2: 1.938  loss_dice_2: 3.245  loss_ce_3: 3.127  loss_mask_3: 2.081  loss_dice_3: 3.373  loss_ce_4: 3.108  loss_mask_4: 1.961  loss_dice_4: 3.206  loss_ce_5: 3.217  loss_mask_5: 1.793  loss_dice_5: 3.304  loss_ce_6: 3.17  loss_mask_6: 1.901  loss_dice_6: 3.344  loss_ce_7: 3.169  loss_mask_7: 1.812  loss_dice_7: 3.297  loss_ce_8: 3.26  loss_mask_8: 1.88  loss_dice_8: 3.288    time: 0.4377  last_time: 0.4332  data_time: 0.0227  last_data_time: 0.0220   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 08:44:42 d2.utils.events]: \u001b[0m eta: 1 day, 20:31:34  iter: 939  total_loss: 82.2  loss_ce: 2.866  loss_mask: 2.172  loss_dice: 3.252  loss_ce_0: 7.486  loss_mask_0: 1.949  loss_dice_0: 2.978  loss_ce_1: 3.133  loss_mask_1: 1.917  loss_dice_1: 3.048  loss_ce_2: 3.282  loss_mask_2: 2.093  loss_dice_2: 3.056  loss_ce_3: 2.959  loss_mask_3: 2.136  loss_dice_3: 3.126  loss_ce_4: 2.962  loss_mask_4: 2.218  loss_dice_4: 3.09  loss_ce_5: 3.058  loss_mask_5: 1.86  loss_dice_5: 3.163  loss_ce_6: 2.903  loss_mask_6: 2.062  loss_dice_6: 3.231  loss_ce_7: 2.983  loss_mask_7: 1.981  loss_dice_7: 3.271  loss_ce_8: 2.926  loss_mask_8: 2.041  loss_dice_8: 3.065    time: 0.4376  last_time: 0.4379  data_time: 0.0227  last_data_time: 0.0215   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 08:44:50 d2.utils.events]: \u001b[0m eta: 1 day, 20:30:56  iter: 959  total_loss: 82.45  loss_ce: 3.335  loss_mask: 2.098  loss_dice: 2.87  loss_ce_0: 7.418  loss_mask_0: 1.983  loss_dice_0: 3.024  loss_ce_1: 3.14  loss_mask_1: 1.943  loss_dice_1: 2.683  loss_ce_2: 3.356  loss_mask_2: 1.874  loss_dice_2: 2.638  loss_ce_3: 3.308  loss_mask_3: 2.103  loss_dice_3: 2.804  loss_ce_4: 3.226  loss_mask_4: 1.969  loss_dice_4: 2.837  loss_ce_5: 3.238  loss_mask_5: 1.883  loss_dice_5: 2.708  loss_ce_6: 3.36  loss_mask_6: 2.075  loss_dice_6: 3.031  loss_ce_7: 3.389  loss_mask_7: 1.994  loss_dice_7: 2.725  loss_ce_8: 3.374  loss_mask_8: 1.843  loss_dice_8: 2.869    time: 0.4376  last_time: 0.4292  data_time: 0.0230  last_data_time: 0.0222   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 08:44:59 d2.utils.events]: \u001b[0m eta: 1 day, 20:30:44  iter: 979  total_loss: 85.62  loss_ce: 2.933  loss_mask: 1.975  loss_dice: 3.381  loss_ce_0: 7.433  loss_mask_0: 1.808  loss_dice_0: 3.296  loss_ce_1: 2.794  loss_mask_1: 1.754  loss_dice_1: 3.184  loss_ce_2: 2.886  loss_mask_2: 1.895  loss_dice_2: 3.088  loss_ce_3: 2.905  loss_mask_3: 2.075  loss_dice_3: 3.201  loss_ce_4: 2.755  loss_mask_4: 1.931  loss_dice_4: 3.222  loss_ce_5: 2.947  loss_mask_5: 1.958  loss_dice_5: 3.362  loss_ce_6: 2.864  loss_mask_6: 1.987  loss_dice_6: 3.282  loss_ce_7: 2.938  loss_mask_7: 1.743  loss_dice_7: 3.235  loss_ce_8: 2.868  loss_mask_8: 2.039  loss_dice_8: 3.232    time: 0.4376  last_time: 0.4362  data_time: 0.0224  last_data_time: 0.0261   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 08:45:08 d2.utils.events]: \u001b[0m eta: 1 day, 20:30:39  iter: 999  total_loss: 89.9  loss_ce: 3.253  loss_mask: 1.928  loss_dice: 3.103  loss_ce_0: 7.475  loss_mask_0: 2.016  loss_dice_0: 3.273  loss_ce_1: 3.235  loss_mask_1: 2.13  loss_dice_1: 3.13  loss_ce_2: 3.298  loss_mask_2: 1.963  loss_dice_2: 3.063  loss_ce_3: 3.188  loss_mask_3: 1.899  loss_dice_3: 3.015  loss_ce_4: 3.128  loss_mask_4: 1.992  loss_dice_4: 2.993  loss_ce_5: 3.162  loss_mask_5: 1.992  loss_dice_5: 3.094  loss_ce_6: 3.425  loss_mask_6: 2.071  loss_dice_6: 3.072  loss_ce_7: 3.162  loss_mask_7: 2.118  loss_dice_7: 3.114  loss_ce_8: 3.277  loss_mask_8: 2.009  loss_dice_8: 3.053    time: 0.4376  last_time: 0.4542  data_time: 0.0241  last_data_time: 0.0213   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 08:45:17 d2.utils.events]: \u001b[0m eta: 1 day, 20:30:27  iter: 1019  total_loss: 86.35  loss_ce: 3.349  loss_mask: 2.091  loss_dice: 2.861  loss_ce_0: 7.427  loss_mask_0: 1.958  loss_dice_0: 2.839  loss_ce_1: 3.206  loss_mask_1: 1.914  loss_dice_1: 2.674  loss_ce_2: 3.261  loss_mask_2: 1.873  loss_dice_2: 2.657  loss_ce_3: 3.064  loss_mask_3: 2.01  loss_dice_3: 2.767  loss_ce_4: 3.316  loss_mask_4: 1.982  loss_dice_4: 2.765  loss_ce_5: 3.215  loss_mask_5: 2.256  loss_dice_5: 2.889  loss_ce_6: 3.25  loss_mask_6: 2.03  loss_dice_6: 2.738  loss_ce_7: 3.173  loss_mask_7: 2.05  loss_dice_7: 2.912  loss_ce_8: 3.103  loss_mask_8: 2.189  loss_dice_8: 2.662    time: 0.4376  last_time: 0.4372  data_time: 0.0235  last_data_time: 0.0208   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 08:45:25 d2.utils.events]: \u001b[0m eta: 1 day, 20:29:33  iter: 1039  total_loss: 83.77  loss_ce: 2.816  loss_mask: 1.965  loss_dice: 3.009  loss_ce_0: 7.44  loss_mask_0: 1.906  loss_dice_0: 3.092  loss_ce_1: 2.853  loss_mask_1: 1.751  loss_dice_1: 2.975  loss_ce_2: 2.898  loss_mask_2: 1.779  loss_dice_2: 2.937  loss_ce_3: 2.782  loss_mask_3: 1.95  loss_dice_3: 2.949  loss_ce_4: 2.921  loss_mask_4: 1.986  loss_dice_4: 3.144  loss_ce_5: 2.8  loss_mask_5: 2.273  loss_dice_5: 3.082  loss_ce_6: 2.719  loss_mask_6: 1.993  loss_dice_6: 3.072  loss_ce_7: 2.68  loss_mask_7: 2.102  loss_dice_7: 3.154  loss_ce_8: 2.774  loss_mask_8: 2.062  loss_dice_8: 2.973    time: 0.4375  last_time: 0.4256  data_time: 0.0225  last_data_time: 0.0200   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 08:45:34 d2.utils.events]: \u001b[0m eta: 1 day, 20:27:47  iter: 1059  total_loss: 83.7  loss_ce: 2.895  loss_mask: 2.219  loss_dice: 2.966  loss_ce_0: 7.262  loss_mask_0: 2.123  loss_dice_0: 2.964  loss_ce_1: 2.99  loss_mask_1: 1.9  loss_dice_1: 2.581  loss_ce_2: 2.96  loss_mask_2: 2.093  loss_dice_2: 2.651  loss_ce_3: 2.983  loss_mask_3: 2.126  loss_dice_3: 2.952  loss_ce_4: 2.944  loss_mask_4: 2.09  loss_dice_4: 2.845  loss_ce_5: 2.9  loss_mask_5: 2.307  loss_dice_5: 2.995  loss_ce_6: 2.99  loss_mask_6: 2.157  loss_dice_6: 2.819  loss_ce_7: 2.95  loss_mask_7: 2.128  loss_dice_7: 2.942  loss_ce_8: 2.877  loss_mask_8: 2.101  loss_dice_8: 2.955    time: 0.4375  last_time: 0.4271  data_time: 0.0218  last_data_time: 0.0211   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 08:45:43 d2.utils.events]: \u001b[0m eta: 1 day, 20:27:03  iter: 1079  total_loss: 87.94  loss_ce: 3.18  loss_mask: 2.013  loss_dice: 3.035  loss_ce_0: 7.238  loss_mask_0: 1.845  loss_dice_0: 3.003  loss_ce_1: 3.073  loss_mask_1: 1.9  loss_dice_1: 2.881  loss_ce_2: 3.185  loss_mask_2: 2.016  loss_dice_2: 2.758  loss_ce_3: 3.282  loss_mask_3: 2.116  loss_dice_3: 2.908  loss_ce_4: 3.16  loss_mask_4: 1.85  loss_dice_4: 2.865  loss_ce_5: 3.136  loss_mask_5: 2.09  loss_dice_5: 3.076  loss_ce_6: 3.235  loss_mask_6: 2.106  loss_dice_6: 3.082  loss_ce_7: 3.226  loss_mask_7: 2.132  loss_dice_7: 3.164  loss_ce_8: 3.203  loss_mask_8: 1.935  loss_dice_8: 2.832    time: 0.4374  last_time: 0.4346  data_time: 0.0225  last_data_time: 0.0239   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 08:45:51 d2.utils.events]: \u001b[0m eta: 1 day, 20:26:49  iter: 1099  total_loss: 83.73  loss_ce: 3.02  loss_mask: 1.888  loss_dice: 3.104  loss_ce_0: 7.212  loss_mask_0: 1.925  loss_dice_0: 3.08  loss_ce_1: 2.931  loss_mask_1: 1.77  loss_dice_1: 2.965  loss_ce_2: 2.93  loss_mask_2: 1.78  loss_dice_2: 3.116  loss_ce_3: 2.973  loss_mask_3: 2.033  loss_dice_3: 2.97  loss_ce_4: 3.031  loss_mask_4: 1.935  loss_dice_4: 2.995  loss_ce_5: 2.914  loss_mask_5: 1.985  loss_dice_5: 3.134  loss_ce_6: 2.933  loss_mask_6: 2.076  loss_dice_6: 3.157  loss_ce_7: 2.97  loss_mask_7: 1.914  loss_dice_7: 3.255  loss_ce_8: 2.951  loss_mask_8: 1.988  loss_dice_8: 3.157    time: 0.4374  last_time: 0.4225  data_time: 0.0234  last_data_time: 0.0174   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 08:46:00 d2.utils.events]: \u001b[0m eta: 1 day, 20:26:46  iter: 1119  total_loss: 89.62  loss_ce: 3.434  loss_mask: 1.95  loss_dice: 3.062  loss_ce_0: 7.29  loss_mask_0: 1.933  loss_dice_0: 3.13  loss_ce_1: 3.376  loss_mask_1: 1.909  loss_dice_1: 2.954  loss_ce_2: 3.562  loss_mask_2: 1.895  loss_dice_2: 2.822  loss_ce_3: 3.408  loss_mask_3: 2.082  loss_dice_3: 2.999  loss_ce_4: 3.609  loss_mask_4: 2.056  loss_dice_4: 2.995  loss_ce_5: 3.477  loss_mask_5: 2.339  loss_dice_5: 2.978  loss_ce_6: 3.58  loss_mask_6: 2.212  loss_dice_6: 3.046  loss_ce_7: 3.624  loss_mask_7: 2.292  loss_dice_7: 3.218  loss_ce_8: 3.573  loss_mask_8: 2.077  loss_dice_8: 3.08    time: 0.4374  last_time: 0.4344  data_time: 0.0232  last_data_time: 0.0235   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 08:46:09 d2.utils.events]: \u001b[0m eta: 1 day, 20:26:41  iter: 1139  total_loss: 85.43  loss_ce: 3.1  loss_mask: 1.973  loss_dice: 3.078  loss_ce_0: 7.158  loss_mask_0: 1.87  loss_dice_0: 3.116  loss_ce_1: 2.978  loss_mask_1: 2.008  loss_dice_1: 3.028  loss_ce_2: 2.993  loss_mask_2: 1.858  loss_dice_2: 2.856  loss_ce_3: 2.922  loss_mask_3: 1.87  loss_dice_3: 3.023  loss_ce_4: 3.071  loss_mask_4: 1.824  loss_dice_4: 2.991  loss_ce_5: 2.903  loss_mask_5: 1.964  loss_dice_5: 3.016  loss_ce_6: 3.036  loss_mask_6: 1.984  loss_dice_6: 3.147  loss_ce_7: 2.951  loss_mask_7: 1.882  loss_dice_7: 2.989  loss_ce_8: 3.093  loss_mask_8: 1.928  loss_dice_8: 2.978    time: 0.4374  last_time: 0.4330  data_time: 0.0232  last_data_time: 0.0223   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 08:46:18 d2.utils.events]: \u001b[0m eta: 1 day, 20:26:22  iter: 1159  total_loss: 88.39  loss_ce: 3.308  loss_mask: 2.184  loss_dice: 3.089  loss_ce_0: 7.207  loss_mask_0: 1.998  loss_dice_0: 3.104  loss_ce_1: 3.214  loss_mask_1: 1.975  loss_dice_1: 2.885  loss_ce_2: 3.287  loss_mask_2: 2.083  loss_dice_2: 2.826  loss_ce_3: 3.151  loss_mask_3: 2.185  loss_dice_3: 3.047  loss_ce_4: 3.379  loss_mask_4: 2.025  loss_dice_4: 2.965  loss_ce_5: 3.29  loss_mask_5: 1.957  loss_dice_5: 3.078  loss_ce_6: 3.225  loss_mask_6: 2.181  loss_dice_6: 2.851  loss_ce_7: 3.242  loss_mask_7: 2.124  loss_dice_7: 3  loss_ce_8: 3.363  loss_mask_8: 2.171  loss_dice_8: 3.099    time: 0.4373  last_time: 0.4297  data_time: 0.0227  last_data_time: 0.0198   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 08:46:26 d2.utils.events]: \u001b[0m eta: 1 day, 20:25:35  iter: 1179  total_loss: 86.2  loss_ce: 2.9  loss_mask: 1.843  loss_dice: 3.344  loss_ce_0: 7.161  loss_mask_0: 1.7  loss_dice_0: 3.037  loss_ce_1: 2.954  loss_mask_1: 1.749  loss_dice_1: 2.944  loss_ce_2: 2.851  loss_mask_2: 1.776  loss_dice_2: 2.854  loss_ce_3: 2.948  loss_mask_3: 1.86  loss_dice_3: 2.956  loss_ce_4: 2.959  loss_mask_4: 1.981  loss_dice_4: 3.162  loss_ce_5: 2.982  loss_mask_5: 1.976  loss_dice_5: 3.104  loss_ce_6: 2.937  loss_mask_6: 2.098  loss_dice_6: 3.264  loss_ce_7: 2.725  loss_mask_7: 1.929  loss_dice_7: 3.129  loss_ce_8: 3.038  loss_mask_8: 1.777  loss_dice_8: 3.24    time: 0.4373  last_time: 0.4303  data_time: 0.0223  last_data_time: 0.0209   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 08:46:35 d2.utils.events]: \u001b[0m eta: 1 day, 20:25:02  iter: 1199  total_loss: 84.42  loss_ce: 3.251  loss_mask: 2.022  loss_dice: 2.824  loss_ce_0: 7.077  loss_mask_0: 1.883  loss_dice_0: 2.946  loss_ce_1: 3.427  loss_mask_1: 1.647  loss_dice_1: 2.734  loss_ce_2: 3.324  loss_mask_2: 1.782  loss_dice_2: 2.816  loss_ce_3: 3.315  loss_mask_3: 2.007  loss_dice_3: 2.872  loss_ce_4: 3.299  loss_mask_4: 1.792  loss_dice_4: 2.838  loss_ce_5: 3.338  loss_mask_5: 1.822  loss_dice_5: 3.014  loss_ce_6: 3.124  loss_mask_6: 1.968  loss_dice_6: 3.046  loss_ce_7: 3.191  loss_mask_7: 1.952  loss_dice_7: 2.869  loss_ce_8: 3.287  loss_mask_8: 1.869  loss_dice_8: 2.954    time: 0.4373  last_time: 0.4375  data_time: 0.0230  last_data_time: 0.0264   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 08:46:44 d2.utils.events]: \u001b[0m eta: 1 day, 20:24:49  iter: 1219  total_loss: 84.57  loss_ce: 3.011  loss_mask: 2.076  loss_dice: 2.601  loss_ce_0: 7.151  loss_mask_0: 2.156  loss_dice_0: 2.827  loss_ce_1: 2.876  loss_mask_1: 2.054  loss_dice_1: 2.531  loss_ce_2: 2.841  loss_mask_2: 2.27  loss_dice_2: 2.721  loss_ce_3: 2.763  loss_mask_3: 2.24  loss_dice_3: 2.633  loss_ce_4: 2.965  loss_mask_4: 2.15  loss_dice_4: 2.568  loss_ce_5: 2.876  loss_mask_5: 2.09  loss_dice_5: 2.657  loss_ce_6: 2.922  loss_mask_6: 2.324  loss_dice_6: 2.709  loss_ce_7: 3.028  loss_mask_7: 1.978  loss_dice_7: 2.752  loss_ce_8: 2.957  loss_mask_8: 2.1  loss_dice_8: 2.719    time: 0.4372  last_time: 0.4308  data_time: 0.0234  last_data_time: 0.0224   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 08:46:52 d2.utils.events]: \u001b[0m eta: 1 day, 20:24:40  iter: 1239  total_loss: 86.16  loss_ce: 3.205  loss_mask: 1.806  loss_dice: 3.081  loss_ce_0: 7.01  loss_mask_0: 1.719  loss_dice_0: 3.202  loss_ce_1: 3.142  loss_mask_1: 1.658  loss_dice_1: 2.924  loss_ce_2: 3.143  loss_mask_2: 1.892  loss_dice_2: 3.059  loss_ce_3: 2.961  loss_mask_3: 2.134  loss_dice_3: 3.271  loss_ce_4: 3.112  loss_mask_4: 1.848  loss_dice_4: 3.247  loss_ce_5: 3.122  loss_mask_5: 1.929  loss_dice_5: 3.088  loss_ce_6: 3.15  loss_mask_6: 1.95  loss_dice_6: 3.137  loss_ce_7: 3.157  loss_mask_7: 1.971  loss_dice_7: 3.072  loss_ce_8: 3.24  loss_mask_8: 1.929  loss_dice_8: 3.166    time: 0.4372  last_time: 0.4453  data_time: 0.0227  last_data_time: 0.0274   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 08:47:01 d2.utils.events]: \u001b[0m eta: 1 day, 20:24:05  iter: 1259  total_loss: 86.14  loss_ce: 2.953  loss_mask: 1.94  loss_dice: 2.925  loss_ce_0: 6.912  loss_mask_0: 2.013  loss_dice_0: 3.054  loss_ce_1: 2.837  loss_mask_1: 1.912  loss_dice_1: 2.828  loss_ce_2: 2.878  loss_mask_2: 2.034  loss_dice_2: 2.94  loss_ce_3: 2.945  loss_mask_3: 2.051  loss_dice_3: 2.754  loss_ce_4: 3.171  loss_mask_4: 1.887  loss_dice_4: 2.755  loss_ce_5: 3.034  loss_mask_5: 2.037  loss_dice_5: 2.88  loss_ce_6: 3.032  loss_mask_6: 2.031  loss_dice_6: 2.999  loss_ce_7: 3.04  loss_mask_7: 2.148  loss_dice_7: 2.967  loss_ce_8: 2.995  loss_mask_8: 2.031  loss_dice_8: 2.933    time: 0.4372  last_time: 0.4280  data_time: 0.0228  last_data_time: 0.0191   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 08:47:10 d2.utils.events]: \u001b[0m eta: 1 day, 20:23:57  iter: 1279  total_loss: 83.11  loss_ce: 3.229  loss_mask: 1.93  loss_dice: 2.729  loss_ce_0: 6.887  loss_mask_0: 1.966  loss_dice_0: 2.819  loss_ce_1: 3.193  loss_mask_1: 2.015  loss_dice_1: 2.605  loss_ce_2: 3.196  loss_mask_2: 1.971  loss_dice_2: 2.639  loss_ce_3: 3.094  loss_mask_3: 1.88  loss_dice_3: 2.609  loss_ce_4: 3.147  loss_mask_4: 1.908  loss_dice_4: 2.537  loss_ce_5: 3.095  loss_mask_5: 1.91  loss_dice_5: 2.829  loss_ce_6: 3.319  loss_mask_6: 2.098  loss_dice_6: 2.658  loss_ce_7: 3.234  loss_mask_7: 1.912  loss_dice_7: 2.684  loss_ce_8: 3.06  loss_mask_8: 1.867  loss_dice_8: 2.651    time: 0.4372  last_time: 0.4352  data_time: 0.0226  last_data_time: 0.0236   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 08:47:19 d2.utils.events]: \u001b[0m eta: 1 day, 20:23:58  iter: 1299  total_loss: 82.25  loss_ce: 3.042  loss_mask: 2.234  loss_dice: 3.182  loss_ce_0: 6.935  loss_mask_0: 1.886  loss_dice_0: 3.165  loss_ce_1: 3.001  loss_mask_1: 1.945  loss_dice_1: 2.785  loss_ce_2: 3.073  loss_mask_2: 1.814  loss_dice_2: 2.964  loss_ce_3: 3.049  loss_mask_3: 1.902  loss_dice_3: 2.921  loss_ce_4: 3.141  loss_mask_4: 1.951  loss_dice_4: 2.934  loss_ce_5: 3.117  loss_mask_5: 1.914  loss_dice_5: 2.866  loss_ce_6: 3.088  loss_mask_6: 1.912  loss_dice_6: 3.226  loss_ce_7: 3.136  loss_mask_7: 1.94  loss_dice_7: 2.861  loss_ce_8: 3.006  loss_mask_8: 2.111  loss_dice_8: 3.141    time: 0.4372  last_time: 0.4533  data_time: 0.0237  last_data_time: 0.0234   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 08:47:27 d2.utils.events]: \u001b[0m eta: 1 day, 20:23:21  iter: 1319  total_loss: 85.84  loss_ce: 2.939  loss_mask: 1.928  loss_dice: 2.966  loss_ce_0: 6.879  loss_mask_0: 1.918  loss_dice_0: 2.951  loss_ce_1: 2.794  loss_mask_1: 1.945  loss_dice_1: 2.635  loss_ce_2: 3.131  loss_mask_2: 1.972  loss_dice_2: 2.68  loss_ce_3: 3.004  loss_mask_3: 2.037  loss_dice_3: 2.826  loss_ce_4: 2.96  loss_mask_4: 1.973  loss_dice_4: 2.781  loss_ce_5: 2.824  loss_mask_5: 2.218  loss_dice_5: 2.908  loss_ce_6: 2.923  loss_mask_6: 2.257  loss_dice_6: 2.964  loss_ce_7: 2.961  loss_mask_7: 2.112  loss_dice_7: 2.871  loss_ce_8: 2.915  loss_mask_8: 1.95  loss_dice_8: 2.905    time: 0.4372  last_time: 0.4488  data_time: 0.0225  last_data_time: 0.0239   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 08:47:36 d2.utils.events]: \u001b[0m eta: 1 day, 20:23:33  iter: 1339  total_loss: 84.19  loss_ce: 3.011  loss_mask: 2.183  loss_dice: 2.88  loss_ce_0: 6.844  loss_mask_0: 1.857  loss_dice_0: 2.869  loss_ce_1: 3.315  loss_mask_1: 1.97  loss_dice_1: 2.754  loss_ce_2: 3.067  loss_mask_2: 2.115  loss_dice_2: 2.82  loss_ce_3: 3.316  loss_mask_3: 2.052  loss_dice_3: 2.729  loss_ce_4: 3.398  loss_mask_4: 1.998  loss_dice_4: 2.887  loss_ce_5: 3.393  loss_mask_5: 1.985  loss_dice_5: 2.695  loss_ce_6: 3.358  loss_mask_6: 2.265  loss_dice_6: 2.819  loss_ce_7: 3.314  loss_mask_7: 2.072  loss_dice_7: 2.723  loss_ce_8: 3.127  loss_mask_8: 2.283  loss_dice_8: 2.921    time: 0.4372  last_time: 0.4382  data_time: 0.0227  last_data_time: 0.0237   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 08:47:45 d2.utils.events]: \u001b[0m eta: 1 day, 20:23:22  iter: 1359  total_loss: 84.44  loss_ce: 3.115  loss_mask: 1.84  loss_dice: 3.139  loss_ce_0: 6.727  loss_mask_0: 1.838  loss_dice_0: 3.114  loss_ce_1: 3.055  loss_mask_1: 1.751  loss_dice_1: 2.913  loss_ce_2: 3.135  loss_mask_2: 1.784  loss_dice_2: 2.985  loss_ce_3: 3.185  loss_mask_3: 1.932  loss_dice_3: 2.889  loss_ce_4: 3.127  loss_mask_4: 1.919  loss_dice_4: 3.076  loss_ce_5: 3.066  loss_mask_5: 1.768  loss_dice_5: 2.954  loss_ce_6: 3.075  loss_mask_6: 1.735  loss_dice_6: 2.936  loss_ce_7: 3.016  loss_mask_7: 1.904  loss_dice_7: 2.986  loss_ce_8: 2.993  loss_mask_8: 1.851  loss_dice_8: 3.163    time: 0.4372  last_time: 0.4285  data_time: 0.0282  last_data_time: 0.0183   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 08:47:54 d2.utils.events]: \u001b[0m eta: 1 day, 20:23:23  iter: 1379  total_loss: 87.48  loss_ce: 3.226  loss_mask: 2.034  loss_dice: 3.128  loss_ce_0: 6.751  loss_mask_0: 2.129  loss_dice_0: 3.136  loss_ce_1: 3.161  loss_mask_1: 2  loss_dice_1: 2.917  loss_ce_2: 3.009  loss_mask_2: 2.036  loss_dice_2: 3.022  loss_ce_3: 3.114  loss_mask_3: 2.28  loss_dice_3: 2.927  loss_ce_4: 3.3  loss_mask_4: 2.023  loss_dice_4: 3  loss_ce_5: 3.171  loss_mask_5: 2.047  loss_dice_5: 3.057  loss_ce_6: 3.273  loss_mask_6: 1.932  loss_dice_6: 3.042  loss_ce_7: 3.228  loss_mask_7: 1.856  loss_dice_7: 3.156  loss_ce_8: 3.208  loss_mask_8: 2.127  loss_dice_8: 3.269    time: 0.4372  last_time: 0.4432  data_time: 0.0234  last_data_time: 0.0281   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 08:48:03 d2.utils.events]: \u001b[0m eta: 1 day, 20:23:07  iter: 1399  total_loss: 88.88  loss_ce: 3.203  loss_mask: 2.223  loss_dice: 3.075  loss_ce_0: 6.944  loss_mask_0: 1.939  loss_dice_0: 3.126  loss_ce_1: 3.078  loss_mask_1: 2.02  loss_dice_1: 2.935  loss_ce_2: 3.025  loss_mask_2: 2.25  loss_dice_2: 3.094  loss_ce_3: 2.968  loss_mask_3: 2.349  loss_dice_3: 3.357  loss_ce_4: 3.107  loss_mask_4: 2.13  loss_dice_4: 2.965  loss_ce_5: 3.084  loss_mask_5: 1.959  loss_dice_5: 2.884  loss_ce_6: 3.279  loss_mask_6: 2.139  loss_dice_6: 3.132  loss_ce_7: 3.18  loss_mask_7: 2.054  loss_dice_7: 2.945  loss_ce_8: 3.107  loss_mask_8: 2.235  loss_dice_8: 3.114    time: 0.4372  last_time: 0.4404  data_time: 0.0238  last_data_time: 0.0259   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 08:48:11 d2.utils.events]: \u001b[0m eta: 1 day, 20:23:05  iter: 1419  total_loss: 84.63  loss_ce: 3.286  loss_mask: 1.968  loss_dice: 2.916  loss_ce_0: 6.784  loss_mask_0: 1.865  loss_dice_0: 3.046  loss_ce_1: 3.414  loss_mask_1: 1.864  loss_dice_1: 2.881  loss_ce_2: 3.337  loss_mask_2: 1.762  loss_dice_2: 2.827  loss_ce_3: 3.386  loss_mask_3: 2.024  loss_dice_3: 3.078  loss_ce_4: 3.336  loss_mask_4: 2.056  loss_dice_4: 2.902  loss_ce_5: 3.227  loss_mask_5: 2.003  loss_dice_5: 3.04  loss_ce_6: 3.294  loss_mask_6: 1.906  loss_dice_6: 3.055  loss_ce_7: 3.334  loss_mask_7: 1.929  loss_dice_7: 2.97  loss_ce_8: 3.163  loss_mask_8: 2.025  loss_dice_8: 3.079    time: 0.4372  last_time: 0.4256  data_time: 0.0241  last_data_time: 0.0193   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 08:48:20 d2.utils.events]: \u001b[0m eta: 1 day, 20:22:51  iter: 1439  total_loss: 84.51  loss_ce: 2.914  loss_mask: 2.125  loss_dice: 2.901  loss_ce_0: 6.743  loss_mask_0: 2.133  loss_dice_0: 2.618  loss_ce_1: 2.854  loss_mask_1: 2.018  loss_dice_1: 2.471  loss_ce_2: 2.812  loss_mask_2: 2.079  loss_dice_2: 2.497  loss_ce_3: 2.748  loss_mask_3: 2.153  loss_dice_3: 2.474  loss_ce_4: 2.744  loss_mask_4: 2.109  loss_dice_4: 2.627  loss_ce_5: 2.722  loss_mask_5: 2.272  loss_dice_5: 2.588  loss_ce_6: 2.831  loss_mask_6: 2.324  loss_dice_6: 2.644  loss_ce_7: 2.731  loss_mask_7: 2.359  loss_dice_7: 2.75  loss_ce_8: 2.698  loss_mask_8: 2.186  loss_dice_8: 2.729    time: 0.4373  last_time: 0.4343  data_time: 0.0220  last_data_time: 0.0225   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 08:48:29 d2.utils.events]: \u001b[0m eta: 1 day, 20:22:42  iter: 1459  total_loss: 86.2  loss_ce: 3.207  loss_mask: 1.943  loss_dice: 3.072  loss_ce_0: 6.679  loss_mask_0: 1.877  loss_dice_0: 3.06  loss_ce_1: 3.282  loss_mask_1: 2.112  loss_dice_1: 2.995  loss_ce_2: 3.31  loss_mask_2: 2.106  loss_dice_2: 2.897  loss_ce_3: 3.183  loss_mask_3: 2.268  loss_dice_3: 3.196  loss_ce_4: 3.294  loss_mask_4: 1.954  loss_dice_4: 3.111  loss_ce_5: 3.171  loss_mask_5: 2.122  loss_dice_5: 3.017  loss_ce_6: 3.248  loss_mask_6: 2.061  loss_dice_6: 2.987  loss_ce_7: 3.231  loss_mask_7: 2  loss_dice_7: 3.078  loss_ce_8: 3.027  loss_mask_8: 2.125  loss_dice_8: 3.145    time: 0.4372  last_time: 0.4535  data_time: 0.0237  last_data_time: 0.0427   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 08:48:38 d2.utils.events]: \u001b[0m eta: 1 day, 20:21:42  iter: 1479  total_loss: 80.71  loss_ce: 2.764  loss_mask: 1.927  loss_dice: 2.945  loss_ce_0: 6.55  loss_mask_0: 1.986  loss_dice_0: 2.989  loss_ce_1: 2.836  loss_mask_1: 1.92  loss_dice_1: 2.747  loss_ce_2: 2.884  loss_mask_2: 1.83  loss_dice_2: 2.64  loss_ce_3: 2.788  loss_mask_3: 1.82  loss_dice_3: 2.844  loss_ce_4: 2.897  loss_mask_4: 1.923  loss_dice_4: 2.756  loss_ce_5: 2.717  loss_mask_5: 1.902  loss_dice_5: 2.895  loss_ce_6: 2.807  loss_mask_6: 1.919  loss_dice_6: 2.925  loss_ce_7: 2.882  loss_mask_7: 1.784  loss_dice_7: 2.797  loss_ce_8: 2.537  loss_mask_8: 2.346  loss_dice_8: 3.156    time: 0.4372  last_time: 0.4353  data_time: 0.0219  last_data_time: 0.0211   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 08:48:46 d2.utils.events]: \u001b[0m eta: 1 day, 20:21:27  iter: 1499  total_loss: 83.88  loss_ce: 2.982  loss_mask: 2.179  loss_dice: 3.046  loss_ce_0: 6.616  loss_mask_0: 2.053  loss_dice_0: 2.916  loss_ce_1: 3.101  loss_mask_1: 2.042  loss_dice_1: 2.83  loss_ce_2: 3.093  loss_mask_2: 2.059  loss_dice_2: 2.828  loss_ce_3: 3.014  loss_mask_3: 2.122  loss_dice_3: 2.927  loss_ce_4: 3.157  loss_mask_4: 1.984  loss_dice_4: 2.793  loss_ce_5: 3.182  loss_mask_5: 1.918  loss_dice_5: 2.911  loss_ce_6: 3.193  loss_mask_6: 2.204  loss_dice_6: 2.892  loss_ce_7: 3.092  loss_mask_7: 2.028  loss_dice_7: 2.972  loss_ce_8: 2.901  loss_mask_8: 2.284  loss_dice_8: 2.883    time: 0.4372  last_time: 0.4290  data_time: 0.0240  last_data_time: 0.0201   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 08:48:55 d2.utils.events]: \u001b[0m eta: 1 day, 20:21:21  iter: 1519  total_loss: 87.55  loss_ce: 3.661  loss_mask: 1.71  loss_dice: 2.98  loss_ce_0: 6.613  loss_mask_0: 1.819  loss_dice_0: 3.012  loss_ce_1: 3.699  loss_mask_1: 1.708  loss_dice_1: 2.857  loss_ce_2: 3.52  loss_mask_2: 1.796  loss_dice_2: 2.923  loss_ce_3: 3.645  loss_mask_3: 1.911  loss_dice_3: 3.074  loss_ce_4: 3.591  loss_mask_4: 1.833  loss_dice_4: 2.86  loss_ce_5: 3.627  loss_mask_5: 1.845  loss_dice_5: 3.016  loss_ce_6: 3.666  loss_mask_6: 1.888  loss_dice_6: 2.949  loss_ce_7: 3.538  loss_mask_7: 1.858  loss_dice_7: 2.958  loss_ce_8: 3.801  loss_mask_8: 1.982  loss_dice_8: 3.016    time: 0.4372  last_time: 0.4372  data_time: 0.0245  last_data_time: 0.0239   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 08:49:04 d2.utils.events]: \u001b[0m eta: 1 day, 20:21:12  iter: 1539  total_loss: 86.32  loss_ce: 3.321  loss_mask: 1.799  loss_dice: 3.056  loss_ce_0: 6.586  loss_mask_0: 2.073  loss_dice_0: 3.31  loss_ce_1: 3.296  loss_mask_1: 1.764  loss_dice_1: 2.966  loss_ce_2: 3.421  loss_mask_2: 1.774  loss_dice_2: 3.082  loss_ce_3: 3.3  loss_mask_3: 1.708  loss_dice_3: 3.052  loss_ce_4: 3.212  loss_mask_4: 1.83  loss_dice_4: 3.025  loss_ce_5: 3.351  loss_mask_5: 1.867  loss_dice_5: 3.149  loss_ce_6: 3.396  loss_mask_6: 1.719  loss_dice_6: 2.994  loss_ce_7: 3.255  loss_mask_7: 1.858  loss_dice_7: 3.084  loss_ce_8: 3.341  loss_mask_8: 1.879  loss_dice_8: 3.1    time: 0.4372  last_time: 0.4343  data_time: 0.0226  last_data_time: 0.0237   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 08:49:13 d2.utils.events]: \u001b[0m eta: 1 day, 20:21:07  iter: 1559  total_loss: 87.44  loss_ce: 3.447  loss_mask: 1.958  loss_dice: 2.88  loss_ce_0: 6.592  loss_mask_0: 2.003  loss_dice_0: 3.09  loss_ce_1: 3.528  loss_mask_1: 1.778  loss_dice_1: 2.723  loss_ce_2: 3.667  loss_mask_2: 1.798  loss_dice_2: 2.734  loss_ce_3: 3.593  loss_mask_3: 1.934  loss_dice_3: 2.9  loss_ce_4: 3.541  loss_mask_4: 1.936  loss_dice_4: 2.928  loss_ce_5: 3.595  loss_mask_5: 1.871  loss_dice_5: 3.008  loss_ce_6: 3.611  loss_mask_6: 1.965  loss_dice_6: 2.958  loss_ce_7: 3.5  loss_mask_7: 1.913  loss_dice_7: 2.929  loss_ce_8: 3.49  loss_mask_8: 1.899  loss_dice_8: 2.973    time: 0.4372  last_time: 0.4429  data_time: 0.0237  last_data_time: 0.0272   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 08:49:21 d2.utils.events]: \u001b[0m eta: 1 day, 20:20:51  iter: 1579  total_loss: 84.88  loss_ce: 3.361  loss_mask: 1.869  loss_dice: 3.08  loss_ce_0: 6.544  loss_mask_0: 1.795  loss_dice_0: 3.165  loss_ce_1: 3.531  loss_mask_1: 1.751  loss_dice_1: 2.848  loss_ce_2: 3.456  loss_mask_2: 1.853  loss_dice_2: 2.819  loss_ce_3: 3.414  loss_mask_3: 1.788  loss_dice_3: 2.915  loss_ce_4: 3.321  loss_mask_4: 1.825  loss_dice_4: 3.12  loss_ce_5: 3.391  loss_mask_5: 1.853  loss_dice_5: 3.061  loss_ce_6: 3.42  loss_mask_6: 1.874  loss_dice_6: 3.018  loss_ce_7: 3.471  loss_mask_7: 1.848  loss_dice_7: 3.09  loss_ce_8: 3.51  loss_mask_8: 1.841  loss_dice_8: 2.927    time: 0.4372  last_time: 0.4263  data_time: 0.0239  last_data_time: 0.0209   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 08:49:30 d2.utils.events]: \u001b[0m eta: 1 day, 20:20:38  iter: 1599  total_loss: 84.95  loss_ce: 2.948  loss_mask: 1.948  loss_dice: 2.848  loss_ce_0: 6.424  loss_mask_0: 2.073  loss_dice_0: 3.026  loss_ce_1: 2.987  loss_mask_1: 1.949  loss_dice_1: 2.784  loss_ce_2: 2.948  loss_mask_2: 1.92  loss_dice_2: 2.717  loss_ce_3: 2.891  loss_mask_3: 1.965  loss_dice_3: 2.944  loss_ce_4: 3.001  loss_mask_4: 1.926  loss_dice_4: 2.723  loss_ce_5: 2.943  loss_mask_5: 1.917  loss_dice_5: 3.074  loss_ce_6: 2.943  loss_mask_6: 1.949  loss_dice_6: 3.06  loss_ce_7: 2.892  loss_mask_7: 2.129  loss_dice_7: 3.082  loss_ce_8: 3.023  loss_mask_8: 1.999  loss_dice_8: 2.894    time: 0.4371  last_time: 0.4356  data_time: 0.0225  last_data_time: 0.0260   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 08:49:39 d2.utils.events]: \u001b[0m eta: 1 day, 20:20:17  iter: 1619  total_loss: 82.56  loss_ce: 3.033  loss_mask: 1.856  loss_dice: 2.9  loss_ce_0: 6.339  loss_mask_0: 1.979  loss_dice_0: 3.015  loss_ce_1: 2.976  loss_mask_1: 1.98  loss_dice_1: 2.782  loss_ce_2: 3.105  loss_mask_2: 1.973  loss_dice_2: 2.777  loss_ce_3: 2.932  loss_mask_3: 2.033  loss_dice_3: 2.886  loss_ce_4: 3.073  loss_mask_4: 1.828  loss_dice_4: 2.748  loss_ce_5: 3.044  loss_mask_5: 2.113  loss_dice_5: 2.83  loss_ce_6: 3.046  loss_mask_6: 2.088  loss_dice_6: 2.86  loss_ce_7: 2.986  loss_mask_7: 1.868  loss_dice_7: 2.832  loss_ce_8: 3.078  loss_mask_8: 1.96  loss_dice_8: 2.943    time: 0.4371  last_time: 0.4294  data_time: 0.0227  last_data_time: 0.0198   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 08:49:47 d2.utils.events]: \u001b[0m eta: 1 day, 20:19:33  iter: 1639  total_loss: 85.24  loss_ce: 3.178  loss_mask: 2.087  loss_dice: 2.876  loss_ce_0: 6.426  loss_mask_0: 1.91  loss_dice_0: 3.077  loss_ce_1: 3.095  loss_mask_1: 1.923  loss_dice_1: 2.825  loss_ce_2: 3.168  loss_mask_2: 2.171  loss_dice_2: 2.956  loss_ce_3: 3.107  loss_mask_3: 2.112  loss_dice_3: 3.22  loss_ce_4: 3.204  loss_mask_4: 1.908  loss_dice_4: 2.917  loss_ce_5: 3.111  loss_mask_5: 1.999  loss_dice_5: 2.89  loss_ce_6: 3.194  loss_mask_6: 2.016  loss_dice_6: 2.926  loss_ce_7: 3.131  loss_mask_7: 1.934  loss_dice_7: 2.973  loss_ce_8: 3.157  loss_mask_8: 2.003  loss_dice_8: 2.903    time: 0.4371  last_time: 0.4355  data_time: 0.0230  last_data_time: 0.0198   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 08:49:56 d2.utils.events]: \u001b[0m eta: 1 day, 20:19:24  iter: 1659  total_loss: 84.74  loss_ce: 3.741  loss_mask: 1.867  loss_dice: 2.845  loss_ce_0: 6.484  loss_mask_0: 1.825  loss_dice_0: 3.018  loss_ce_1: 3.56  loss_mask_1: 1.815  loss_dice_1: 2.742  loss_ce_2: 3.681  loss_mask_2: 1.81  loss_dice_2: 2.796  loss_ce_3: 3.494  loss_mask_3: 1.958  loss_dice_3: 2.971  loss_ce_4: 3.736  loss_mask_4: 1.8  loss_dice_4: 2.916  loss_ce_5: 3.852  loss_mask_5: 1.786  loss_dice_5: 2.882  loss_ce_6: 3.738  loss_mask_6: 2.067  loss_dice_6: 3.011  loss_ce_7: 3.757  loss_mask_7: 1.81  loss_dice_7: 2.895  loss_ce_8: 3.824  loss_mask_8: 1.882  loss_dice_8: 2.933    time: 0.4371  last_time: 0.4280  data_time: 0.0233  last_data_time: 0.0182   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 08:50:05 d2.utils.events]: \u001b[0m eta: 1 day, 20:19:14  iter: 1679  total_loss: 86.48  loss_ce: 3.403  loss_mask: 2.081  loss_dice: 3.118  loss_ce_0: 6.324  loss_mask_0: 1.842  loss_dice_0: 2.949  loss_ce_1: 3.251  loss_mask_1: 1.871  loss_dice_1: 2.887  loss_ce_2: 3.333  loss_mask_2: 1.824  loss_dice_2: 2.786  loss_ce_3: 3.318  loss_mask_3: 1.871  loss_dice_3: 2.932  loss_ce_4: 3.269  loss_mask_4: 1.963  loss_dice_4: 3.01  loss_ce_5: 3.289  loss_mask_5: 1.952  loss_dice_5: 3.058  loss_ce_6: 3.223  loss_mask_6: 1.993  loss_dice_6: 3.041  loss_ce_7: 3.187  loss_mask_7: 2.083  loss_dice_7: 2.94  loss_ce_8: 3.293  loss_mask_8: 1.894  loss_dice_8: 3.029    time: 0.4371  last_time: 0.4344  data_time: 0.0225  last_data_time: 0.0244   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 08:50:14 d2.utils.events]: \u001b[0m eta: 1 day, 20:18:57  iter: 1699  total_loss: 82.14  loss_ce: 3.166  loss_mask: 2.262  loss_dice: 2.832  loss_ce_0: 6.256  loss_mask_0: 1.899  loss_dice_0: 2.755  loss_ce_1: 3.201  loss_mask_1: 2.087  loss_dice_1: 2.568  loss_ce_2: 3.126  loss_mask_2: 1.945  loss_dice_2: 2.488  loss_ce_3: 3.118  loss_mask_3: 2.048  loss_dice_3: 2.675  loss_ce_4: 3.147  loss_mask_4: 2.017  loss_dice_4: 2.61  loss_ce_5: 3.205  loss_mask_5: 1.99  loss_dice_5: 2.607  loss_ce_6: 3.227  loss_mask_6: 1.925  loss_dice_6: 2.612  loss_ce_7: 3.239  loss_mask_7: 2.167  loss_dice_7: 2.772  loss_ce_8: 3.052  loss_mask_8: 2.327  loss_dice_8: 2.677    time: 0.4371  last_time: 0.4397  data_time: 0.0227  last_data_time: 0.0230   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 08:50:22 d2.utils.events]: \u001b[0m eta: 1 day, 20:18:55  iter: 1719  total_loss: 85.31  loss_ce: 3.386  loss_mask: 2.125  loss_dice: 2.778  loss_ce_0: 6.251  loss_mask_0: 1.888  loss_dice_0: 2.793  loss_ce_1: 3.33  loss_mask_1: 1.824  loss_dice_1: 2.719  loss_ce_2: 3.404  loss_mask_2: 2.089  loss_dice_2: 2.607  loss_ce_3: 3.34  loss_mask_3: 1.88  loss_dice_3: 2.958  loss_ce_4: 3.277  loss_mask_4: 1.891  loss_dice_4: 2.947  loss_ce_5: 3.334  loss_mask_5: 2.096  loss_dice_5: 2.772  loss_ce_6: 3.285  loss_mask_6: 2.068  loss_dice_6: 2.813  loss_ce_7: 3.171  loss_mask_7: 1.84  loss_dice_7: 2.965  loss_ce_8: 3.347  loss_mask_8: 1.885  loss_dice_8: 2.912    time: 0.4370  last_time: 0.4348  data_time: 0.0226  last_data_time: 0.0229   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 08:50:31 d2.utils.events]: \u001b[0m eta: 1 day, 20:18:21  iter: 1739  total_loss: 82.36  loss_ce: 3.07  loss_mask: 1.821  loss_dice: 2.792  loss_ce_0: 6.283  loss_mask_0: 1.953  loss_dice_0: 3.027  loss_ce_1: 3.13  loss_mask_1: 2.001  loss_dice_1: 2.861  loss_ce_2: 3.041  loss_mask_2: 1.872  loss_dice_2: 2.822  loss_ce_3: 2.984  loss_mask_3: 2.034  loss_dice_3: 2.899  loss_ce_4: 3.136  loss_mask_4: 1.823  loss_dice_4: 3.047  loss_ce_5: 3.06  loss_mask_5: 1.747  loss_dice_5: 2.859  loss_ce_6: 3.014  loss_mask_6: 2.086  loss_dice_6: 3.105  loss_ce_7: 3.145  loss_mask_7: 1.933  loss_dice_7: 2.93  loss_ce_8: 3.074  loss_mask_8: 2.003  loss_dice_8: 3.045    time: 0.4370  last_time: 0.4294  data_time: 0.0222  last_data_time: 0.0227   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 08:50:40 d2.utils.events]: \u001b[0m eta: 1 day, 20:17:54  iter: 1759  total_loss: 85.51  loss_ce: 3.098  loss_mask: 2.008  loss_dice: 3.047  loss_ce_0: 6.26  loss_mask_0: 1.96  loss_dice_0: 3.025  loss_ce_1: 3.154  loss_mask_1: 1.998  loss_dice_1: 2.852  loss_ce_2: 3.163  loss_mask_2: 1.899  loss_dice_2: 2.905  loss_ce_3: 3.314  loss_mask_3: 1.869  loss_dice_3: 2.909  loss_ce_4: 3.293  loss_mask_4: 1.905  loss_dice_4: 2.971  loss_ce_5: 3.286  loss_mask_5: 1.966  loss_dice_5: 2.975  loss_ce_6: 3.262  loss_mask_6: 1.891  loss_dice_6: 3.049  loss_ce_7: 3.389  loss_mask_7: 1.934  loss_dice_7: 2.84  loss_ce_8: 3.317  loss_mask_8: 1.994  loss_dice_8: 2.952    time: 0.4370  last_time: 0.4373  data_time: 0.0239  last_data_time: 0.0254   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 08:50:48 d2.utils.events]: \u001b[0m eta: 1 day, 20:17:45  iter: 1779  total_loss: 84.51  loss_ce: 3.045  loss_mask: 2.212  loss_dice: 2.861  loss_ce_0: 6.168  loss_mask_0: 2.123  loss_dice_0: 2.953  loss_ce_1: 3.063  loss_mask_1: 2  loss_dice_1: 2.875  loss_ce_2: 3.054  loss_mask_2: 2.008  loss_dice_2: 2.77  loss_ce_3: 3.102  loss_mask_3: 2.196  loss_dice_3: 2.742  loss_ce_4: 3.116  loss_mask_4: 2.141  loss_dice_4: 2.725  loss_ce_5: 3.1  loss_mask_5: 2.031  loss_dice_5: 2.858  loss_ce_6: 3.155  loss_mask_6: 2.114  loss_dice_6: 2.963  loss_ce_7: 3.124  loss_mask_7: 2.072  loss_dice_7: 2.902  loss_ce_8: 3.101  loss_mask_8: 2.261  loss_dice_8: 2.864    time: 0.4370  last_time: 0.4394  data_time: 0.0220  last_data_time: 0.0215   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 08:50:57 d2.utils.events]: \u001b[0m eta: 1 day, 20:17:30  iter: 1799  total_loss: 82.59  loss_ce: 3.445  loss_mask: 1.861  loss_dice: 2.88  loss_ce_0: 6.171  loss_mask_0: 1.749  loss_dice_0: 2.987  loss_ce_1: 3.357  loss_mask_1: 1.838  loss_dice_1: 2.668  loss_ce_2: 3.409  loss_mask_2: 2.171  loss_dice_2: 2.813  loss_ce_3: 3.377  loss_mask_3: 1.81  loss_dice_3: 2.792  loss_ce_4: 3.314  loss_mask_4: 1.797  loss_dice_4: 2.814  loss_ce_5: 3.315  loss_mask_5: 1.807  loss_dice_5: 2.829  loss_ce_6: 3.367  loss_mask_6: 1.718  loss_dice_6: 2.762  loss_ce_7: 3.358  loss_mask_7: 1.907  loss_dice_7: 2.989  loss_ce_8: 3.339  loss_mask_8: 1.752  loss_dice_8: 2.976    time: 0.4370  last_time: 0.4329  data_time: 0.0227  last_data_time: 0.0230   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 08:51:06 d2.utils.events]: \u001b[0m eta: 1 day, 20:17:30  iter: 1819  total_loss: 85.11  loss_ce: 2.946  loss_mask: 2.149  loss_dice: 2.875  loss_ce_0: 6.064  loss_mask_0: 1.925  loss_dice_0: 3.079  loss_ce_1: 3.04  loss_mask_1: 1.931  loss_dice_1: 2.966  loss_ce_2: 2.999  loss_mask_2: 1.975  loss_dice_2: 2.844  loss_ce_3: 3.054  loss_mask_3: 2.13  loss_dice_3: 3.038  loss_ce_4: 3.033  loss_mask_4: 2.045  loss_dice_4: 2.937  loss_ce_5: 3.019  loss_mask_5: 1.961  loss_dice_5: 2.986  loss_ce_6: 3.051  loss_mask_6: 1.983  loss_dice_6: 2.883  loss_ce_7: 3.031  loss_mask_7: 1.903  loss_dice_7: 2.838  loss_ce_8: 3.039  loss_mask_8: 2.02  loss_dice_8: 2.964    time: 0.4369  last_time: 0.4273  data_time: 0.0230  last_data_time: 0.0219   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 08:51:15 d2.utils.events]: \u001b[0m eta: 1 day, 20:17:32  iter: 1839  total_loss: 86.64  loss_ce: 3.578  loss_mask: 1.976  loss_dice: 2.829  loss_ce_0: 6.138  loss_mask_0: 1.964  loss_dice_0: 2.981  loss_ce_1: 3.445  loss_mask_1: 1.924  loss_dice_1: 2.723  loss_ce_2: 3.543  loss_mask_2: 1.838  loss_dice_2: 2.727  loss_ce_3: 3.574  loss_mask_3: 2.054  loss_dice_3: 2.967  loss_ce_4: 3.68  loss_mask_4: 1.95  loss_dice_4: 2.808  loss_ce_5: 3.767  loss_mask_5: 1.852  loss_dice_5: 2.714  loss_ce_6: 3.698  loss_mask_6: 1.935  loss_dice_6: 2.721  loss_ce_7: 3.718  loss_mask_7: 2.008  loss_dice_7: 2.842  loss_ce_8: 3.726  loss_mask_8: 1.933  loss_dice_8: 2.814    time: 0.4369  last_time: 0.4361  data_time: 0.0237  last_data_time: 0.0227   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 08:51:23 d2.utils.events]: \u001b[0m eta: 1 day, 20:17:18  iter: 1859  total_loss: 79.93  loss_ce: 2.93  loss_mask: 2.035  loss_dice: 2.998  loss_ce_0: 5.984  loss_mask_0: 1.96  loss_dice_0: 2.964  loss_ce_1: 2.804  loss_mask_1: 1.891  loss_dice_1: 2.821  loss_ce_2: 3.015  loss_mask_2: 1.804  loss_dice_2: 2.79  loss_ce_3: 2.931  loss_mask_3: 1.897  loss_dice_3: 2.937  loss_ce_4: 2.789  loss_mask_4: 1.876  loss_dice_4: 2.956  loss_ce_5: 2.887  loss_mask_5: 1.781  loss_dice_5: 2.879  loss_ce_6: 2.716  loss_mask_6: 1.895  loss_dice_6: 2.876  loss_ce_7: 2.729  loss_mask_7: 1.923  loss_dice_7: 2.821  loss_ce_8: 2.933  loss_mask_8: 1.799  loss_dice_8: 2.897    time: 0.4369  last_time: 0.4356  data_time: 0.0224  last_data_time: 0.0227   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 08:51:32 d2.utils.events]: \u001b[0m eta: 1 day, 20:17:15  iter: 1879  total_loss: 82.97  loss_ce: 3.49  loss_mask: 2.096  loss_dice: 2.551  loss_ce_0: 5.986  loss_mask_0: 2.044  loss_dice_0: 2.649  loss_ce_1: 3.446  loss_mask_1: 1.921  loss_dice_1: 2.525  loss_ce_2: 3.426  loss_mask_2: 1.886  loss_dice_2: 2.523  loss_ce_3: 3.449  loss_mask_3: 1.729  loss_dice_3: 2.624  loss_ce_4: 3.419  loss_mask_4: 1.978  loss_dice_4: 2.719  loss_ce_5: 3.476  loss_mask_5: 1.94  loss_dice_5: 2.572  loss_ce_6: 3.451  loss_mask_6: 1.959  loss_dice_6: 2.674  loss_ce_7: 3.398  loss_mask_7: 1.846  loss_dice_7: 2.474  loss_ce_8: 3.517  loss_mask_8: 1.82  loss_dice_8: 2.522    time: 0.4369  last_time: 0.4296  data_time: 0.0233  last_data_time: 0.0197   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 08:51:41 d2.utils.events]: \u001b[0m eta: 1 day, 20:16:56  iter: 1899  total_loss: 81.7  loss_ce: 3.278  loss_mask: 2.058  loss_dice: 2.588  loss_ce_0: 5.994  loss_mask_0: 1.95  loss_dice_0: 2.828  loss_ce_1: 3.241  loss_mask_1: 1.886  loss_dice_1: 2.47  loss_ce_2: 3.396  loss_mask_2: 1.95  loss_dice_2: 2.454  loss_ce_3: 3.217  loss_mask_3: 2.1  loss_dice_3: 2.785  loss_ce_4: 3.292  loss_mask_4: 2.067  loss_dice_4: 2.718  loss_ce_5: 3.313  loss_mask_5: 1.945  loss_dice_5: 2.702  loss_ce_6: 3.264  loss_mask_6: 1.923  loss_dice_6: 2.727  loss_ce_7: 3.344  loss_mask_7: 1.895  loss_dice_7: 2.55  loss_ce_8: 3.31  loss_mask_8: 1.813  loss_dice_8: 2.594    time: 0.4369  last_time: 0.4352  data_time: 0.0236  last_data_time: 0.0244   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 08:51:50 d2.utils.events]: \u001b[0m eta: 1 day, 20:16:55  iter: 1919  total_loss: 85.02  loss_ce: 2.607  loss_mask: 1.934  loss_dice: 3.216  loss_ce_0: 5.806  loss_mask_0: 1.919  loss_dice_0: 3.304  loss_ce_1: 2.694  loss_mask_1: 1.91  loss_dice_1: 3.19  loss_ce_2: 2.785  loss_mask_2: 1.831  loss_dice_2: 3.098  loss_ce_3: 2.816  loss_mask_3: 1.927  loss_dice_3: 3.255  loss_ce_4: 2.793  loss_mask_4: 1.82  loss_dice_4: 3.209  loss_ce_5: 2.689  loss_mask_5: 2.003  loss_dice_5: 3.255  loss_ce_6: 2.627  loss_mask_6: 1.955  loss_dice_6: 3.182  loss_ce_7: 2.733  loss_mask_7: 1.907  loss_dice_7: 3.127  loss_ce_8: 2.726  loss_mask_8: 1.858  loss_dice_8: 3.091    time: 0.4368  last_time: 0.4405  data_time: 0.0231  last_data_time: 0.0268   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 08:51:58 d2.utils.events]: \u001b[0m eta: 1 day, 20:16:43  iter: 1939  total_loss: 80.71  loss_ce: 2.684  loss_mask: 2.206  loss_dice: 2.777  loss_ce_0: 5.847  loss_mask_0: 2.234  loss_dice_0: 2.929  loss_ce_1: 2.785  loss_mask_1: 2.086  loss_dice_1: 2.675  loss_ce_2: 2.981  loss_mask_2: 1.834  loss_dice_2: 2.643  loss_ce_3: 2.891  loss_mask_3: 2.139  loss_dice_3: 2.82  loss_ce_4: 2.678  loss_mask_4: 2.426  loss_dice_4: 2.99  loss_ce_5: 2.832  loss_mask_5: 2.283  loss_dice_5: 2.859  loss_ce_6: 2.769  loss_mask_6: 2.268  loss_dice_6: 2.976  loss_ce_7: 2.846  loss_mask_7: 2.249  loss_dice_7: 2.849  loss_ce_8: 2.951  loss_mask_8: 2.19  loss_dice_8: 2.802    time: 0.4368  last_time: 0.4326  data_time: 0.0230  last_data_time: 0.0233   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 08:52:07 d2.utils.events]: \u001b[0m eta: 1 day, 20:16:37  iter: 1959  total_loss: 80.42  loss_ce: 2.829  loss_mask: 1.972  loss_dice: 2.792  loss_ce_0: 5.81  loss_mask_0: 2.042  loss_dice_0: 2.895  loss_ce_1: 2.84  loss_mask_1: 1.86  loss_dice_1: 2.776  loss_ce_2: 3.008  loss_mask_2: 1.869  loss_dice_2: 2.654  loss_ce_3: 2.78  loss_mask_3: 1.963  loss_dice_3: 2.806  loss_ce_4: 2.884  loss_mask_4: 1.907  loss_dice_4: 2.907  loss_ce_5: 3.087  loss_mask_5: 1.95  loss_dice_5: 2.748  loss_ce_6: 2.919  loss_mask_6: 1.906  loss_dice_6: 2.873  loss_ce_7: 2.933  loss_mask_7: 1.927  loss_dice_7: 2.714  loss_ce_8: 2.929  loss_mask_8: 1.964  loss_dice_8: 2.637    time: 0.4368  last_time: 0.4319  data_time: 0.0230  last_data_time: 0.0238   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 08:52:16 d2.utils.events]: \u001b[0m eta: 1 day, 20:16:21  iter: 1979  total_loss: 82.15  loss_ce: 3.091  loss_mask: 2.145  loss_dice: 2.823  loss_ce_0: 5.899  loss_mask_0: 1.896  loss_dice_0: 2.738  loss_ce_1: 3.249  loss_mask_1: 1.854  loss_dice_1: 2.586  loss_ce_2: 3.319  loss_mask_2: 1.921  loss_dice_2: 2.516  loss_ce_3: 3.268  loss_mask_3: 1.931  loss_dice_3: 2.608  loss_ce_4: 3.276  loss_mask_4: 1.976  loss_dice_4: 2.786  loss_ce_5: 3.295  loss_mask_5: 1.964  loss_dice_5: 2.599  loss_ce_6: 3.348  loss_mask_6: 2.003  loss_dice_6: 2.621  loss_ce_7: 3.322  loss_mask_7: 1.823  loss_dice_7: 2.532  loss_ce_8: 3.409  loss_mask_8: 1.957  loss_dice_8: 2.546    time: 0.4368  last_time: 0.4426  data_time: 0.0230  last_data_time: 0.0281   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 08:52:24 d2.utils.events]: \u001b[0m eta: 1 day, 20:16:12  iter: 1999  total_loss: 86  loss_ce: 3.39  loss_mask: 2.099  loss_dice: 3.188  loss_ce_0: 5.883  loss_mask_0: 1.944  loss_dice_0: 3.017  loss_ce_1: 3.518  loss_mask_1: 1.94  loss_dice_1: 2.746  loss_ce_2: 3.447  loss_mask_2: 1.784  loss_dice_2: 2.739  loss_ce_3: 3.631  loss_mask_3: 1.845  loss_dice_3: 2.809  loss_ce_4: 3.457  loss_mask_4: 1.977  loss_dice_4: 2.859  loss_ce_5: 3.602  loss_mask_5: 1.949  loss_dice_5: 2.933  loss_ce_6: 3.433  loss_mask_6: 1.9  loss_dice_6: 2.842  loss_ce_7: 3.566  loss_mask_7: 1.951  loss_dice_7: 2.825  loss_ce_8: 3.453  loss_mask_8: 1.848  loss_dice_8: 2.792    time: 0.4368  last_time: 0.4313  data_time: 0.0235  last_data_time: 0.0203   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 08:52:33 d2.utils.events]: \u001b[0m eta: 1 day, 20:16:11  iter: 2019  total_loss: 81.03  loss_ce: 3.251  loss_mask: 1.912  loss_dice: 2.98  loss_ce_0: 5.789  loss_mask_0: 1.871  loss_dice_0: 2.845  loss_ce_1: 3.271  loss_mask_1: 1.933  loss_dice_1: 2.535  loss_ce_2: 3.369  loss_mask_2: 1.778  loss_dice_2: 2.562  loss_ce_3: 3.258  loss_mask_3: 1.914  loss_dice_3: 2.61  loss_ce_4: 3.3  loss_mask_4: 1.939  loss_dice_4: 2.49  loss_ce_5: 3.341  loss_mask_5: 1.806  loss_dice_5: 2.709  loss_ce_6: 3.229  loss_mask_6: 1.734  loss_dice_6: 2.608  loss_ce_7: 3.283  loss_mask_7: 1.741  loss_dice_7: 2.602  loss_ce_8: 3.21  loss_mask_8: 1.755  loss_dice_8: 2.676    time: 0.4368  last_time: 0.4305  data_time: 0.0242  last_data_time: 0.0219   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 08:52:42 d2.utils.events]: \u001b[0m eta: 1 day, 20:16:04  iter: 2039  total_loss: 76.61  loss_ce: 3.022  loss_mask: 1.999  loss_dice: 2.69  loss_ce_0: 5.551  loss_mask_0: 2.084  loss_dice_0: 2.958  loss_ce_1: 2.926  loss_mask_1: 1.89  loss_dice_1: 2.614  loss_ce_2: 3.013  loss_mask_2: 1.942  loss_dice_2: 2.549  loss_ce_3: 2.952  loss_mask_3: 2.088  loss_dice_3: 2.835  loss_ce_4: 2.934  loss_mask_4: 1.987  loss_dice_4: 2.592  loss_ce_5: 2.913  loss_mask_5: 2.106  loss_dice_5: 2.619  loss_ce_6: 2.913  loss_mask_6: 2.328  loss_dice_6: 2.724  loss_ce_7: 2.944  loss_mask_7: 1.976  loss_dice_7: 2.864  loss_ce_8: 2.884  loss_mask_8: 2.104  loss_dice_8: 2.805    time: 0.4368  last_time: 0.4498  data_time: 0.0241  last_data_time: 0.0331   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 08:52:51 d2.utils.events]: \u001b[0m eta: 1 day, 20:16:13  iter: 2059  total_loss: 84.05  loss_ce: 3.227  loss_mask: 1.804  loss_dice: 3.025  loss_ce_0: 5.677  loss_mask_0: 1.88  loss_dice_0: 3.026  loss_ce_1: 3.404  loss_mask_1: 1.783  loss_dice_1: 2.721  loss_ce_2: 3.308  loss_mask_2: 1.773  loss_dice_2: 2.749  loss_ce_3: 3.429  loss_mask_3: 1.971  loss_dice_3: 2.91  loss_ce_4: 3.34  loss_mask_4: 1.739  loss_dice_4: 2.927  loss_ce_5: 3.389  loss_mask_5: 1.795  loss_dice_5: 3.132  loss_ce_6: 3.271  loss_mask_6: 1.89  loss_dice_6: 2.984  loss_ce_7: 3.401  loss_mask_7: 1.822  loss_dice_7: 2.924  loss_ce_8: 3.406  loss_mask_8: 1.934  loss_dice_8: 2.989    time: 0.4368  last_time: 0.4397  data_time: 0.0237  last_data_time: 0.0259   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 08:53:00 d2.utils.events]: \u001b[0m eta: 1 day, 20:16:21  iter: 2079  total_loss: 86.96  loss_ce: 3.324  loss_mask: 2.282  loss_dice: 3.094  loss_ce_0: 5.718  loss_mask_0: 2.011  loss_dice_0: 2.882  loss_ce_1: 3.415  loss_mask_1: 2.018  loss_dice_1: 2.715  loss_ce_2: 3.532  loss_mask_2: 1.976  loss_dice_2: 2.756  loss_ce_3: 3.427  loss_mask_3: 1.994  loss_dice_3: 2.76  loss_ce_4: 3.532  loss_mask_4: 1.914  loss_dice_4: 2.968  loss_ce_5: 3.499  loss_mask_5: 2.071  loss_dice_5: 2.971  loss_ce_6: 3.309  loss_mask_6: 2.173  loss_dice_6: 3.018  loss_ce_7: 3.399  loss_mask_7: 2.22  loss_dice_7: 3.007  loss_ce_8: 3.452  loss_mask_8: 2.057  loss_dice_8: 2.956    time: 0.4369  last_time: 0.4330  data_time: 0.0245  last_data_time: 0.0218   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 08:53:08 d2.utils.events]: \u001b[0m eta: 1 day, 20:16:15  iter: 2099  total_loss: 85.81  loss_ce: 3.185  loss_mask: 2.042  loss_dice: 2.938  loss_ce_0: 5.79  loss_mask_0: 1.87  loss_dice_0: 2.979  loss_ce_1: 3.197  loss_mask_1: 1.941  loss_dice_1: 2.775  loss_ce_2: 3.183  loss_mask_2: 2.102  loss_dice_2: 2.829  loss_ce_3: 3.251  loss_mask_3: 1.903  loss_dice_3: 2.837  loss_ce_4: 3.345  loss_mask_4: 1.932  loss_dice_4: 2.855  loss_ce_5: 3.381  loss_mask_5: 1.875  loss_dice_5: 2.85  loss_ce_6: 3.211  loss_mask_6: 1.948  loss_dice_6: 2.912  loss_ce_7: 3.152  loss_mask_7: 2.045  loss_dice_7: 2.976  loss_ce_8: 3.281  loss_mask_8: 2.063  loss_dice_8: 3.011    time: 0.4369  last_time: 0.4559  data_time: 0.0238  last_data_time: 0.0237   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 08:53:17 d2.utils.events]: \u001b[0m eta: 1 day, 20:16:15  iter: 2119  total_loss: 78.11  loss_ce: 3.011  loss_mask: 1.66  loss_dice: 2.829  loss_ce_0: 5.567  loss_mask_0: 1.713  loss_dice_0: 3.119  loss_ce_1: 2.872  loss_mask_1: 1.754  loss_dice_1: 2.857  loss_ce_2: 2.847  loss_mask_2: 1.879  loss_dice_2: 2.933  loss_ce_3: 2.884  loss_mask_3: 1.768  loss_dice_3: 2.868  loss_ce_4: 2.933  loss_mask_4: 1.532  loss_dice_4: 2.907  loss_ce_5: 3.065  loss_mask_5: 1.441  loss_dice_5: 2.99  loss_ce_6: 2.898  loss_mask_6: 1.96  loss_dice_6: 2.926  loss_ce_7: 3.054  loss_mask_7: 1.814  loss_dice_7: 2.882  loss_ce_8: 2.997  loss_mask_8: 1.649  loss_dice_8: 2.861    time: 0.4369  last_time: 0.4285  data_time: 0.0234  last_data_time: 0.0207   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 08:53:26 d2.utils.events]: \u001b[0m eta: 1 day, 20:16:40  iter: 2139  total_loss: 82.91  loss_ce: 3.466  loss_mask: 2.105  loss_dice: 2.716  loss_ce_0: 5.501  loss_mask_0: 1.851  loss_dice_0: 2.939  loss_ce_1: 3.382  loss_mask_1: 1.793  loss_dice_1: 2.65  loss_ce_2: 3.305  loss_mask_2: 1.71  loss_dice_2: 2.676  loss_ce_3: 3.29  loss_mask_3: 1.829  loss_dice_3: 2.783  loss_ce_4: 3.321  loss_mask_4: 1.868  loss_dice_4: 2.946  loss_ce_5: 3.276  loss_mask_5: 1.869  loss_dice_5: 2.966  loss_ce_6: 3.298  loss_mask_6: 1.934  loss_dice_6: 2.866  loss_ce_7: 3.306  loss_mask_7: 1.928  loss_dice_7: 2.866  loss_ce_8: 3.365  loss_mask_8: 1.924  loss_dice_8: 2.678    time: 0.4370  last_time: 0.4338  data_time: 0.0244  last_data_time: 0.0243   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 08:53:35 d2.utils.events]: \u001b[0m eta: 1 day, 20:16:16  iter: 2159  total_loss: 80.25  loss_ce: 3.153  loss_mask: 1.8  loss_dice: 2.856  loss_ce_0: 5.499  loss_mask_0: 1.827  loss_dice_0: 2.925  loss_ce_1: 3.055  loss_mask_1: 1.891  loss_dice_1: 2.675  loss_ce_2: 3.195  loss_mask_2: 1.849  loss_dice_2: 2.721  loss_ce_3: 3.148  loss_mask_3: 1.758  loss_dice_3: 2.798  loss_ce_4: 3.083  loss_mask_4: 1.943  loss_dice_4: 2.713  loss_ce_5: 3.116  loss_mask_5: 2.064  loss_dice_5: 2.704  loss_ce_6: 2.957  loss_mask_6: 2.063  loss_dice_6: 2.852  loss_ce_7: 3.046  loss_mask_7: 1.844  loss_dice_7: 2.903  loss_ce_8: 3.02  loss_mask_8: 2.024  loss_dice_8: 2.939    time: 0.4370  last_time: 0.4402  data_time: 0.0233  last_data_time: 0.0242   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 08:53:44 d2.utils.events]: \u001b[0m eta: 1 day, 20:16:38  iter: 2179  total_loss: 84.28  loss_ce: 3.381  loss_mask: 1.658  loss_dice: 2.998  loss_ce_0: 5.668  loss_mask_0: 1.658  loss_dice_0: 3.061  loss_ce_1: 3.252  loss_mask_1: 1.643  loss_dice_1: 2.757  loss_ce_2: 3.236  loss_mask_2: 1.551  loss_dice_2: 2.886  loss_ce_3: 3.292  loss_mask_3: 1.739  loss_dice_3: 2.908  loss_ce_4: 3.215  loss_mask_4: 1.987  loss_dice_4: 3.064  loss_ce_5: 3.203  loss_mask_5: 1.874  loss_dice_5: 3.247  loss_ce_6: 3.117  loss_mask_6: 1.859  loss_dice_6: 3.058  loss_ce_7: 3.399  loss_mask_7: 1.745  loss_dice_7: 2.94  loss_ce_8: 3.31  loss_mask_8: 1.73  loss_dice_8: 2.845    time: 0.4370  last_time: 0.4592  data_time: 0.0239  last_data_time: 0.0198   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 08:53:53 d2.utils.events]: \u001b[0m eta: 1 day, 20:16:43  iter: 2199  total_loss: 84.68  loss_ce: 3.311  loss_mask: 1.879  loss_dice: 3.066  loss_ce_0: 5.598  loss_mask_0: 1.923  loss_dice_0: 3.13  loss_ce_1: 3.225  loss_mask_1: 1.781  loss_dice_1: 2.839  loss_ce_2: 3.229  loss_mask_2: 1.821  loss_dice_2: 2.895  loss_ce_3: 3.3  loss_mask_3: 1.856  loss_dice_3: 2.976  loss_ce_4: 3.355  loss_mask_4: 1.94  loss_dice_4: 3.083  loss_ce_5: 3.132  loss_mask_5: 1.914  loss_dice_5: 3.076  loss_ce_6: 3.135  loss_mask_6: 2.128  loss_dice_6: 3.159  loss_ce_7: 3.225  loss_mask_7: 2.05  loss_dice_7: 3.023  loss_ce_8: 3.189  loss_mask_8: 1.943  loss_dice_8: 3.091    time: 0.4371  last_time: 0.4351  data_time: 0.0251  last_data_time: 0.0235   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 08:54:01 d2.utils.events]: \u001b[0m eta: 1 day, 20:16:34  iter: 2219  total_loss: 84.54  loss_ce: 3.118  loss_mask: 1.828  loss_dice: 2.798  loss_ce_0: 5.608  loss_mask_0: 1.893  loss_dice_0: 2.943  loss_ce_1: 3.102  loss_mask_1: 1.923  loss_dice_1: 2.643  loss_ce_2: 3.066  loss_mask_2: 1.827  loss_dice_2: 2.632  loss_ce_3: 3.193  loss_mask_3: 1.889  loss_dice_3: 2.831  loss_ce_4: 3.038  loss_mask_4: 1.861  loss_dice_4: 2.764  loss_ce_5: 3.032  loss_mask_5: 2.037  loss_dice_5: 2.961  loss_ce_6: 2.967  loss_mask_6: 1.95  loss_dice_6: 2.951  loss_ce_7: 3.016  loss_mask_7: 2.251  loss_dice_7: 2.882  loss_ce_8: 2.957  loss_mask_8: 2.112  loss_dice_8: 2.855    time: 0.4371  last_time: 0.4399  data_time: 0.0236  last_data_time: 0.0276   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 08:54:10 d2.utils.events]: \u001b[0m eta: 1 day, 20:16:40  iter: 2239  total_loss: 78.74  loss_ce: 2.681  loss_mask: 1.886  loss_dice: 2.812  loss_ce_0: 5.241  loss_mask_0: 1.867  loss_dice_0: 2.857  loss_ce_1: 2.537  loss_mask_1: 2.06  loss_dice_1: 2.656  loss_ce_2: 2.626  loss_mask_2: 1.909  loss_dice_2: 2.62  loss_ce_3: 2.709  loss_mask_3: 1.964  loss_dice_3: 2.742  loss_ce_4: 2.744  loss_mask_4: 1.885  loss_dice_4: 2.716  loss_ce_5: 2.527  loss_mask_5: 2.082  loss_dice_5: 2.878  loss_ce_6: 2.658  loss_mask_6: 2.055  loss_dice_6: 2.82  loss_ce_7: 2.747  loss_mask_7: 1.964  loss_dice_7: 2.758  loss_ce_8: 2.686  loss_mask_8: 1.867  loss_dice_8: 2.545    time: 0.4371  last_time: 0.4432  data_time: 0.0216  last_data_time: 0.0231   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 08:54:19 d2.utils.events]: \u001b[0m eta: 1 day, 20:16:31  iter: 2259  total_loss: 77.78  loss_ce: 2.914  loss_mask: 1.789  loss_dice: 2.63  loss_ce_0: 5.384  loss_mask_0: 1.948  loss_dice_0: 2.916  loss_ce_1: 2.832  loss_mask_1: 1.766  loss_dice_1: 2.706  loss_ce_2: 2.989  loss_mask_2: 1.931  loss_dice_2: 2.723  loss_ce_3: 2.849  loss_mask_3: 2.086  loss_dice_3: 2.7  loss_ce_4: 2.836  loss_mask_4: 1.855  loss_dice_4: 2.839  loss_ce_5: 2.784  loss_mask_5: 2.109  loss_dice_5: 2.753  loss_ce_6: 2.853  loss_mask_6: 2.023  loss_dice_6: 2.801  loss_ce_7: 3.006  loss_mask_7: 1.832  loss_dice_7: 2.759  loss_ce_8: 3.021  loss_mask_8: 1.963  loss_dice_8: 2.706    time: 0.4371  last_time: 0.4341  data_time: 0.0234  last_data_time: 0.0244   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 08:54:28 d2.utils.events]: \u001b[0m eta: 1 day, 20:16:32  iter: 2279  total_loss: 84.98  loss_ce: 3.436  loss_mask: 1.836  loss_dice: 2.815  loss_ce_0: 5.392  loss_mask_0: 1.915  loss_dice_0: 3.054  loss_ce_1: 3.43  loss_mask_1: 1.891  loss_dice_1: 2.795  loss_ce_2: 3.448  loss_mask_2: 2  loss_dice_2: 2.954  loss_ce_3: 3.372  loss_mask_3: 2.085  loss_dice_3: 3.065  loss_ce_4: 3.401  loss_mask_4: 2.018  loss_dice_4: 2.971  loss_ce_5: 3.486  loss_mask_5: 2.197  loss_dice_5: 2.932  loss_ce_6: 3.413  loss_mask_6: 1.964  loss_dice_6: 2.952  loss_ce_7: 3.488  loss_mask_7: 1.955  loss_dice_7: 2.847  loss_ce_8: 3.468  loss_mask_8: 1.856  loss_dice_8: 2.926    time: 0.4371  last_time: 0.4387  data_time: 0.0234  last_data_time: 0.0238   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 08:54:36 d2.utils.events]: \u001b[0m eta: 1 day, 20:16:00  iter: 2299  total_loss: 79.95  loss_ce: 3.199  loss_mask: 1.726  loss_dice: 2.911  loss_ce_0: 5.364  loss_mask_0: 1.515  loss_dice_0: 2.995  loss_ce_1: 3.256  loss_mask_1: 1.496  loss_dice_1: 2.713  loss_ce_2: 3.245  loss_mask_2: 1.63  loss_dice_2: 2.71  loss_ce_3: 3.281  loss_mask_3: 1.747  loss_dice_3: 2.787  loss_ce_4: 3.261  loss_mask_4: 1.821  loss_dice_4: 2.897  loss_ce_5: 3.237  loss_mask_5: 1.686  loss_dice_5: 2.679  loss_ce_6: 3.23  loss_mask_6: 1.739  loss_dice_6: 2.903  loss_ce_7: 3.295  loss_mask_7: 1.778  loss_dice_7: 2.767  loss_ce_8: 3.347  loss_mask_8: 1.738  loss_dice_8: 2.881    time: 0.4371  last_time: 0.4322  data_time: 0.0235  last_data_time: 0.0239   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 08:54:45 d2.utils.events]: \u001b[0m eta: 1 day, 20:15:41  iter: 2319  total_loss: 79.91  loss_ce: 2.837  loss_mask: 2.048  loss_dice: 2.778  loss_ce_0: 5.227  loss_mask_0: 1.918  loss_dice_0: 2.824  loss_ce_1: 2.914  loss_mask_1: 2.059  loss_dice_1: 2.628  loss_ce_2: 2.978  loss_mask_2: 1.925  loss_dice_2: 2.594  loss_ce_3: 2.926  loss_mask_3: 2.127  loss_dice_3: 2.739  loss_ce_4: 2.832  loss_mask_4: 2.06  loss_dice_4: 2.845  loss_ce_5: 2.919  loss_mask_5: 1.89  loss_dice_5: 2.757  loss_ce_6: 3.044  loss_mask_6: 1.892  loss_dice_6: 2.86  loss_ce_7: 3.149  loss_mask_7: 1.865  loss_dice_7: 2.574  loss_ce_8: 2.995  loss_mask_8: 1.96  loss_dice_8: 2.753    time: 0.4371  last_time: 0.4362  data_time: 0.0234  last_data_time: 0.0281   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 08:54:54 d2.utils.events]: \u001b[0m eta: 1 day, 20:15:31  iter: 2339  total_loss: 80.66  loss_ce: 3.129  loss_mask: 1.811  loss_dice: 3.085  loss_ce_0: 5.229  loss_mask_0: 1.852  loss_dice_0: 3.212  loss_ce_1: 3.005  loss_mask_1: 1.902  loss_dice_1: 3.099  loss_ce_2: 2.987  loss_mask_2: 1.836  loss_dice_2: 2.935  loss_ce_3: 2.859  loss_mask_3: 1.92  loss_dice_3: 2.96  loss_ce_4: 2.934  loss_mask_4: 1.999  loss_dice_4: 3.136  loss_ce_5: 2.978  loss_mask_5: 1.75  loss_dice_5: 3.034  loss_ce_6: 2.994  loss_mask_6: 1.84  loss_dice_6: 3.116  loss_ce_7: 3.108  loss_mask_7: 1.963  loss_dice_7: 3.07  loss_ce_8: 2.977  loss_mask_8: 1.921  loss_dice_8: 3.2    time: 0.4371  last_time: 0.4302  data_time: 0.0234  last_data_time: 0.0232   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 08:55:03 d2.utils.events]: \u001b[0m eta: 1 day, 20:15:39  iter: 2359  total_loss: 83.05  loss_ce: 2.964  loss_mask: 2.157  loss_dice: 2.836  loss_ce_0: 5.183  loss_mask_0: 1.878  loss_dice_0: 2.841  loss_ce_1: 2.979  loss_mask_1: 2.046  loss_dice_1: 2.946  loss_ce_2: 3.087  loss_mask_2: 1.9  loss_dice_2: 2.835  loss_ce_3: 2.992  loss_mask_3: 2.015  loss_dice_3: 3.009  loss_ce_4: 3.093  loss_mask_4: 2.033  loss_dice_4: 2.921  loss_ce_5: 3.204  loss_mask_5: 1.99  loss_dice_5: 2.952  loss_ce_6: 3.091  loss_mask_6: 1.951  loss_dice_6: 2.944  loss_ce_7: 3.017  loss_mask_7: 2.047  loss_dice_7: 3.015  loss_ce_8: 3.003  loss_mask_8: 2.166  loss_dice_8: 2.874    time: 0.4371  last_time: 0.4384  data_time: 0.0238  last_data_time: 0.0265   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 08:55:11 d2.utils.events]: \u001b[0m eta: 1 day, 20:15:49  iter: 2379  total_loss: 85.42  loss_ce: 3.416  loss_mask: 1.925  loss_dice: 2.684  loss_ce_0: 5.337  loss_mask_0: 1.992  loss_dice_0: 2.783  loss_ce_1: 3.432  loss_mask_1: 1.926  loss_dice_1: 2.657  loss_ce_2: 3.55  loss_mask_2: 1.936  loss_dice_2: 2.721  loss_ce_3: 3.463  loss_mask_3: 2.03  loss_dice_3: 2.83  loss_ce_4: 3.545  loss_mask_4: 1.975  loss_dice_4: 2.889  loss_ce_5: 3.601  loss_mask_5: 2.08  loss_dice_5: 2.751  loss_ce_6: 3.599  loss_mask_6: 1.873  loss_dice_6: 2.795  loss_ce_7: 3.625  loss_mask_7: 1.884  loss_dice_7: 2.771  loss_ce_8: 3.474  loss_mask_8: 2.07  loss_dice_8: 2.689    time: 0.4372  last_time: 0.4399  data_time: 0.0237  last_data_time: 0.0211   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 08:55:20 d2.utils.events]: \u001b[0m eta: 1 day, 20:15:05  iter: 2399  total_loss: 81.31  loss_ce: 3.189  loss_mask: 1.829  loss_dice: 2.83  loss_ce_0: 5.267  loss_mask_0: 1.975  loss_dice_0: 2.86  loss_ce_1: 3.298  loss_mask_1: 1.777  loss_dice_1: 2.544  loss_ce_2: 3.414  loss_mask_2: 1.859  loss_dice_2: 2.712  loss_ce_3: 3.215  loss_mask_3: 1.876  loss_dice_3: 2.626  loss_ce_4: 3.392  loss_mask_4: 1.84  loss_dice_4: 2.763  loss_ce_5: 3.295  loss_mask_5: 2.024  loss_dice_5: 2.605  loss_ce_6: 3.292  loss_mask_6: 1.845  loss_dice_6: 2.684  loss_ce_7: 3.375  loss_mask_7: 1.961  loss_dice_7: 2.949  loss_ce_8: 3.231  loss_mask_8: 1.775  loss_dice_8: 2.861    time: 0.4371  last_time: 0.4250  data_time: 0.0231  last_data_time: 0.0199   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 08:55:29 d2.utils.events]: \u001b[0m eta: 1 day, 20:14:56  iter: 2419  total_loss: 78.42  loss_ce: 2.711  loss_mask: 2.003  loss_dice: 2.753  loss_ce_0: 5.04  loss_mask_0: 1.928  loss_dice_0: 2.904  loss_ce_1: 2.979  loss_mask_1: 1.804  loss_dice_1: 2.653  loss_ce_2: 2.918  loss_mask_2: 2.064  loss_dice_2: 2.694  loss_ce_3: 2.813  loss_mask_3: 2.175  loss_dice_3: 2.698  loss_ce_4: 2.901  loss_mask_4: 2.087  loss_dice_4: 2.924  loss_ce_5: 2.817  loss_mask_5: 2.024  loss_dice_5: 2.837  loss_ce_6: 2.92  loss_mask_6: 2.044  loss_dice_6: 2.667  loss_ce_7: 2.904  loss_mask_7: 1.96  loss_dice_7: 2.954  loss_ce_8: 2.874  loss_mask_8: 2.176  loss_dice_8: 2.866    time: 0.4371  last_time: 0.4359  data_time: 0.0220  last_data_time: 0.0271   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 08:55:38 d2.utils.events]: \u001b[0m eta: 1 day, 20:14:46  iter: 2439  total_loss: 80.31  loss_ce: 3.157  loss_mask: 1.935  loss_dice: 2.668  loss_ce_0: 5.255  loss_mask_0: 1.958  loss_dice_0: 2.876  loss_ce_1: 3.178  loss_mask_1: 1.902  loss_dice_1: 2.688  loss_ce_2: 3.127  loss_mask_2: 2.167  loss_dice_2: 2.75  loss_ce_3: 3.166  loss_mask_3: 2.001  loss_dice_3: 2.723  loss_ce_4: 3.075  loss_mask_4: 2.105  loss_dice_4: 2.714  loss_ce_5: 3.053  loss_mask_5: 2.218  loss_dice_5: 2.984  loss_ce_6: 3.213  loss_mask_6: 1.992  loss_dice_6: 2.763  loss_ce_7: 3.182  loss_mask_7: 1.991  loss_dice_7: 2.88  loss_ce_8: 3.018  loss_mask_8: 2.067  loss_dice_8: 2.828    time: 0.4371  last_time: 0.4269  data_time: 0.0230  last_data_time: 0.0177   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 08:55:46 d2.utils.events]: \u001b[0m eta: 1 day, 20:14:36  iter: 2459  total_loss: 78.79  loss_ce: 3.017  loss_mask: 1.749  loss_dice: 2.795  loss_ce_0: 5.023  loss_mask_0: 1.759  loss_dice_0: 2.807  loss_ce_1: 3.197  loss_mask_1: 1.755  loss_dice_1: 2.772  loss_ce_2: 3.178  loss_mask_2: 1.737  loss_dice_2: 2.797  loss_ce_3: 3.076  loss_mask_3: 1.88  loss_dice_3: 2.785  loss_ce_4: 3.084  loss_mask_4: 1.886  loss_dice_4: 2.855  loss_ce_5: 3.184  loss_mask_5: 1.743  loss_dice_5: 2.767  loss_ce_6: 3.168  loss_mask_6: 1.749  loss_dice_6: 2.846  loss_ce_7: 3.063  loss_mask_7: 1.776  loss_dice_7: 3.024  loss_ce_8: 3.012  loss_mask_8: 1.779  loss_dice_8: 2.695    time: 0.4371  last_time: 0.4337  data_time: 0.0233  last_data_time: 0.0270   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 08:55:55 d2.utils.events]: \u001b[0m eta: 1 day, 20:14:23  iter: 2479  total_loss: 82.87  loss_ce: 3.405  loss_mask: 1.969  loss_dice: 2.597  loss_ce_0: 5.034  loss_mask_0: 1.985  loss_dice_0: 2.85  loss_ce_1: 3.187  loss_mask_1: 1.817  loss_dice_1: 2.471  loss_ce_2: 3.259  loss_mask_2: 1.792  loss_dice_2: 2.571  loss_ce_3: 3.259  loss_mask_3: 1.958  loss_dice_3: 2.629  loss_ce_4: 3.22  loss_mask_4: 1.904  loss_dice_4: 2.765  loss_ce_5: 3.351  loss_mask_5: 2.031  loss_dice_5: 2.609  loss_ce_6: 3.302  loss_mask_6: 2.049  loss_dice_6: 2.766  loss_ce_7: 3.351  loss_mask_7: 1.994  loss_dice_7: 2.544  loss_ce_8: 3.319  loss_mask_8: 1.854  loss_dice_8: 2.737    time: 0.4371  last_time: 0.4280  data_time: 0.0225  last_data_time: 0.0228   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 08:56:04 d2.utils.events]: \u001b[0m eta: 1 day, 20:13:57  iter: 2499  total_loss: 79.92  loss_ce: 3.002  loss_mask: 1.842  loss_dice: 3.003  loss_ce_0: 5.003  loss_mask_0: 1.842  loss_dice_0: 2.943  loss_ce_1: 3.087  loss_mask_1: 1.712  loss_dice_1: 2.725  loss_ce_2: 2.966  loss_mask_2: 1.797  loss_dice_2: 2.927  loss_ce_3: 2.931  loss_mask_3: 1.743  loss_dice_3: 2.744  loss_ce_4: 2.99  loss_mask_4: 1.793  loss_dice_4: 2.906  loss_ce_5: 2.95  loss_mask_5: 1.761  loss_dice_5: 2.945  loss_ce_6: 3.053  loss_mask_6: 1.935  loss_dice_6: 3.084  loss_ce_7: 3  loss_mask_7: 1.658  loss_dice_7: 2.912  loss_ce_8: 3.154  loss_mask_8: 1.836  loss_dice_8: 2.856    time: 0.4370  last_time: 0.4280  data_time: 0.0219  last_data_time: 0.0195   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 08:56:12 d2.utils.events]: \u001b[0m eta: 1 day, 20:13:41  iter: 2519  total_loss: 79.9  loss_ce: 2.976  loss_mask: 1.981  loss_dice: 2.888  loss_ce_0: 5.072  loss_mask_0: 1.808  loss_dice_0: 2.902  loss_ce_1: 3.177  loss_mask_1: 1.712  loss_dice_1: 2.747  loss_ce_2: 3.147  loss_mask_2: 1.826  loss_dice_2: 2.731  loss_ce_3: 3.123  loss_mask_3: 1.718  loss_dice_3: 2.815  loss_ce_4: 3.194  loss_mask_4: 1.821  loss_dice_4: 2.767  loss_ce_5: 3.147  loss_mask_5: 1.861  loss_dice_5: 2.821  loss_ce_6: 3.091  loss_mask_6: 2.132  loss_dice_6: 2.825  loss_ce_7: 3.248  loss_mask_7: 1.775  loss_dice_7: 2.736  loss_ce_8: 3.165  loss_mask_8: 1.916  loss_dice_8: 2.787    time: 0.4370  last_time: 0.4348  data_time: 0.0239  last_data_time: 0.0177   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 08:56:21 d2.utils.events]: \u001b[0m eta: 1 day, 20:13:14  iter: 2539  total_loss: 81.39  loss_ce: 3.098  loss_mask: 1.988  loss_dice: 2.858  loss_ce_0: 4.988  loss_mask_0: 1.87  loss_dice_0: 3.036  loss_ce_1: 3.12  loss_mask_1: 1.819  loss_dice_1: 2.734  loss_ce_2: 3.055  loss_mask_2: 1.763  loss_dice_2: 2.856  loss_ce_3: 3.128  loss_mask_3: 2.06  loss_dice_3: 2.745  loss_ce_4: 3.023  loss_mask_4: 1.898  loss_dice_4: 2.78  loss_ce_5: 3.15  loss_mask_5: 2.019  loss_dice_5: 2.928  loss_ce_6: 3.101  loss_mask_6: 1.879  loss_dice_6: 3.039  loss_ce_7: 3.193  loss_mask_7: 1.907  loss_dice_7: 2.765  loss_ce_8: 3.053  loss_mask_8: 2.048  loss_dice_8: 2.812    time: 0.4370  last_time: 0.4301  data_time: 0.0220  last_data_time: 0.0225   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 08:56:30 d2.utils.events]: \u001b[0m eta: 1 day, 20:12:31  iter: 2559  total_loss: 81.32  loss_ce: 3.067  loss_mask: 1.855  loss_dice: 3.008  loss_ce_0: 4.951  loss_mask_0: 1.976  loss_dice_0: 3.042  loss_ce_1: 2.981  loss_mask_1: 1.834  loss_dice_1: 2.804  loss_ce_2: 3.047  loss_mask_2: 1.914  loss_dice_2: 2.935  loss_ce_3: 3.113  loss_mask_3: 1.835  loss_dice_3: 2.949  loss_ce_4: 2.996  loss_mask_4: 1.927  loss_dice_4: 2.964  loss_ce_5: 2.987  loss_mask_5: 1.964  loss_dice_5: 2.996  loss_ce_6: 2.98  loss_mask_6: 2.104  loss_dice_6: 3.17  loss_ce_7: 3.054  loss_mask_7: 1.934  loss_dice_7: 2.845  loss_ce_8: 3.064  loss_mask_8: 2.003  loss_dice_8: 2.857    time: 0.4370  last_time: 0.4310  data_time: 0.0234  last_data_time: 0.0211   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 08:56:39 d2.utils.events]: \u001b[0m eta: 1 day, 20:12:22  iter: 2579  total_loss: 79.48  loss_ce: 2.981  loss_mask: 1.712  loss_dice: 2.738  loss_ce_0: 4.859  loss_mask_0: 1.739  loss_dice_0: 2.873  loss_ce_1: 2.916  loss_mask_1: 1.721  loss_dice_1: 2.555  loss_ce_2: 2.95  loss_mask_2: 1.843  loss_dice_2: 2.803  loss_ce_3: 2.924  loss_mask_3: 1.642  loss_dice_3: 2.532  loss_ce_4: 2.855  loss_mask_4: 1.705  loss_dice_4: 2.757  loss_ce_5: 2.98  loss_mask_5: 1.709  loss_dice_5: 2.682  loss_ce_6: 2.789  loss_mask_6: 1.918  loss_dice_6: 2.873  loss_ce_7: 2.848  loss_mask_7: 1.768  loss_dice_7: 2.745  loss_ce_8: 2.919  loss_mask_8: 1.634  loss_dice_8: 2.767    time: 0.4369  last_time: 0.4395  data_time: 0.0236  last_data_time: 0.0211   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 08:56:47 d2.utils.events]: \u001b[0m eta: 1 day, 20:12:08  iter: 2599  total_loss: 80.03  loss_ce: 3.023  loss_mask: 1.834  loss_dice: 2.792  loss_ce_0: 4.893  loss_mask_0: 1.885  loss_dice_0: 3.026  loss_ce_1: 3.024  loss_mask_1: 1.717  loss_dice_1: 2.836  loss_ce_2: 2.938  loss_mask_2: 2.073  loss_dice_2: 2.809  loss_ce_3: 2.972  loss_mask_3: 1.827  loss_dice_3: 2.823  loss_ce_4: 3.039  loss_mask_4: 1.987  loss_dice_4: 2.882  loss_ce_5: 2.989  loss_mask_5: 1.766  loss_dice_5: 2.812  loss_ce_6: 2.952  loss_mask_6: 1.983  loss_dice_6: 2.926  loss_ce_7: 2.99  loss_mask_7: 1.689  loss_dice_7: 2.942  loss_ce_8: 3.007  loss_mask_8: 2.04  loss_dice_8: 2.877    time: 0.4369  last_time: 0.4242  data_time: 0.0225  last_data_time: 0.0183   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 08:56:56 d2.utils.events]: \u001b[0m eta: 1 day, 20:11:55  iter: 2619  total_loss: 78.07  loss_ce: 2.969  loss_mask: 1.993  loss_dice: 2.631  loss_ce_0: 4.835  loss_mask_0: 1.898  loss_dice_0: 2.664  loss_ce_1: 2.858  loss_mask_1: 1.865  loss_dice_1: 2.618  loss_ce_2: 2.947  loss_mask_2: 1.866  loss_dice_2: 2.367  loss_ce_3: 2.89  loss_mask_3: 1.95  loss_dice_3: 2.46  loss_ce_4: 2.952  loss_mask_4: 2.03  loss_dice_4: 2.638  loss_ce_5: 2.961  loss_mask_5: 1.937  loss_dice_5: 2.582  loss_ce_6: 2.899  loss_mask_6: 1.887  loss_dice_6: 2.781  loss_ce_7: 2.924  loss_mask_7: 1.908  loss_dice_7: 2.593  loss_ce_8: 2.81  loss_mask_8: 2.242  loss_dice_8: 2.752    time: 0.4369  last_time: 0.4306  data_time: 0.0234  last_data_time: 0.0214   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 08:57:05 d2.utils.events]: \u001b[0m eta: 1 day, 20:11:02  iter: 2639  total_loss: 83.75  loss_ce: 3.385  loss_mask: 2.012  loss_dice: 2.711  loss_ce_0: 5.034  loss_mask_0: 1.953  loss_dice_0: 2.857  loss_ce_1: 3.321  loss_mask_1: 1.959  loss_dice_1: 2.608  loss_ce_2: 3.303  loss_mask_2: 1.918  loss_dice_2: 2.721  loss_ce_3: 3.443  loss_mask_3: 1.931  loss_dice_3: 2.739  loss_ce_4: 3.317  loss_mask_4: 2.006  loss_dice_4: 2.747  loss_ce_5: 3.232  loss_mask_5: 1.912  loss_dice_5: 2.95  loss_ce_6: 3.291  loss_mask_6: 1.955  loss_dice_6: 2.931  loss_ce_7: 3.401  loss_mask_7: 2.083  loss_dice_7: 2.692  loss_ce_8: 3.376  loss_mask_8: 2.056  loss_dice_8: 2.83    time: 0.4369  last_time: 0.4274  data_time: 0.0227  last_data_time: 0.0170   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 08:57:13 d2.utils.events]: \u001b[0m eta: 1 day, 20:10:54  iter: 2659  total_loss: 80.61  loss_ce: 3.078  loss_mask: 2.2  loss_dice: 2.627  loss_ce_0: 4.803  loss_mask_0: 2.129  loss_dice_0: 2.792  loss_ce_1: 2.966  loss_mask_1: 1.942  loss_dice_1: 2.615  loss_ce_2: 2.977  loss_mask_2: 2.104  loss_dice_2: 2.594  loss_ce_3: 3.118  loss_mask_3: 2.352  loss_dice_3: 2.652  loss_ce_4: 3.054  loss_mask_4: 2.035  loss_dice_4: 2.539  loss_ce_5: 2.954  loss_mask_5: 2.35  loss_dice_5: 2.719  loss_ce_6: 3.135  loss_mask_6: 1.926  loss_dice_6: 2.615  loss_ce_7: 2.987  loss_mask_7: 2.198  loss_dice_7: 2.6  loss_ce_8: 2.971  loss_mask_8: 2.244  loss_dice_8: 2.68    time: 0.4369  last_time: 0.4249  data_time: 0.0230  last_data_time: 0.0177   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 08:57:22 d2.utils.events]: \u001b[0m eta: 1 day, 20:10:45  iter: 2679  total_loss: 83.17  loss_ce: 3.462  loss_mask: 1.854  loss_dice: 3  loss_ce_0: 4.896  loss_mask_0: 1.801  loss_dice_0: 2.959  loss_ce_1: 3.374  loss_mask_1: 1.768  loss_dice_1: 2.766  loss_ce_2: 3.426  loss_mask_2: 1.717  loss_dice_2: 2.952  loss_ce_3: 3.426  loss_mask_3: 1.69  loss_dice_3: 2.829  loss_ce_4: 3.44  loss_mask_4: 1.858  loss_dice_4: 2.941  loss_ce_5: 3.275  loss_mask_5: 1.799  loss_dice_5: 2.961  loss_ce_6: 3.232  loss_mask_6: 1.953  loss_dice_6: 3.091  loss_ce_7: 3.298  loss_mask_7: 1.822  loss_dice_7: 3.005  loss_ce_8: 3.35  loss_mask_8: 1.973  loss_dice_8: 3.004    time: 0.4369  last_time: 0.4301  data_time: 0.0229  last_data_time: 0.0225   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 08:57:31 d2.utils.events]: \u001b[0m eta: 1 day, 20:10:22  iter: 2699  total_loss: 78.08  loss_ce: 2.743  loss_mask: 2.179  loss_dice: 2.573  loss_ce_0: 4.576  loss_mask_0: 2.031  loss_dice_0: 2.596  loss_ce_1: 2.868  loss_mask_1: 1.966  loss_dice_1: 2.584  loss_ce_2: 2.68  loss_mask_2: 1.943  loss_dice_2: 2.835  loss_ce_3: 2.617  loss_mask_3: 2.202  loss_dice_3: 2.682  loss_ce_4: 2.83  loss_mask_4: 2.027  loss_dice_4: 2.674  loss_ce_5: 2.845  loss_mask_5: 2.084  loss_dice_5: 2.583  loss_ce_6: 2.735  loss_mask_6: 2.111  loss_dice_6: 2.601  loss_ce_7: 2.697  loss_mask_7: 2.116  loss_dice_7: 2.748  loss_ce_8: 2.517  loss_mask_8: 2.079  loss_dice_8: 2.808    time: 0.4368  last_time: 0.4276  data_time: 0.0221  last_data_time: 0.0178   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 08:57:39 d2.utils.events]: \u001b[0m eta: 1 day, 20:09:32  iter: 2719  total_loss: 77.4  loss_ce: 3.008  loss_mask: 1.756  loss_dice: 2.722  loss_ce_0: 4.613  loss_mask_0: 1.933  loss_dice_0: 2.862  loss_ce_1: 3.034  loss_mask_1: 1.754  loss_dice_1: 2.719  loss_ce_2: 3.001  loss_mask_2: 1.72  loss_dice_2: 2.691  loss_ce_3: 2.992  loss_mask_3: 1.81  loss_dice_3: 2.72  loss_ce_4: 2.999  loss_mask_4: 1.717  loss_dice_4: 2.74  loss_ce_5: 3.159  loss_mask_5: 1.724  loss_dice_5: 2.785  loss_ce_6: 3.094  loss_mask_6: 1.969  loss_dice_6: 2.96  loss_ce_7: 3.113  loss_mask_7: 1.612  loss_dice_7: 2.778  loss_ce_8: 3.133  loss_mask_8: 1.796  loss_dice_8: 2.944    time: 0.4368  last_time: 0.4346  data_time: 0.0223  last_data_time: 0.0242   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 08:57:48 d2.utils.events]: \u001b[0m eta: 1 day, 20:09:16  iter: 2739  total_loss: 84.33  loss_ce: 3.189  loss_mask: 2.198  loss_dice: 2.917  loss_ce_0: 4.636  loss_mask_0: 2.102  loss_dice_0: 2.959  loss_ce_1: 3.213  loss_mask_1: 2.175  loss_dice_1: 2.908  loss_ce_2: 3.164  loss_mask_2: 2.235  loss_dice_2: 2.803  loss_ce_3: 3.173  loss_mask_3: 2.103  loss_dice_3: 3.003  loss_ce_4: 3.2  loss_mask_4: 1.961  loss_dice_4: 2.857  loss_ce_5: 3.206  loss_mask_5: 2.092  loss_dice_5: 2.969  loss_ce_6: 3.286  loss_mask_6: 2.132  loss_dice_6: 2.796  loss_ce_7: 3.169  loss_mask_7: 2.176  loss_dice_7: 2.839  loss_ce_8: 3.138  loss_mask_8: 2.287  loss_dice_8: 3.074    time: 0.4368  last_time: 0.4287  data_time: 0.0224  last_data_time: 0.0201   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 08:57:57 d2.utils.events]: \u001b[0m eta: 1 day, 20:09:08  iter: 2759  total_loss: 79.63  loss_ce: 2.894  loss_mask: 2.114  loss_dice: 2.689  loss_ce_0: 4.747  loss_mask_0: 2.039  loss_dice_0: 2.897  loss_ce_1: 2.865  loss_mask_1: 2.093  loss_dice_1: 2.721  loss_ce_2: 2.88  loss_mask_2: 2.109  loss_dice_2: 2.699  loss_ce_3: 2.895  loss_mask_3: 2.17  loss_dice_3: 2.737  loss_ce_4: 2.975  loss_mask_4: 2.085  loss_dice_4: 2.731  loss_ce_5: 2.936  loss_mask_5: 2.322  loss_dice_5: 2.947  loss_ce_6: 3.052  loss_mask_6: 1.961  loss_dice_6: 2.732  loss_ce_7: 3.022  loss_mask_7: 2.084  loss_dice_7: 2.65  loss_ce_8: 2.877  loss_mask_8: 1.885  loss_dice_8: 2.567    time: 0.4368  last_time: 0.4288  data_time: 0.0224  last_data_time: 0.0211   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 08:58:05 d2.utils.events]: \u001b[0m eta: 1 day, 20:08:43  iter: 2779  total_loss: 80.27  loss_ce: 2.674  loss_mask: 2.344  loss_dice: 2.684  loss_ce_0: 4.629  loss_mask_0: 1.873  loss_dice_0: 2.696  loss_ce_1: 2.932  loss_mask_1: 1.876  loss_dice_1: 2.494  loss_ce_2: 2.8  loss_mask_2: 2.122  loss_dice_2: 2.612  loss_ce_3: 2.825  loss_mask_3: 2.239  loss_dice_3: 2.915  loss_ce_4: 2.819  loss_mask_4: 1.913  loss_dice_4: 2.686  loss_ce_5: 2.814  loss_mask_5: 2.176  loss_dice_5: 2.814  loss_ce_6: 2.824  loss_mask_6: 2.032  loss_dice_6: 2.815  loss_ce_7: 2.939  loss_mask_7: 2.101  loss_dice_7: 2.705  loss_ce_8: 3.032  loss_mask_8: 2.13  loss_dice_8: 2.766    time: 0.4367  last_time: 0.4362  data_time: 0.0221  last_data_time: 0.0215   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 08:58:14 d2.utils.events]: \u001b[0m eta: 1 day, 20:07:49  iter: 2799  total_loss: 77.6  loss_ce: 2.991  loss_mask: 2.054  loss_dice: 2.761  loss_ce_0: 4.645  loss_mask_0: 1.938  loss_dice_0: 2.576  loss_ce_1: 3.039  loss_mask_1: 1.876  loss_dice_1: 2.463  loss_ce_2: 3.04  loss_mask_2: 1.818  loss_dice_2: 2.341  loss_ce_3: 3.037  loss_mask_3: 1.938  loss_dice_3: 2.559  loss_ce_4: 2.965  loss_mask_4: 1.957  loss_dice_4: 2.649  loss_ce_5: 2.947  loss_mask_5: 1.963  loss_dice_5: 2.594  loss_ce_6: 3.026  loss_mask_6: 2.002  loss_dice_6: 2.585  loss_ce_7: 3.013  loss_mask_7: 1.948  loss_dice_7: 2.531  loss_ce_8: 3.052  loss_mask_8: 1.859  loss_dice_8: 2.508    time: 0.4367  last_time: 0.4316  data_time: 0.0220  last_data_time: 0.0222   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 08:58:23 d2.utils.events]: \u001b[0m eta: 1 day, 20:07:54  iter: 2819  total_loss: 84.4  loss_ce: 3.52  loss_mask: 2.085  loss_dice: 2.968  loss_ce_0: 4.757  loss_mask_0: 1.833  loss_dice_0: 2.978  loss_ce_1: 3.58  loss_mask_1: 2.001  loss_dice_1: 2.815  loss_ce_2: 3.597  loss_mask_2: 1.754  loss_dice_2: 2.695  loss_ce_3: 3.582  loss_mask_3: 1.94  loss_dice_3: 2.967  loss_ce_4: 3.504  loss_mask_4: 1.945  loss_dice_4: 2.916  loss_ce_5: 3.681  loss_mask_5: 1.74  loss_dice_5: 2.85  loss_ce_6: 3.511  loss_mask_6: 1.903  loss_dice_6: 2.893  loss_ce_7: 3.466  loss_mask_7: 2.223  loss_dice_7: 3.085  loss_ce_8: 3.623  loss_mask_8: 1.801  loss_dice_8: 2.782    time: 0.4367  last_time: 0.4343  data_time: 0.0230  last_data_time: 0.0254   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 08:58:32 d2.utils.events]: \u001b[0m eta: 1 day, 20:07:24  iter: 2839  total_loss: 77.53  loss_ce: 3.111  loss_mask: 1.772  loss_dice: 2.598  loss_ce_0: 4.564  loss_mask_0: 1.955  loss_dice_0: 2.684  loss_ce_1: 3.104  loss_mask_1: 1.849  loss_dice_1: 2.414  loss_ce_2: 3.141  loss_mask_2: 1.984  loss_dice_2: 2.435  loss_ce_3: 3.147  loss_mask_3: 1.886  loss_dice_3: 2.592  loss_ce_4: 3.126  loss_mask_4: 1.893  loss_dice_4: 2.557  loss_ce_5: 3.236  loss_mask_5: 2.026  loss_dice_5: 2.514  loss_ce_6: 3.147  loss_mask_6: 1.823  loss_dice_6: 2.487  loss_ce_7: 3.235  loss_mask_7: 1.979  loss_dice_7: 2.438  loss_ce_8: 3.059  loss_mask_8: 1.986  loss_dice_8: 2.629    time: 0.4367  last_time: 0.4399  data_time: 0.0228  last_data_time: 0.0261   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 08:58:40 d2.utils.events]: \u001b[0m eta: 1 day, 20:06:34  iter: 2859  total_loss: 77.59  loss_ce: 2.837  loss_mask: 2.05  loss_dice: 2.657  loss_ce_0: 4.448  loss_mask_0: 2.259  loss_dice_0: 2.829  loss_ce_1: 2.797  loss_mask_1: 1.886  loss_dice_1: 2.634  loss_ce_2: 2.855  loss_mask_2: 1.91  loss_dice_2: 2.766  loss_ce_3: 2.937  loss_mask_3: 1.999  loss_dice_3: 2.715  loss_ce_4: 2.901  loss_mask_4: 2.013  loss_dice_4: 2.578  loss_ce_5: 2.893  loss_mask_5: 2.088  loss_dice_5: 2.729  loss_ce_6: 2.918  loss_mask_6: 2.109  loss_dice_6: 2.954  loss_ce_7: 2.814  loss_mask_7: 2.04  loss_dice_7: 2.472  loss_ce_8: 2.759  loss_mask_8: 1.994  loss_dice_8: 2.759    time: 0.4367  last_time: 0.4258  data_time: 0.0212  last_data_time: 0.0206   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 08:58:49 d2.utils.events]: \u001b[0m eta: 1 day, 20:05:59  iter: 2879  total_loss: 77.74  loss_ce: 3.033  loss_mask: 2.093  loss_dice: 2.856  loss_ce_0: 4.514  loss_mask_0: 2.056  loss_dice_0: 2.86  loss_ce_1: 2.983  loss_mask_1: 1.792  loss_dice_1: 2.604  loss_ce_2: 3.07  loss_mask_2: 2.077  loss_dice_2: 2.628  loss_ce_3: 3.069  loss_mask_3: 1.949  loss_dice_3: 2.788  loss_ce_4: 2.961  loss_mask_4: 1.806  loss_dice_4: 2.683  loss_ce_5: 3.086  loss_mask_5: 2.043  loss_dice_5: 2.732  loss_ce_6: 3.015  loss_mask_6: 1.866  loss_dice_6: 2.777  loss_ce_7: 3.151  loss_mask_7: 2.024  loss_dice_7: 2.675  loss_ce_8: 3.246  loss_mask_8: 1.99  loss_dice_8: 2.698    time: 0.4367  last_time: 0.4290  data_time: 0.0224  last_data_time: 0.0236   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 08:58:58 d2.utils.events]: \u001b[0m eta: 1 day, 20:06:01  iter: 2899  total_loss: 81.31  loss_ce: 3.102  loss_mask: 2.07  loss_dice: 2.987  loss_ce_0: 4.667  loss_mask_0: 1.807  loss_dice_0: 3.081  loss_ce_1: 3.163  loss_mask_1: 1.874  loss_dice_1: 2.779  loss_ce_2: 3.195  loss_mask_2: 1.794  loss_dice_2: 2.834  loss_ce_3: 3.092  loss_mask_3: 1.944  loss_dice_3: 2.991  loss_ce_4: 3.318  loss_mask_4: 2.025  loss_dice_4: 2.919  loss_ce_5: 3.263  loss_mask_5: 1.755  loss_dice_5: 3.079  loss_ce_6: 3.225  loss_mask_6: 2.096  loss_dice_6: 2.929  loss_ce_7: 3.274  loss_mask_7: 1.759  loss_dice_7: 2.89  loss_ce_8: 3.308  loss_mask_8: 2.015  loss_dice_8: 2.888    time: 0.4366  last_time: 0.4529  data_time: 0.0237  last_data_time: 0.0245   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 08:59:06 d2.utils.events]: \u001b[0m eta: 1 day, 20:05:35  iter: 2919  total_loss: 77.17  loss_ce: 2.897  loss_mask: 1.759  loss_dice: 2.841  loss_ce_0: 4.446  loss_mask_0: 2.078  loss_dice_0: 2.872  loss_ce_1: 2.996  loss_mask_1: 1.869  loss_dice_1: 2.826  loss_ce_2: 2.999  loss_mask_2: 1.724  loss_dice_2: 2.703  loss_ce_3: 3.028  loss_mask_3: 1.896  loss_dice_3: 2.941  loss_ce_4: 3.027  loss_mask_4: 1.78  loss_dice_4: 2.822  loss_ce_5: 2.95  loss_mask_5: 1.926  loss_dice_5: 2.77  loss_ce_6: 2.987  loss_mask_6: 1.917  loss_dice_6: 2.862  loss_ce_7: 2.935  loss_mask_7: 1.882  loss_dice_7: 2.776  loss_ce_8: 3.03  loss_mask_8: 1.949  loss_dice_8: 2.817    time: 0.4366  last_time: 0.4402  data_time: 0.0226  last_data_time: 0.0299   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 08:59:15 d2.utils.events]: \u001b[0m eta: 1 day, 20:05:23  iter: 2939  total_loss: 79  loss_ce: 2.893  loss_mask: 1.717  loss_dice: 2.531  loss_ce_0: 4.456  loss_mask_0: 1.737  loss_dice_0: 2.884  loss_ce_1: 2.824  loss_mask_1: 1.71  loss_dice_1: 2.401  loss_ce_2: 2.918  loss_mask_2: 1.841  loss_dice_2: 2.82  loss_ce_3: 2.837  loss_mask_3: 1.739  loss_dice_3: 2.674  loss_ce_4: 2.946  loss_mask_4: 1.825  loss_dice_4: 2.586  loss_ce_5: 2.837  loss_mask_5: 1.731  loss_dice_5: 2.614  loss_ce_6: 3.008  loss_mask_6: 1.743  loss_dice_6: 2.711  loss_ce_7: 2.979  loss_mask_7: 1.757  loss_dice_7: 2.703  loss_ce_8: 2.953  loss_mask_8: 1.837  loss_dice_8: 2.603    time: 0.4366  last_time: 0.4308  data_time: 0.0222  last_data_time: 0.0236   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 08:59:24 d2.utils.events]: \u001b[0m eta: 1 day, 20:04:58  iter: 2959  total_loss: 80.48  loss_ce: 3.181  loss_mask: 2.01  loss_dice: 2.932  loss_ce_0: 4.532  loss_mask_0: 1.847  loss_dice_0: 2.898  loss_ce_1: 3.085  loss_mask_1: 1.82  loss_dice_1: 2.665  loss_ce_2: 3.197  loss_mask_2: 2.046  loss_dice_2: 2.79  loss_ce_3: 3.182  loss_mask_3: 1.991  loss_dice_3: 2.847  loss_ce_4: 3.129  loss_mask_4: 1.896  loss_dice_4: 2.81  loss_ce_5: 3.065  loss_mask_5: 1.833  loss_dice_5: 2.709  loss_ce_6: 3.317  loss_mask_6: 2.102  loss_dice_6: 2.864  loss_ce_7: 3.17  loss_mask_7: 1.865  loss_dice_7: 2.88  loss_ce_8: 3.215  loss_mask_8: 1.861  loss_dice_8: 2.802    time: 0.4366  last_time: 0.4291  data_time: 0.0226  last_data_time: 0.0182   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 08:59:33 d2.utils.events]: \u001b[0m eta: 1 day, 20:05:02  iter: 2979  total_loss: 82.59  loss_ce: 3.55  loss_mask: 1.822  loss_dice: 2.709  loss_ce_0: 4.56  loss_mask_0: 1.81  loss_dice_0: 2.791  loss_ce_1: 3.402  loss_mask_1: 1.819  loss_dice_1: 2.738  loss_ce_2: 3.506  loss_mask_2: 1.733  loss_dice_2: 2.715  loss_ce_3: 3.474  loss_mask_3: 2.003  loss_dice_3: 2.977  loss_ce_4: 3.555  loss_mask_4: 1.848  loss_dice_4: 2.856  loss_ce_5: 3.444  loss_mask_5: 1.833  loss_dice_5: 2.767  loss_ce_6: 3.496  loss_mask_6: 1.729  loss_dice_6: 2.744  loss_ce_7: 3.527  loss_mask_7: 1.963  loss_dice_7: 2.884  loss_ce_8: 3.505  loss_mask_8: 1.688  loss_dice_8: 2.77    time: 0.4366  last_time: 0.4367  data_time: 0.0239  last_data_time: 0.0237   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 08:59:41 d2.utils.events]: \u001b[0m eta: 1 day, 20:04:46  iter: 2999  total_loss: 80.51  loss_ce: 3.343  loss_mask: 1.823  loss_dice: 2.702  loss_ce_0: 4.588  loss_mask_0: 1.73  loss_dice_0: 2.879  loss_ce_1: 3.254  loss_mask_1: 1.743  loss_dice_1: 2.746  loss_ce_2: 3.431  loss_mask_2: 1.725  loss_dice_2: 2.802  loss_ce_3: 3.271  loss_mask_3: 1.911  loss_dice_3: 2.752  loss_ce_4: 3.217  loss_mask_4: 1.812  loss_dice_4: 2.701  loss_ce_5: 3.261  loss_mask_5: 1.859  loss_dice_5: 2.769  loss_ce_6: 3.157  loss_mask_6: 2.054  loss_dice_6: 2.824  loss_ce_7: 3.384  loss_mask_7: 1.882  loss_dice_7: 2.837  loss_ce_8: 3.356  loss_mask_8: 1.872  loss_dice_8: 2.887    time: 0.4366  last_time: 0.4293  data_time: 0.0223  last_data_time: 0.0209   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 08:59:50 d2.utils.events]: \u001b[0m eta: 1 day, 20:04:16  iter: 3019  total_loss: 80.24  loss_ce: 2.767  loss_mask: 1.886  loss_dice: 2.882  loss_ce_0: 4.287  loss_mask_0: 1.744  loss_dice_0: 2.981  loss_ce_1: 2.912  loss_mask_1: 1.872  loss_dice_1: 2.856  loss_ce_2: 3.007  loss_mask_2: 1.757  loss_dice_2: 2.699  loss_ce_3: 2.783  loss_mask_3: 1.946  loss_dice_3: 2.875  loss_ce_4: 2.873  loss_mask_4: 1.986  loss_dice_4: 2.96  loss_ce_5: 2.896  loss_mask_5: 1.848  loss_dice_5: 2.82  loss_ce_6: 2.5  loss_mask_6: 2.105  loss_dice_6: 3.28  loss_ce_7: 2.92  loss_mask_7: 1.911  loss_dice_7: 2.947  loss_ce_8: 2.957  loss_mask_8: 1.791  loss_dice_8: 2.785    time: 0.4366  last_time: 0.4280  data_time: 0.0228  last_data_time: 0.0208   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 08:59:59 d2.utils.events]: \u001b[0m eta: 1 day, 20:04:01  iter: 3039  total_loss: 79.44  loss_ce: 2.799  loss_mask: 2.073  loss_dice: 2.807  loss_ce_0: 4.298  loss_mask_0: 1.989  loss_dice_0: 2.888  loss_ce_1: 2.764  loss_mask_1: 1.968  loss_dice_1: 2.763  loss_ce_2: 2.807  loss_mask_2: 1.857  loss_dice_2: 2.704  loss_ce_3: 2.825  loss_mask_3: 1.984  loss_dice_3: 2.688  loss_ce_4: 2.852  loss_mask_4: 2.028  loss_dice_4: 3.02  loss_ce_5: 2.853  loss_mask_5: 1.921  loss_dice_5: 2.866  loss_ce_6: 2.67  loss_mask_6: 2.261  loss_dice_6: 3.021  loss_ce_7: 2.728  loss_mask_7: 1.925  loss_dice_7: 2.906  loss_ce_8: 2.774  loss_mask_8: 1.918  loss_dice_8: 2.803    time: 0.4366  last_time: 0.4392  data_time: 0.0220  last_data_time: 0.0260   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 09:00:07 d2.utils.events]: \u001b[0m eta: 1 day, 20:03:40  iter: 3059  total_loss: 81.71  loss_ce: 2.775  loss_mask: 1.93  loss_dice: 2.744  loss_ce_0: 4.161  loss_mask_0: 2.012  loss_dice_0: 2.886  loss_ce_1: 2.677  loss_mask_1: 1.878  loss_dice_1: 2.642  loss_ce_2: 2.735  loss_mask_2: 1.971  loss_dice_2: 2.772  loss_ce_3: 2.832  loss_mask_3: 1.904  loss_dice_3: 2.763  loss_ce_4: 2.767  loss_mask_4: 2.081  loss_dice_4: 2.816  loss_ce_5: 2.712  loss_mask_5: 2.097  loss_dice_5: 2.857  loss_ce_6: 2.728  loss_mask_6: 2.212  loss_dice_6: 2.9  loss_ce_7: 2.745  loss_mask_7: 2.042  loss_dice_7: 2.849  loss_ce_8: 2.78  loss_mask_8: 1.957  loss_dice_8: 2.794    time: 0.4366  last_time: 0.4265  data_time: 0.0219  last_data_time: 0.0204   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 09:00:16 d2.utils.events]: \u001b[0m eta: 1 day, 20:03:25  iter: 3079  total_loss: 75.41  loss_ce: 2.787  loss_mask: 1.917  loss_dice: 2.721  loss_ce_0: 4.146  loss_mask_0: 2.041  loss_dice_0: 2.744  loss_ce_1: 2.8  loss_mask_1: 2.036  loss_dice_1: 2.62  loss_ce_2: 2.741  loss_mask_2: 2.007  loss_dice_2: 2.622  loss_ce_3: 2.635  loss_mask_3: 2.117  loss_dice_3: 2.721  loss_ce_4: 2.749  loss_mask_4: 1.958  loss_dice_4: 2.756  loss_ce_5: 2.828  loss_mask_5: 2.069  loss_dice_5: 2.674  loss_ce_6: 2.731  loss_mask_6: 2.192  loss_dice_6: 2.881  loss_ce_7: 2.743  loss_mask_7: 2.133  loss_dice_7: 2.702  loss_ce_8: 2.838  loss_mask_8: 1.926  loss_dice_8: 2.708    time: 0.4365  last_time: 0.4367  data_time: 0.0225  last_data_time: 0.0251   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 09:00:25 d2.utils.events]: \u001b[0m eta: 1 day, 20:02:56  iter: 3099  total_loss: 77.18  loss_ce: 2.735  loss_mask: 1.61  loss_dice: 3.044  loss_ce_0: 4.143  loss_mask_0: 1.734  loss_dice_0: 3.038  loss_ce_1: 2.664  loss_mask_1: 1.64  loss_dice_1: 2.767  loss_ce_2: 2.816  loss_mask_2: 1.602  loss_dice_2: 2.887  loss_ce_3: 2.682  loss_mask_3: 1.901  loss_dice_3: 2.969  loss_ce_4: 2.641  loss_mask_4: 1.677  loss_dice_4: 3.003  loss_ce_5: 2.758  loss_mask_5: 1.643  loss_dice_5: 2.939  loss_ce_6: 2.737  loss_mask_6: 1.713  loss_dice_6: 3.109  loss_ce_7: 2.643  loss_mask_7: 1.74  loss_dice_7: 3.241  loss_ce_8: 2.687  loss_mask_8: 1.771  loss_dice_8: 3.121    time: 0.4365  last_time: 0.4320  data_time: 0.0222  last_data_time: 0.0195   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 09:00:33 d2.utils.events]: \u001b[0m eta: 1 day, 20:02:35  iter: 3119  total_loss: 83.85  loss_ce: 3.484  loss_mask: 1.927  loss_dice: 2.916  loss_ce_0: 4.442  loss_mask_0: 2.009  loss_dice_0: 2.944  loss_ce_1: 3.401  loss_mask_1: 2.005  loss_dice_1: 2.821  loss_ce_2: 3.491  loss_mask_2: 2.01  loss_dice_2: 2.734  loss_ce_3: 3.315  loss_mask_3: 2.006  loss_dice_3: 2.898  loss_ce_4: 3.369  loss_mask_4: 1.852  loss_dice_4: 2.828  loss_ce_5: 3.408  loss_mask_5: 1.917  loss_dice_5: 2.942  loss_ce_6: 3.444  loss_mask_6: 1.962  loss_dice_6: 3.016  loss_ce_7: 3.395  loss_mask_7: 1.997  loss_dice_7: 2.889  loss_ce_8: 3.323  loss_mask_8: 2.076  loss_dice_8: 2.973    time: 0.4365  last_time: 0.4419  data_time: 0.0232  last_data_time: 0.0224   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 09:00:42 d2.utils.events]: \u001b[0m eta: 1 day, 20:02:04  iter: 3139  total_loss: 79.71  loss_ce: 2.777  loss_mask: 2.082  loss_dice: 2.701  loss_ce_0: 4.175  loss_mask_0: 2.019  loss_dice_0: 2.618  loss_ce_1: 2.885  loss_mask_1: 2.036  loss_dice_1: 2.59  loss_ce_2: 2.998  loss_mask_2: 1.97  loss_dice_2: 2.447  loss_ce_3: 2.86  loss_mask_3: 2.039  loss_dice_3: 2.691  loss_ce_4: 2.885  loss_mask_4: 2.042  loss_dice_4: 2.651  loss_ce_5: 3.075  loss_mask_5: 2.121  loss_dice_5: 2.724  loss_ce_6: 2.867  loss_mask_6: 2.097  loss_dice_6: 2.722  loss_ce_7: 2.977  loss_mask_7: 2.144  loss_dice_7: 2.76  loss_ce_8: 2.955  loss_mask_8: 2.074  loss_dice_8: 2.602    time: 0.4365  last_time: 0.4269  data_time: 0.0229  last_data_time: 0.0196   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 09:00:51 d2.utils.events]: \u001b[0m eta: 1 day, 20:01:50  iter: 3159  total_loss: 79.06  loss_ce: 3.271  loss_mask: 1.896  loss_dice: 2.894  loss_ce_0: 4.279  loss_mask_0: 1.812  loss_dice_0: 2.843  loss_ce_1: 3.239  loss_mask_1: 1.812  loss_dice_1: 2.567  loss_ce_2: 3.25  loss_mask_2: 1.857  loss_dice_2: 2.656  loss_ce_3: 3.35  loss_mask_3: 1.72  loss_dice_3: 2.789  loss_ce_4: 3.226  loss_mask_4: 1.935  loss_dice_4: 2.695  loss_ce_5: 3.372  loss_mask_5: 1.706  loss_dice_5: 2.763  loss_ce_6: 3.099  loss_mask_6: 2.149  loss_dice_6: 3.047  loss_ce_7: 3.104  loss_mask_7: 1.95  loss_dice_7: 3.001  loss_ce_8: 3.339  loss_mask_8: 1.788  loss_dice_8: 2.763    time: 0.4365  last_time: 0.4301  data_time: 0.0225  last_data_time: 0.0236   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 09:01:00 d2.utils.events]: \u001b[0m eta: 1 day, 20:01:29  iter: 3179  total_loss: 84.79  loss_ce: 3.594  loss_mask: 1.947  loss_dice: 2.855  loss_ce_0: 4.529  loss_mask_0: 1.915  loss_dice_0: 2.9  loss_ce_1: 3.542  loss_mask_1: 1.869  loss_dice_1: 2.805  loss_ce_2: 3.621  loss_mask_2: 1.775  loss_dice_2: 2.736  loss_ce_3: 3.629  loss_mask_3: 2.068  loss_dice_3: 2.741  loss_ce_4: 3.632  loss_mask_4: 1.968  loss_dice_4: 2.827  loss_ce_5: 3.633  loss_mask_5: 1.953  loss_dice_5: 2.927  loss_ce_6: 3.61  loss_mask_6: 2.169  loss_dice_6: 2.939  loss_ce_7: 3.554  loss_mask_7: 2.137  loss_dice_7: 3.06  loss_ce_8: 3.704  loss_mask_8: 1.9  loss_dice_8: 2.989    time: 0.4365  last_time: 0.4426  data_time: 0.0238  last_data_time: 0.0261   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 09:01:08 d2.utils.events]: \u001b[0m eta: 1 day, 20:01:07  iter: 3199  total_loss: 83.87  loss_ce: 3.594  loss_mask: 1.748  loss_dice: 2.962  loss_ce_0: 4.586  loss_mask_0: 1.807  loss_dice_0: 3.015  loss_ce_1: 3.449  loss_mask_1: 1.731  loss_dice_1: 2.94  loss_ce_2: 3.463  loss_mask_2: 1.824  loss_dice_2: 2.878  loss_ce_3: 3.337  loss_mask_3: 1.81  loss_dice_3: 2.918  loss_ce_4: 3.402  loss_mask_4: 1.768  loss_dice_4: 2.919  loss_ce_5: 3.557  loss_mask_5: 1.757  loss_dice_5: 2.898  loss_ce_6: 3.423  loss_mask_6: 1.891  loss_dice_6: 2.834  loss_ce_7: 3.617  loss_mask_7: 1.773  loss_dice_7: 3.037  loss_ce_8: 3.477  loss_mask_8: 1.818  loss_dice_8: 2.972    time: 0.4365  last_time: 0.4307  data_time: 0.0240  last_data_time: 0.0227   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 09:01:17 d2.utils.events]: \u001b[0m eta: 1 day, 20:01:11  iter: 3219  total_loss: 86.43  loss_ce: 3.754  loss_mask: 1.843  loss_dice: 2.909  loss_ce_0: 4.523  loss_mask_0: 1.947  loss_dice_0: 2.85  loss_ce_1: 3.777  loss_mask_1: 1.725  loss_dice_1: 2.676  loss_ce_2: 3.746  loss_mask_2: 1.904  loss_dice_2: 2.806  loss_ce_3: 3.781  loss_mask_3: 1.791  loss_dice_3: 2.847  loss_ce_4: 3.678  loss_mask_4: 1.889  loss_dice_4: 2.784  loss_ce_5: 3.668  loss_mask_5: 2.177  loss_dice_5: 2.9  loss_ce_6: 3.857  loss_mask_6: 1.951  loss_dice_6: 2.934  loss_ce_7: 3.812  loss_mask_7: 2.032  loss_dice_7: 2.86  loss_ce_8: 3.774  loss_mask_8: 1.916  loss_dice_8: 3.004    time: 0.4365  last_time: 0.4334  data_time: 0.0262  last_data_time: 0.0232   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 09:01:26 d2.utils.events]: \u001b[0m eta: 1 day, 20:01:00  iter: 3239  total_loss: 79.47  loss_ce: 3.196  loss_mask: 1.979  loss_dice: 2.877  loss_ce_0: 4.338  loss_mask_0: 2.058  loss_dice_0: 2.835  loss_ce_1: 3.082  loss_mask_1: 1.9  loss_dice_1: 2.607  loss_ce_2: 3.265  loss_mask_2: 2.058  loss_dice_2: 2.64  loss_ce_3: 3.245  loss_mask_3: 1.873  loss_dice_3: 2.51  loss_ce_4: 3.243  loss_mask_4: 1.924  loss_dice_4: 2.674  loss_ce_5: 3.21  loss_mask_5: 2.071  loss_dice_5: 2.644  loss_ce_6: 3.206  loss_mask_6: 1.936  loss_dice_6: 2.68  loss_ce_7: 3.357  loss_mask_7: 2.076  loss_dice_7: 2.597  loss_ce_8: 3.308  loss_mask_8: 2.02  loss_dice_8: 2.688    time: 0.4365  last_time: 0.4277  data_time: 0.0232  last_data_time: 0.0194   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 09:01:35 d2.utils.events]: \u001b[0m eta: 1 day, 20:00:51  iter: 3259  total_loss: 80.69  loss_ce: 3.264  loss_mask: 1.788  loss_dice: 2.829  loss_ce_0: 4.246  loss_mask_0: 1.733  loss_dice_0: 2.977  loss_ce_1: 3.164  loss_mask_1: 1.796  loss_dice_1: 2.64  loss_ce_2: 3.158  loss_mask_2: 1.855  loss_dice_2: 2.682  loss_ce_3: 3.153  loss_mask_3: 1.796  loss_dice_3: 2.712  loss_ce_4: 3.21  loss_mask_4: 1.927  loss_dice_4: 2.913  loss_ce_5: 3.162  loss_mask_5: 2.066  loss_dice_5: 2.876  loss_ce_6: 3.201  loss_mask_6: 1.828  loss_dice_6: 2.895  loss_ce_7: 3.243  loss_mask_7: 1.697  loss_dice_7: 2.835  loss_ce_8: 3.164  loss_mask_8: 1.792  loss_dice_8: 2.864    time: 0.4365  last_time: 0.4483  data_time: 0.0231  last_data_time: 0.0213   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 09:01:43 d2.utils.events]: \u001b[0m eta: 1 day, 20:00:21  iter: 3279  total_loss: 82.01  loss_ce: 3.297  loss_mask: 2.069  loss_dice: 2.796  loss_ce_0: 4.223  loss_mask_0: 1.89  loss_dice_0: 2.893  loss_ce_1: 3.448  loss_mask_1: 1.887  loss_dice_1: 2.768  loss_ce_2: 3.127  loss_mask_2: 2.085  loss_dice_2: 3.01  loss_ce_3: 3.089  loss_mask_3: 1.995  loss_dice_3: 2.848  loss_ce_4: 3.251  loss_mask_4: 1.847  loss_dice_4: 2.776  loss_ce_5: 3.256  loss_mask_5: 1.89  loss_dice_5: 2.843  loss_ce_6: 3.315  loss_mask_6: 1.959  loss_dice_6: 2.831  loss_ce_7: 3.354  loss_mask_7: 1.818  loss_dice_7: 2.996  loss_ce_8: 3.311  loss_mask_8: 1.803  loss_dice_8: 2.956    time: 0.4365  last_time: 0.4318  data_time: 0.0235  last_data_time: 0.0241   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 09:01:52 d2.utils.events]: \u001b[0m eta: 1 day, 20:00:07  iter: 3299  total_loss: 78.31  loss_ce: 3.094  loss_mask: 1.855  loss_dice: 2.672  loss_ce_0: 4.107  loss_mask_0: 1.871  loss_dice_0: 2.665  loss_ce_1: 2.997  loss_mask_1: 1.823  loss_dice_1: 2.752  loss_ce_2: 2.949  loss_mask_2: 1.971  loss_dice_2: 2.68  loss_ce_3: 2.931  loss_mask_3: 2.143  loss_dice_3: 2.889  loss_ce_4: 2.907  loss_mask_4: 2.071  loss_dice_4: 2.836  loss_ce_5: 2.838  loss_mask_5: 2.219  loss_dice_5: 2.883  loss_ce_6: 3.041  loss_mask_6: 2.005  loss_dice_6: 2.89  loss_ce_7: 2.992  loss_mask_7: 1.968  loss_dice_7: 2.859  loss_ce_8: 3.058  loss_mask_8: 1.901  loss_dice_8: 2.701    time: 0.4365  last_time: 0.4291  data_time: 0.0224  last_data_time: 0.0232   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 09:02:01 d2.utils.events]: \u001b[0m eta: 1 day, 20:00:01  iter: 3319  total_loss: 81.04  loss_ce: 3.373  loss_mask: 1.72  loss_dice: 2.537  loss_ce_0: 4.241  loss_mask_0: 1.878  loss_dice_0: 2.856  loss_ce_1: 3.201  loss_mask_1: 1.822  loss_dice_1: 2.656  loss_ce_2: 3.305  loss_mask_2: 1.731  loss_dice_2: 2.684  loss_ce_3: 3.188  loss_mask_3: 1.819  loss_dice_3: 2.651  loss_ce_4: 3.266  loss_mask_4: 1.79  loss_dice_4: 2.737  loss_ce_5: 3.31  loss_mask_5: 1.922  loss_dice_5: 2.757  loss_ce_6: 3.272  loss_mask_6: 1.731  loss_dice_6: 2.582  loss_ce_7: 3.112  loss_mask_7: 2.101  loss_dice_7: 2.905  loss_ce_8: 3.348  loss_mask_8: 1.947  loss_dice_8: 2.887    time: 0.4365  last_time: 0.4517  data_time: 0.0239  last_data_time: 0.0430   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 09:02:10 d2.utils.events]: \u001b[0m eta: 1 day, 19:59:46  iter: 3339  total_loss: 78.2  loss_ce: 3.379  loss_mask: 1.798  loss_dice: 2.47  loss_ce_0: 4.196  loss_mask_0: 1.932  loss_dice_0: 2.7  loss_ce_1: 3.239  loss_mask_1: 1.849  loss_dice_1: 2.609  loss_ce_2: 3.429  loss_mask_2: 1.971  loss_dice_2: 2.512  loss_ce_3: 3.464  loss_mask_3: 1.876  loss_dice_3: 2.377  loss_ce_4: 3.41  loss_mask_4: 1.921  loss_dice_4: 2.497  loss_ce_5: 3.379  loss_mask_5: 2.122  loss_dice_5: 2.501  loss_ce_6: 3.439  loss_mask_6: 2.024  loss_dice_6: 2.546  loss_ce_7: 3.354  loss_mask_7: 1.976  loss_dice_7: 2.598  loss_ce_8: 3.28  loss_mask_8: 1.904  loss_dice_8: 2.704    time: 0.4365  last_time: 0.4414  data_time: 0.0235  last_data_time: 0.0257   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 09:02:18 d2.utils.events]: \u001b[0m eta: 1 day, 19:59:38  iter: 3359  total_loss: 83.75  loss_ce: 3.552  loss_mask: 1.988  loss_dice: 2.83  loss_ce_0: 4.253  loss_mask_0: 1.795  loss_dice_0: 2.968  loss_ce_1: 3.396  loss_mask_1: 1.821  loss_dice_1: 2.814  loss_ce_2: 3.566  loss_mask_2: 1.711  loss_dice_2: 2.787  loss_ce_3: 3.58  loss_mask_3: 1.748  loss_dice_3: 2.903  loss_ce_4: 3.543  loss_mask_4: 1.693  loss_dice_4: 2.935  loss_ce_5: 3.579  loss_mask_5: 1.882  loss_dice_5: 2.867  loss_ce_6: 3.56  loss_mask_6: 1.851  loss_dice_6: 2.745  loss_ce_7: 3.359  loss_mask_7: 1.745  loss_dice_7: 2.895  loss_ce_8: 3.502  loss_mask_8: 1.736  loss_dice_8: 2.871    time: 0.4365  last_time: 0.4277  data_time: 0.0237  last_data_time: 0.0220   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 09:02:27 d2.utils.events]: \u001b[0m eta: 1 day, 19:59:11  iter: 3379  total_loss: 80.13  loss_ce: 3.049  loss_mask: 1.958  loss_dice: 2.867  loss_ce_0: 4.06  loss_mask_0: 1.896  loss_dice_0: 2.851  loss_ce_1: 3.075  loss_mask_1: 1.943  loss_dice_1: 2.656  loss_ce_2: 3.181  loss_mask_2: 1.887  loss_dice_2: 2.853  loss_ce_3: 3.195  loss_mask_3: 1.977  loss_dice_3: 2.755  loss_ce_4: 3.134  loss_mask_4: 1.926  loss_dice_4: 2.898  loss_ce_5: 3.166  loss_mask_5: 1.942  loss_dice_5: 2.765  loss_ce_6: 3.137  loss_mask_6: 1.935  loss_dice_6: 2.777  loss_ce_7: 3.161  loss_mask_7: 1.868  loss_dice_7: 2.882  loss_ce_8: 3.141  loss_mask_8: 2.066  loss_dice_8: 2.837    time: 0.4365  last_time: 0.4264  data_time: 0.0227  last_data_time: 0.0205   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 09:02:36 d2.utils.events]: \u001b[0m eta: 1 day, 19:58:03  iter: 3399  total_loss: 83.3  loss_ce: 3.342  loss_mask: 1.962  loss_dice: 2.775  loss_ce_0: 4.146  loss_mask_0: 1.826  loss_dice_0: 2.842  loss_ce_1: 3.265  loss_mask_1: 1.905  loss_dice_1: 2.714  loss_ce_2: 3.369  loss_mask_2: 1.934  loss_dice_2: 2.756  loss_ce_3: 3.333  loss_mask_3: 2.013  loss_dice_3: 2.651  loss_ce_4: 3.356  loss_mask_4: 2.189  loss_dice_4: 2.766  loss_ce_5: 3.388  loss_mask_5: 1.988  loss_dice_5: 2.777  loss_ce_6: 3.421  loss_mask_6: 2.083  loss_dice_6: 2.797  loss_ce_7: 3.457  loss_mask_7: 1.864  loss_dice_7: 2.844  loss_ce_8: 3.426  loss_mask_8: 1.848  loss_dice_8: 2.761    time: 0.4365  last_time: 0.4397  data_time: 0.0238  last_data_time: 0.0248   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 09:02:44 d2.utils.events]: \u001b[0m eta: 1 day, 19:58:56  iter: 3419  total_loss: 82.82  loss_ce: 3.678  loss_mask: 2.053  loss_dice: 2.598  loss_ce_0: 4.333  loss_mask_0: 2.047  loss_dice_0: 2.757  loss_ce_1: 3.467  loss_mask_1: 2.017  loss_dice_1: 2.46  loss_ce_2: 3.541  loss_mask_2: 1.993  loss_dice_2: 2.554  loss_ce_3: 3.593  loss_mask_3: 2.085  loss_dice_3: 2.448  loss_ce_4: 3.55  loss_mask_4: 2.099  loss_dice_4: 2.462  loss_ce_5: 3.617  loss_mask_5: 1.953  loss_dice_5: 2.501  loss_ce_6: 3.443  loss_mask_6: 2.226  loss_dice_6: 2.647  loss_ce_7: 3.53  loss_mask_7: 2.222  loss_dice_7: 2.656  loss_ce_8: 3.688  loss_mask_8: 2.091  loss_dice_8: 2.507    time: 0.4365  last_time: 0.4432  data_time: 0.0237  last_data_time: 0.0237   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 09:02:53 d2.utils.events]: \u001b[0m eta: 1 day, 19:58:56  iter: 3439  total_loss: 83.72  loss_ce: 3.497  loss_mask: 1.884  loss_dice: 2.746  loss_ce_0: 4.096  loss_mask_0: 1.86  loss_dice_0: 2.995  loss_ce_1: 3.476  loss_mask_1: 1.896  loss_dice_1: 2.788  loss_ce_2: 3.365  loss_mask_2: 1.857  loss_dice_2: 2.734  loss_ce_3: 3.449  loss_mask_3: 1.9  loss_dice_3: 2.873  loss_ce_4: 3.361  loss_mask_4: 1.986  loss_dice_4: 3.118  loss_ce_5: 3.5  loss_mask_5: 1.76  loss_dice_5: 2.974  loss_ce_6: 3.256  loss_mask_6: 2.131  loss_dice_6: 2.997  loss_ce_7: 3.499  loss_mask_7: 1.962  loss_dice_7: 3.105  loss_ce_8: 3.459  loss_mask_8: 1.849  loss_dice_8: 2.895    time: 0.4365  last_time: 0.4299  data_time: 0.0237  last_data_time: 0.0240   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 09:03:02 d2.utils.events]: \u001b[0m eta: 1 day, 19:58:48  iter: 3459  total_loss: 85.63  loss_ce: 3.414  loss_mask: 1.869  loss_dice: 2.889  loss_ce_0: 4.183  loss_mask_0: 1.894  loss_dice_0: 2.854  loss_ce_1: 3.384  loss_mask_1: 1.886  loss_dice_1: 2.674  loss_ce_2: 3.518  loss_mask_2: 1.882  loss_dice_2: 2.852  loss_ce_3: 3.443  loss_mask_3: 1.85  loss_dice_3: 2.786  loss_ce_4: 3.482  loss_mask_4: 2.002  loss_dice_4: 2.857  loss_ce_5: 3.532  loss_mask_5: 1.919  loss_dice_5: 2.867  loss_ce_6: 3.411  loss_mask_6: 1.934  loss_dice_6: 2.895  loss_ce_7: 3.617  loss_mask_7: 1.895  loss_dice_7: 2.796  loss_ce_8: 3.488  loss_mask_8: 2.094  loss_dice_8: 2.84    time: 0.4365  last_time: 0.4253  data_time: 0.0241  last_data_time: 0.0200   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 09:03:11 d2.utils.events]: \u001b[0m eta: 1 day, 19:58:35  iter: 3479  total_loss: 79.19  loss_ce: 3.398  loss_mask: 1.818  loss_dice: 2.776  loss_ce_0: 4.066  loss_mask_0: 1.797  loss_dice_0: 2.858  loss_ce_1: 3.204  loss_mask_1: 1.999  loss_dice_1: 2.729  loss_ce_2: 3.506  loss_mask_2: 1.748  loss_dice_2: 2.633  loss_ce_3: 3.387  loss_mask_3: 1.836  loss_dice_3: 2.728  loss_ce_4: 3.361  loss_mask_4: 1.894  loss_dice_4: 2.691  loss_ce_5: 3.535  loss_mask_5: 1.916  loss_dice_5: 2.867  loss_ce_6: 3.397  loss_mask_6: 1.912  loss_dice_6: 2.685  loss_ce_7: 3.377  loss_mask_7: 1.762  loss_dice_7: 2.907  loss_ce_8: 3.407  loss_mask_8: 1.756  loss_dice_8: 2.833    time: 0.4365  last_time: 0.4332  data_time: 0.0230  last_data_time: 0.0224   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 09:03:19 d2.utils.events]: \u001b[0m eta: 1 day, 19:58:42  iter: 3499  total_loss: 83.47  loss_ce: 3.335  loss_mask: 1.932  loss_dice: 2.967  loss_ce_0: 4.135  loss_mask_0: 1.85  loss_dice_0: 2.954  loss_ce_1: 3.33  loss_mask_1: 1.789  loss_dice_1: 2.839  loss_ce_2: 3.478  loss_mask_2: 1.835  loss_dice_2: 2.864  loss_ce_3: 3.455  loss_mask_3: 1.917  loss_dice_3: 2.904  loss_ce_4: 3.358  loss_mask_4: 1.914  loss_dice_4: 3.005  loss_ce_5: 3.397  loss_mask_5: 2.098  loss_dice_5: 2.971  loss_ce_6: 3.462  loss_mask_6: 1.908  loss_dice_6: 2.941  loss_ce_7: 3.386  loss_mask_7: 1.925  loss_dice_7: 3.096  loss_ce_8: 3.439  loss_mask_8: 2.117  loss_dice_8: 3.042    time: 0.4364  last_time: 0.4305  data_time: 0.0231  last_data_time: 0.0248   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 09:03:28 d2.utils.events]: \u001b[0m eta: 1 day, 19:58:33  iter: 3519  total_loss: 79.75  loss_ce: 3.361  loss_mask: 1.863  loss_dice: 2.824  loss_ce_0: 4.1  loss_mask_0: 1.803  loss_dice_0: 2.756  loss_ce_1: 3.163  loss_mask_1: 1.788  loss_dice_1: 2.612  loss_ce_2: 3.294  loss_mask_2: 1.836  loss_dice_2: 2.764  loss_ce_3: 3.323  loss_mask_3: 1.906  loss_dice_3: 2.726  loss_ce_4: 3.299  loss_mask_4: 2.03  loss_dice_4: 2.678  loss_ce_5: 3.309  loss_mask_5: 2.107  loss_dice_5: 2.829  loss_ce_6: 3.334  loss_mask_6: 2.116  loss_dice_6: 2.817  loss_ce_7: 3.322  loss_mask_7: 1.929  loss_dice_7: 2.756  loss_ce_8: 3.281  loss_mask_8: 2.079  loss_dice_8: 2.893    time: 0.4364  last_time: 0.4390  data_time: 0.0231  last_data_time: 0.0228   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 09:03:37 d2.utils.events]: \u001b[0m eta: 1 day, 19:58:40  iter: 3539  total_loss: 83.43  loss_ce: 3.529  loss_mask: 1.852  loss_dice: 2.86  loss_ce_0: 4.188  loss_mask_0: 1.858  loss_dice_0: 3.055  loss_ce_1: 3.261  loss_mask_1: 1.831  loss_dice_1: 2.749  loss_ce_2: 3.362  loss_mask_2: 1.86  loss_dice_2: 2.82  loss_ce_3: 3.372  loss_mask_3: 1.791  loss_dice_3: 2.842  loss_ce_4: 3.329  loss_mask_4: 1.893  loss_dice_4: 2.909  loss_ce_5: 3.214  loss_mask_5: 2.102  loss_dice_5: 3.029  loss_ce_6: 3.381  loss_mask_6: 2.12  loss_dice_6: 3.029  loss_ce_7: 3.488  loss_mask_7: 1.897  loss_dice_7: 3.059  loss_ce_8: 3.524  loss_mask_8: 1.921  loss_dice_8: 2.982    time: 0.4364  last_time: 0.4338  data_time: 0.0238  last_data_time: 0.0215   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 09:03:45 d2.utils.events]: \u001b[0m eta: 1 day, 19:58:33  iter: 3559  total_loss: 79.66  loss_ce: 3.319  loss_mask: 2.097  loss_dice: 2.621  loss_ce_0: 3.948  loss_mask_0: 2.066  loss_dice_0: 2.631  loss_ce_1: 3.289  loss_mask_1: 1.826  loss_dice_1: 2.475  loss_ce_2: 3.42  loss_mask_2: 1.999  loss_dice_2: 2.411  loss_ce_3: 3.472  loss_mask_3: 1.975  loss_dice_3: 2.405  loss_ce_4: 3.397  loss_mask_4: 1.877  loss_dice_4: 2.522  loss_ce_5: 3.04  loss_mask_5: 2.183  loss_dice_5: 2.764  loss_ce_6: 3.374  loss_mask_6: 2.18  loss_dice_6: 2.527  loss_ce_7: 3.423  loss_mask_7: 2.079  loss_dice_7: 2.61  loss_ce_8: 3.466  loss_mask_8: 2.13  loss_dice_8: 2.582    time: 0.4364  last_time: 0.4413  data_time: 0.0227  last_data_time: 0.0260   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 09:03:54 d2.utils.events]: \u001b[0m eta: 1 day, 19:58:24  iter: 3579  total_loss: 79.96  loss_ce: 3.227  loss_mask: 1.889  loss_dice: 2.77  loss_ce_0: 4.197  loss_mask_0: 1.928  loss_dice_0: 2.879  loss_ce_1: 3.242  loss_mask_1: 1.718  loss_dice_1: 2.665  loss_ce_2: 3.302  loss_mask_2: 1.692  loss_dice_2: 2.765  loss_ce_3: 3.355  loss_mask_3: 1.794  loss_dice_3: 2.7  loss_ce_4: 3.438  loss_mask_4: 1.769  loss_dice_4: 2.719  loss_ce_5: 3.51  loss_mask_5: 2.052  loss_dice_5: 2.943  loss_ce_6: 3.365  loss_mask_6: 1.792  loss_dice_6: 2.762  loss_ce_7: 3.318  loss_mask_7: 1.712  loss_dice_7: 2.824  loss_ce_8: 3.217  loss_mask_8: 1.921  loss_dice_8: 2.672    time: 0.4364  last_time: 0.4348  data_time: 0.0234  last_data_time: 0.0259   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 09:04:03 d2.utils.events]: \u001b[0m eta: 1 day, 19:58:22  iter: 3599  total_loss: 81.02  loss_ce: 3.329  loss_mask: 2.075  loss_dice: 2.772  loss_ce_0: 3.941  loss_mask_0: 1.873  loss_dice_0: 2.813  loss_ce_1: 3.285  loss_mask_1: 1.747  loss_dice_1: 2.665  loss_ce_2: 3.251  loss_mask_2: 1.761  loss_dice_2: 2.736  loss_ce_3: 3.312  loss_mask_3: 1.699  loss_dice_3: 2.552  loss_ce_4: 3.302  loss_mask_4: 1.885  loss_dice_4: 2.803  loss_ce_5: 3.344  loss_mask_5: 1.962  loss_dice_5: 2.883  loss_ce_6: 3.447  loss_mask_6: 2.032  loss_dice_6: 2.837  loss_ce_7: 3.303  loss_mask_7: 1.873  loss_dice_7: 2.851  loss_ce_8: 3.406  loss_mask_8: 1.902  loss_dice_8: 2.783    time: 0.4364  last_time: 0.4329  data_time: 0.0226  last_data_time: 0.0255   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 09:04:12 d2.utils.events]: \u001b[0m eta: 1 day, 19:57:57  iter: 3619  total_loss: 81.3  loss_ce: 3.351  loss_mask: 1.944  loss_dice: 2.686  loss_ce_0: 3.998  loss_mask_0: 2.039  loss_dice_0: 2.816  loss_ce_1: 3.245  loss_mask_1: 1.86  loss_dice_1: 2.602  loss_ce_2: 3.111  loss_mask_2: 2.027  loss_dice_2: 2.683  loss_ce_3: 3.278  loss_mask_3: 1.99  loss_dice_3: 2.659  loss_ce_4: 3.304  loss_mask_4: 1.779  loss_dice_4: 2.68  loss_ce_5: 3.378  loss_mask_5: 1.951  loss_dice_5: 2.837  loss_ce_6: 3.306  loss_mask_6: 1.955  loss_dice_6: 2.767  loss_ce_7: 3.289  loss_mask_7: 1.945  loss_dice_7: 2.726  loss_ce_8: 3.301  loss_mask_8: 1.89  loss_dice_8: 2.727    time: 0.4364  last_time: 0.4292  data_time: 0.0226  last_data_time: 0.0201   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 09:04:20 d2.utils.events]: \u001b[0m eta: 1 day, 19:57:48  iter: 3639  total_loss: 78.92  loss_ce: 3.128  loss_mask: 2.03  loss_dice: 2.603  loss_ce_0: 3.842  loss_mask_0: 1.911  loss_dice_0: 2.784  loss_ce_1: 3.262  loss_mask_1: 1.903  loss_dice_1: 2.615  loss_ce_2: 3.056  loss_mask_2: 2.051  loss_dice_2: 2.606  loss_ce_3: 3.159  loss_mask_3: 1.966  loss_dice_3: 2.448  loss_ce_4: 3.289  loss_mask_4: 1.81  loss_dice_4: 2.492  loss_ce_5: 3.263  loss_mask_5: 2.05  loss_dice_5: 2.852  loss_ce_6: 3.127  loss_mask_6: 1.889  loss_dice_6: 2.73  loss_ce_7: 3.259  loss_mask_7: 2.097  loss_dice_7: 2.714  loss_ce_8: 3.201  loss_mask_8: 1.871  loss_dice_8: 2.739    time: 0.4364  last_time: 0.4298  data_time: 0.0233  last_data_time: 0.0247   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 09:04:29 d2.utils.events]: \u001b[0m eta: 1 day, 19:57:37  iter: 3659  total_loss: 77.73  loss_ce: 2.973  loss_mask: 1.964  loss_dice: 2.827  loss_ce_0: 3.897  loss_mask_0: 1.768  loss_dice_0: 2.842  loss_ce_1: 2.97  loss_mask_1: 1.74  loss_dice_1: 2.763  loss_ce_2: 2.896  loss_mask_2: 1.808  loss_dice_2: 2.712  loss_ce_3: 3.093  loss_mask_3: 1.764  loss_dice_3: 2.747  loss_ce_4: 3.062  loss_mask_4: 1.693  loss_dice_4: 2.827  loss_ce_5: 3.063  loss_mask_5: 1.803  loss_dice_5: 2.856  loss_ce_6: 3.03  loss_mask_6: 1.921  loss_dice_6: 2.881  loss_ce_7: 3.181  loss_mask_7: 1.791  loss_dice_7: 2.792  loss_ce_8: 3.125  loss_mask_8: 1.827  loss_dice_8: 2.824    time: 0.4364  last_time: 0.4267  data_time: 0.0236  last_data_time: 0.0183   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 09:04:38 d2.utils.events]: \u001b[0m eta: 1 day, 19:57:28  iter: 3679  total_loss: 82.03  loss_ce: 3.415  loss_mask: 1.669  loss_dice: 2.744  loss_ce_0: 4.091  loss_mask_0: 1.602  loss_dice_0: 2.845  loss_ce_1: 3.509  loss_mask_1: 1.583  loss_dice_1: 2.755  loss_ce_2: 3.405  loss_mask_2: 1.603  loss_dice_2: 2.816  loss_ce_3: 3.423  loss_mask_3: 1.692  loss_dice_3: 2.799  loss_ce_4: 3.405  loss_mask_4: 1.541  loss_dice_4: 2.717  loss_ce_5: 3.393  loss_mask_5: 1.824  loss_dice_5: 3.033  loss_ce_6: 3.524  loss_mask_6: 1.458  loss_dice_6: 2.862  loss_ce_7: 3.507  loss_mask_7: 1.722  loss_dice_7: 2.897  loss_ce_8: 3.629  loss_mask_8: 1.589  loss_dice_8: 2.839    time: 0.4364  last_time: 0.4278  data_time: 0.0236  last_data_time: 0.0213   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 09:04:46 d2.utils.events]: \u001b[0m eta: 1 day, 19:57:32  iter: 3699  total_loss: 76.8  loss_ce: 3.077  loss_mask: 1.852  loss_dice: 2.617  loss_ce_0: 3.884  loss_mask_0: 1.806  loss_dice_0: 2.679  loss_ce_1: 3.045  loss_mask_1: 1.66  loss_dice_1: 2.55  loss_ce_2: 3.109  loss_mask_2: 1.689  loss_dice_2: 2.564  loss_ce_3: 3.137  loss_mask_3: 1.885  loss_dice_3: 2.588  loss_ce_4: 3.177  loss_mask_4: 1.625  loss_dice_4: 2.451  loss_ce_5: 3.054  loss_mask_5: 1.909  loss_dice_5: 2.7  loss_ce_6: 3.012  loss_mask_6: 1.879  loss_dice_6: 2.653  loss_ce_7: 3.093  loss_mask_7: 1.925  loss_dice_7: 2.775  loss_ce_8: 3.073  loss_mask_8: 1.912  loss_dice_8: 2.612    time: 0.4364  last_time: 0.4381  data_time: 0.0237  last_data_time: 0.0227   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 09:04:55 d2.utils.events]: \u001b[0m eta: 1 day, 19:57:47  iter: 3719  total_loss: 82.36  loss_ce: 3.472  loss_mask: 2.13  loss_dice: 2.75  loss_ce_0: 4.166  loss_mask_0: 1.909  loss_dice_0: 2.658  loss_ce_1: 3.513  loss_mask_1: 1.726  loss_dice_1: 2.479  loss_ce_2: 3.6  loss_mask_2: 1.901  loss_dice_2: 2.458  loss_ce_3: 3.581  loss_mask_3: 2.005  loss_dice_3: 2.713  loss_ce_4: 3.555  loss_mask_4: 1.839  loss_dice_4: 2.61  loss_ce_5: 3.592  loss_mask_5: 1.852  loss_dice_5: 2.58  loss_ce_6: 3.572  loss_mask_6: 2.012  loss_dice_6: 2.693  loss_ce_7: 3.599  loss_mask_7: 1.923  loss_dice_7: 2.659  loss_ce_8: 3.584  loss_mask_8: 1.861  loss_dice_8: 2.597    time: 0.4364  last_time: 0.4238  data_time: 0.0231  last_data_time: 0.0180   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 09:05:04 d2.utils.events]: \u001b[0m eta: 1 day, 19:57:44  iter: 3739  total_loss: 78.35  loss_ce: 3.071  loss_mask: 2.142  loss_dice: 2.814  loss_ce_0: 3.872  loss_mask_0: 1.912  loss_dice_0: 2.694  loss_ce_1: 3.125  loss_mask_1: 1.971  loss_dice_1: 2.555  loss_ce_2: 3.144  loss_mask_2: 2.014  loss_dice_2: 2.761  loss_ce_3: 3.201  loss_mask_3: 2.036  loss_dice_3: 2.763  loss_ce_4: 3.155  loss_mask_4: 1.963  loss_dice_4: 2.759  loss_ce_5: 3.168  loss_mask_5: 2.012  loss_dice_5: 2.785  loss_ce_6: 3.263  loss_mask_6: 2.051  loss_dice_6: 2.83  loss_ce_7: 3.101  loss_mask_7: 2.066  loss_dice_7: 2.752  loss_ce_8: 3.142  loss_mask_8: 1.961  loss_dice_8: 2.653    time: 0.4364  last_time: 0.5464  data_time: 0.0229  last_data_time: 0.0268   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 09:05:13 d2.utils.events]: \u001b[0m eta: 1 day, 19:57:35  iter: 3759  total_loss: 81.83  loss_ce: 2.932  loss_mask: 2.001  loss_dice: 2.978  loss_ce_0: 3.687  loss_mask_0: 2.063  loss_dice_0: 2.978  loss_ce_1: 2.745  loss_mask_1: 2.118  loss_dice_1: 2.938  loss_ce_2: 2.861  loss_mask_2: 1.883  loss_dice_2: 2.89  loss_ce_3: 2.94  loss_mask_3: 1.956  loss_dice_3: 2.937  loss_ce_4: 2.804  loss_mask_4: 2.097  loss_dice_4: 3.076  loss_ce_5: 2.876  loss_mask_5: 2.088  loss_dice_5: 2.984  loss_ce_6: 2.854  loss_mask_6: 1.954  loss_dice_6: 2.926  loss_ce_7: 3.136  loss_mask_7: 1.993  loss_dice_7: 3.027  loss_ce_8: 2.908  loss_mask_8: 1.942  loss_dice_8: 2.914    time: 0.4364  last_time: 0.4359  data_time: 0.0220  last_data_time: 0.0224   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 09:05:21 d2.utils.events]: \u001b[0m eta: 1 day, 19:58:00  iter: 3779  total_loss: 82.53  loss_ce: 3.581  loss_mask: 2.004  loss_dice: 2.811  loss_ce_0: 4.031  loss_mask_0: 1.873  loss_dice_0: 2.97  loss_ce_1: 3.513  loss_mask_1: 1.974  loss_dice_1: 2.736  loss_ce_2: 3.502  loss_mask_2: 1.95  loss_dice_2: 2.736  loss_ce_3: 3.591  loss_mask_3: 1.841  loss_dice_3: 2.824  loss_ce_4: 3.486  loss_mask_4: 1.889  loss_dice_4: 2.87  loss_ce_5: 3.477  loss_mask_5: 2.05  loss_dice_5: 2.928  loss_ce_6: 3.621  loss_mask_6: 2.138  loss_dice_6: 2.899  loss_ce_7: 3.534  loss_mask_7: 2.056  loss_dice_7: 3.077  loss_ce_8: 3.505  loss_mask_8: 2.167  loss_dice_8: 2.948    time: 0.4364  last_time: 0.4581  data_time: 0.0249  last_data_time: 0.0429   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 09:05:30 d2.utils.events]: \u001b[0m eta: 1 day, 19:58:18  iter: 3799  total_loss: 82  loss_ce: 3.322  loss_mask: 1.744  loss_dice: 2.945  loss_ce_0: 3.992  loss_mask_0: 1.773  loss_dice_0: 2.953  loss_ce_1: 3.377  loss_mask_1: 1.657  loss_dice_1: 2.668  loss_ce_2: 3.369  loss_mask_2: 1.702  loss_dice_2: 2.771  loss_ce_3: 3.468  loss_mask_3: 1.877  loss_dice_3: 2.931  loss_ce_4: 3.459  loss_mask_4: 1.741  loss_dice_4: 2.756  loss_ce_5: 3.443  loss_mask_5: 1.875  loss_dice_5: 2.979  loss_ce_6: 3.427  loss_mask_6: 1.806  loss_dice_6: 2.886  loss_ce_7: 3.485  loss_mask_7: 1.741  loss_dice_7: 2.96  loss_ce_8: 3.357  loss_mask_8: 1.839  loss_dice_8: 2.893    time: 0.4364  last_time: 0.4337  data_time: 0.0232  last_data_time: 0.0240   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 09:05:39 d2.utils.events]: \u001b[0m eta: 1 day, 19:57:59  iter: 3819  total_loss: 81.37  loss_ce: 2.796  loss_mask: 2.218  loss_dice: 2.794  loss_ce_0: 3.574  loss_mask_0: 1.794  loss_dice_0: 2.77  loss_ce_1: 2.673  loss_mask_1: 2.016  loss_dice_1: 2.654  loss_ce_2: 2.642  loss_mask_2: 2.249  loss_dice_2: 2.681  loss_ce_3: 2.656  loss_mask_3: 1.987  loss_dice_3: 2.721  loss_ce_4: 2.763  loss_mask_4: 1.821  loss_dice_4: 2.695  loss_ce_5: 2.716  loss_mask_5: 1.905  loss_dice_5: 2.713  loss_ce_6: 2.542  loss_mask_6: 1.974  loss_dice_6: 2.803  loss_ce_7: 2.649  loss_mask_7: 1.939  loss_dice_7: 2.856  loss_ce_8: 2.597  loss_mask_8: 2.224  loss_dice_8: 2.833    time: 0.4364  last_time: 0.4338  data_time: 0.0230  last_data_time: 0.0254   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 09:05:48 d2.utils.events]: \u001b[0m eta: 1 day, 19:57:50  iter: 3839  total_loss: 76.03  loss_ce: 2.976  loss_mask: 1.984  loss_dice: 2.53  loss_ce_0: 3.657  loss_mask_0: 1.959  loss_dice_0: 2.79  loss_ce_1: 2.968  loss_mask_1: 1.931  loss_dice_1: 2.551  loss_ce_2: 2.937  loss_mask_2: 2.099  loss_dice_2: 2.551  loss_ce_3: 3.087  loss_mask_3: 2.063  loss_dice_3: 2.541  loss_ce_4: 2.999  loss_mask_4: 2.101  loss_dice_4: 2.582  loss_ce_5: 2.985  loss_mask_5: 2.159  loss_dice_5: 2.453  loss_ce_6: 2.982  loss_mask_6: 2.144  loss_dice_6: 2.64  loss_ce_7: 3.043  loss_mask_7: 2.037  loss_dice_7: 2.848  loss_ce_8: 3.242  loss_mask_8: 2.041  loss_dice_8: 2.678    time: 0.4364  last_time: 0.4348  data_time: 0.0216  last_data_time: 0.0233   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 09:05:56 d2.utils.events]: \u001b[0m eta: 1 day, 19:57:59  iter: 3859  total_loss: 80.33  loss_ce: 3.33  loss_mask: 1.915  loss_dice: 2.71  loss_ce_0: 3.815  loss_mask_0: 1.923  loss_dice_0: 2.848  loss_ce_1: 3.204  loss_mask_1: 1.844  loss_dice_1: 2.604  loss_ce_2: 3.275  loss_mask_2: 1.981  loss_dice_2: 2.621  loss_ce_3: 3.293  loss_mask_3: 1.831  loss_dice_3: 2.797  loss_ce_4: 3.247  loss_mask_4: 1.998  loss_dice_4: 2.63  loss_ce_5: 3.159  loss_mask_5: 1.935  loss_dice_5: 2.767  loss_ce_6: 3.263  loss_mask_6: 1.927  loss_dice_6: 2.702  loss_ce_7: 3.298  loss_mask_7: 1.808  loss_dice_7: 2.847  loss_ce_8: 3.231  loss_mask_8: 1.978  loss_dice_8: 2.812    time: 0.4364  last_time: 0.4363  data_time: 0.0227  last_data_time: 0.0255   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 09:06:05 d2.utils.events]: \u001b[0m eta: 1 day, 19:57:50  iter: 3879  total_loss: 77.81  loss_ce: 3.109  loss_mask: 1.9  loss_dice: 2.801  loss_ce_0: 3.746  loss_mask_0: 1.845  loss_dice_0: 2.882  loss_ce_1: 3.044  loss_mask_1: 1.861  loss_dice_1: 2.731  loss_ce_2: 2.986  loss_mask_2: 1.963  loss_dice_2: 2.561  loss_ce_3: 3.152  loss_mask_3: 1.879  loss_dice_3: 2.687  loss_ce_4: 3.075  loss_mask_4: 1.862  loss_dice_4: 2.682  loss_ce_5: 3.093  loss_mask_5: 1.835  loss_dice_5: 2.786  loss_ce_6: 3.149  loss_mask_6: 1.908  loss_dice_6: 2.831  loss_ce_7: 3.011  loss_mask_7: 1.977  loss_dice_7: 2.839  loss_ce_8: 3.033  loss_mask_8: 1.867  loss_dice_8: 2.849    time: 0.4364  last_time: 0.4350  data_time: 0.0226  last_data_time: 0.0269   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 09:06:14 d2.utils.events]: \u001b[0m eta: 1 day, 19:57:43  iter: 3899  total_loss: 74.66  loss_ce: 2.693  loss_mask: 2.019  loss_dice: 2.781  loss_ce_0: 3.346  loss_mask_0: 1.91  loss_dice_0: 2.753  loss_ce_1: 2.604  loss_mask_1: 1.881  loss_dice_1: 2.651  loss_ce_2: 2.603  loss_mask_2: 1.673  loss_dice_2: 2.573  loss_ce_3: 2.706  loss_mask_3: 1.787  loss_dice_3: 2.663  loss_ce_4: 2.646  loss_mask_4: 1.993  loss_dice_4: 2.791  loss_ce_5: 2.712  loss_mask_5: 1.88  loss_dice_5: 2.54  loss_ce_6: 2.683  loss_mask_6: 2.027  loss_dice_6: 2.872  loss_ce_7: 2.693  loss_mask_7: 2.088  loss_dice_7: 2.708  loss_ce_8: 2.609  loss_mask_8: 1.971  loss_dice_8: 2.671    time: 0.4363  last_time: 0.4677  data_time: 0.0223  last_data_time: 0.0276   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 09:06:22 d2.utils.events]: \u001b[0m eta: 1 day, 19:57:38  iter: 3919  total_loss: 80.56  loss_ce: 2.957  loss_mask: 1.913  loss_dice: 2.674  loss_ce_0: 3.666  loss_mask_0: 1.797  loss_dice_0: 2.824  loss_ce_1: 3.046  loss_mask_1: 1.747  loss_dice_1: 2.437  loss_ce_2: 2.972  loss_mask_2: 1.813  loss_dice_2: 2.491  loss_ce_3: 2.984  loss_mask_3: 2.015  loss_dice_3: 2.789  loss_ce_4: 3.026  loss_mask_4: 1.752  loss_dice_4: 2.62  loss_ce_5: 3.025  loss_mask_5: 1.817  loss_dice_5: 2.669  loss_ce_6: 3.121  loss_mask_6: 2.003  loss_dice_6: 2.621  loss_ce_7: 3.043  loss_mask_7: 1.806  loss_dice_7: 2.671  loss_ce_8: 2.916  loss_mask_8: 1.878  loss_dice_8: 2.811    time: 0.4363  last_time: 0.4498  data_time: 0.0226  last_data_time: 0.0273   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 09:06:31 d2.utils.events]: \u001b[0m eta: 1 day, 19:57:32  iter: 3939  total_loss: 77.56  loss_ce: 3.148  loss_mask: 1.763  loss_dice: 2.663  loss_ce_0: 3.719  loss_mask_0: 1.941  loss_dice_0: 2.788  loss_ce_1: 3.044  loss_mask_1: 1.745  loss_dice_1: 2.609  loss_ce_2: 3.122  loss_mask_2: 1.8  loss_dice_2: 2.677  loss_ce_3: 3.098  loss_mask_3: 1.922  loss_dice_3: 2.677  loss_ce_4: 3.198  loss_mask_4: 1.814  loss_dice_4: 2.66  loss_ce_5: 3.119  loss_mask_5: 1.845  loss_dice_5: 2.541  loss_ce_6: 3.16  loss_mask_6: 1.835  loss_dice_6: 2.579  loss_ce_7: 3.19  loss_mask_7: 1.825  loss_dice_7: 2.882  loss_ce_8: 3.238  loss_mask_8: 1.755  loss_dice_8: 2.643    time: 0.4363  last_time: 0.4276  data_time: 0.0232  last_data_time: 0.0186   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 09:06:40 d2.utils.events]: \u001b[0m eta: 1 day, 19:57:17  iter: 3959  total_loss: 73.42  loss_ce: 2.579  loss_mask: 1.804  loss_dice: 2.392  loss_ce_0: 3.357  loss_mask_0: 2.026  loss_dice_0: 2.641  loss_ce_1: 2.594  loss_mask_1: 1.929  loss_dice_1: 2.49  loss_ce_2: 2.556  loss_mask_2: 1.913  loss_dice_2: 2.32  loss_ce_3: 2.564  loss_mask_3: 1.948  loss_dice_3: 2.477  loss_ce_4: 2.572  loss_mask_4: 2.047  loss_dice_4: 2.372  loss_ce_5: 2.538  loss_mask_5: 2.051  loss_dice_5: 2.689  loss_ce_6: 2.597  loss_mask_6: 2.11  loss_dice_6: 2.64  loss_ce_7: 2.587  loss_mask_7: 1.837  loss_dice_7: 2.439  loss_ce_8: 2.645  loss_mask_8: 2.027  loss_dice_8: 2.52    time: 0.4363  last_time: 0.4317  data_time: 0.0225  last_data_time: 0.0216   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 09:06:49 d2.utils.events]: \u001b[0m eta: 1 day, 19:56:52  iter: 3979  total_loss: 80.51  loss_ce: 3.32  loss_mask: 1.751  loss_dice: 2.841  loss_ce_0: 3.817  loss_mask_0: 1.769  loss_dice_0: 2.813  loss_ce_1: 3.246  loss_mask_1: 1.701  loss_dice_1: 2.649  loss_ce_2: 3.394  loss_mask_2: 1.722  loss_dice_2: 2.691  loss_ce_3: 3.298  loss_mask_3: 1.932  loss_dice_3: 2.666  loss_ce_4: 3.393  loss_mask_4: 1.867  loss_dice_4: 2.768  loss_ce_5: 3.304  loss_mask_5: 1.891  loss_dice_5: 2.811  loss_ce_6: 3.348  loss_mask_6: 1.897  loss_dice_6: 2.784  loss_ce_7: 3.386  loss_mask_7: 1.91  loss_dice_7: 2.958  loss_ce_8: 3.429  loss_mask_8: 1.888  loss_dice_8: 2.857    time: 0.4363  last_time: 0.4387  data_time: 0.0232  last_data_time: 0.0210   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 09:06:57 d2.utils.events]: \u001b[0m eta: 1 day, 19:56:41  iter: 3999  total_loss: 77.95  loss_ce: 3.018  loss_mask: 1.811  loss_dice: 2.695  loss_ce_0: 3.605  loss_mask_0: 1.877  loss_dice_0: 2.977  loss_ce_1: 2.992  loss_mask_1: 1.846  loss_dice_1: 2.695  loss_ce_2: 3.211  loss_mask_2: 1.806  loss_dice_2: 2.71  loss_ce_3: 2.902  loss_mask_3: 1.861  loss_dice_3: 2.72  loss_ce_4: 3.005  loss_mask_4: 1.95  loss_dice_4: 2.649  loss_ce_5: 2.997  loss_mask_5: 1.957  loss_dice_5: 3.048  loss_ce_6: 3.09  loss_mask_6: 1.995  loss_dice_6: 2.792  loss_ce_7: 3.051  loss_mask_7: 1.856  loss_dice_7: 2.778  loss_ce_8: 3.052  loss_mask_8: 1.796  loss_dice_8: 2.773    time: 0.4363  last_time: 0.4286  data_time: 0.0224  last_data_time: 0.0222   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 09:07:06 d2.utils.events]: \u001b[0m eta: 1 day, 19:56:32  iter: 4019  total_loss: 81.74  loss_ce: 3.263  loss_mask: 1.807  loss_dice: 3.107  loss_ce_0: 3.626  loss_mask_0: 1.696  loss_dice_0: 3.037  loss_ce_1: 3.292  loss_mask_1: 1.726  loss_dice_1: 2.76  loss_ce_2: 3.253  loss_mask_2: 1.962  loss_dice_2: 2.938  loss_ce_3: 3.069  loss_mask_3: 1.944  loss_dice_3: 2.961  loss_ce_4: 3.234  loss_mask_4: 1.843  loss_dice_4: 2.994  loss_ce_5: 3.366  loss_mask_5: 1.844  loss_dice_5: 3.124  loss_ce_6: 3.37  loss_mask_6: 1.888  loss_dice_6: 2.925  loss_ce_7: 3.361  loss_mask_7: 1.83  loss_dice_7: 3.141  loss_ce_8: 3.334  loss_mask_8: 1.898  loss_dice_8: 3.13    time: 0.4363  last_time: 0.4264  data_time: 0.0236  last_data_time: 0.0208   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 09:07:15 d2.utils.events]: \u001b[0m eta: 1 day, 19:56:48  iter: 4039  total_loss: 82.23  loss_ce: 3.515  loss_mask: 1.813  loss_dice: 2.831  loss_ce_0: 3.965  loss_mask_0: 1.776  loss_dice_0: 2.882  loss_ce_1: 3.463  loss_mask_1: 1.824  loss_dice_1: 2.713  loss_ce_2: 3.362  loss_mask_2: 1.976  loss_dice_2: 2.831  loss_ce_3: 3.328  loss_mask_3: 2.048  loss_dice_3: 2.874  loss_ce_4: 3.464  loss_mask_4: 2  loss_dice_4: 2.983  loss_ce_5: 3.609  loss_mask_5: 1.857  loss_dice_5: 2.894  loss_ce_6: 3.559  loss_mask_6: 1.924  loss_dice_6: 2.907  loss_ce_7: 3.502  loss_mask_7: 1.867  loss_dice_7: 2.81  loss_ce_8: 3.524  loss_mask_8: 2.002  loss_dice_8: 2.969    time: 0.4363  last_time: 0.4274  data_time: 0.0242  last_data_time: 0.0187   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 09:07:23 d2.utils.events]: \u001b[0m eta: 1 day, 19:56:40  iter: 4059  total_loss: 78.94  loss_ce: 3.191  loss_mask: 1.969  loss_dice: 2.461  loss_ce_0: 3.609  loss_mask_0: 2.002  loss_dice_0: 2.885  loss_ce_1: 3.054  loss_mask_1: 1.819  loss_dice_1: 2.387  loss_ce_2: 3.178  loss_mask_2: 1.986  loss_dice_2: 2.483  loss_ce_3: 3.125  loss_mask_3: 2.279  loss_dice_3: 2.595  loss_ce_4: 3.212  loss_mask_4: 2.165  loss_dice_4: 2.836  loss_ce_5: 3.138  loss_mask_5: 1.999  loss_dice_5: 2.661  loss_ce_6: 3.134  loss_mask_6: 2.142  loss_dice_6: 2.639  loss_ce_7: 3.113  loss_mask_7: 2.083  loss_dice_7: 2.768  loss_ce_8: 3.148  loss_mask_8: 1.916  loss_dice_8: 2.626    time: 0.4363  last_time: 0.4360  data_time: 0.0236  last_data_time: 0.0173   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 09:07:32 d2.utils.events]: \u001b[0m eta: 1 day, 19:56:31  iter: 4079  total_loss: 78.51  loss_ce: 3.064  loss_mask: 1.697  loss_dice: 3.038  loss_ce_0: 3.504  loss_mask_0: 1.768  loss_dice_0: 3.083  loss_ce_1: 3.047  loss_mask_1: 1.791  loss_dice_1: 2.927  loss_ce_2: 3.015  loss_mask_2: 1.948  loss_dice_2: 2.959  loss_ce_3: 3.076  loss_mask_3: 1.785  loss_dice_3: 2.952  loss_ce_4: 2.974  loss_mask_4: 1.79  loss_dice_4: 2.927  loss_ce_5: 3.005  loss_mask_5: 1.848  loss_dice_5: 3.035  loss_ce_6: 3.02  loss_mask_6: 1.732  loss_dice_6: 2.985  loss_ce_7: 2.994  loss_mask_7: 1.81  loss_dice_7: 3.084  loss_ce_8: 3.042  loss_mask_8: 1.814  loss_dice_8: 3.132    time: 0.4363  last_time: 0.4339  data_time: 0.0216  last_data_time: 0.0249   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 09:07:41 d2.utils.events]: \u001b[0m eta: 1 day, 19:56:28  iter: 4099  total_loss: 82.34  loss_ce: 3.731  loss_mask: 1.923  loss_dice: 2.694  loss_ce_0: 3.938  loss_mask_0: 1.798  loss_dice_0: 2.889  loss_ce_1: 3.687  loss_mask_1: 1.811  loss_dice_1: 2.632  loss_ce_2: 3.79  loss_mask_2: 1.789  loss_dice_2: 2.712  loss_ce_3: 3.792  loss_mask_3: 1.856  loss_dice_3: 2.806  loss_ce_4: 3.791  loss_mask_4: 1.758  loss_dice_4: 2.691  loss_ce_5: 3.698  loss_mask_5: 1.784  loss_dice_5: 2.728  loss_ce_6: 3.7  loss_mask_6: 1.897  loss_dice_6: 2.718  loss_ce_7: 3.853  loss_mask_7: 1.95  loss_dice_7: 2.81  loss_ce_8: 3.766  loss_mask_8: 1.848  loss_dice_8: 2.692    time: 0.4363  last_time: 0.4388  data_time: 0.0240  last_data_time: 0.0253   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 09:07:49 d2.utils.events]: \u001b[0m eta: 1 day, 19:56:10  iter: 4119  total_loss: 76.45  loss_ce: 3.066  loss_mask: 2.147  loss_dice: 2.579  loss_ce_0: 3.537  loss_mask_0: 1.972  loss_dice_0: 2.753  loss_ce_1: 3.171  loss_mask_1: 1.922  loss_dice_1: 2.575  loss_ce_2: 3.129  loss_mask_2: 1.882  loss_dice_2: 2.397  loss_ce_3: 3.048  loss_mask_3: 2.076  loss_dice_3: 2.501  loss_ce_4: 3.162  loss_mask_4: 1.968  loss_dice_4: 2.57  loss_ce_5: 3.129  loss_mask_5: 1.967  loss_dice_5: 2.582  loss_ce_6: 3.034  loss_mask_6: 2.141  loss_dice_6: 2.58  loss_ce_7: 3.105  loss_mask_7: 1.97  loss_dice_7: 2.517  loss_ce_8: 3.022  loss_mask_8: 1.953  loss_dice_8: 2.615    time: 0.4362  last_time: 0.4333  data_time: 0.0223  last_data_time: 0.0232   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 09:07:58 d2.utils.events]: \u001b[0m eta: 1 day, 19:56:22  iter: 4139  total_loss: 78.22  loss_ce: 3.321  loss_mask: 1.744  loss_dice: 2.787  loss_ce_0: 3.749  loss_mask_0: 1.819  loss_dice_0: 2.865  loss_ce_1: 3.186  loss_mask_1: 1.801  loss_dice_1: 2.693  loss_ce_2: 3.213  loss_mask_2: 1.706  loss_dice_2: 2.721  loss_ce_3: 3.183  loss_mask_3: 1.68  loss_dice_3: 2.838  loss_ce_4: 3.087  loss_mask_4: 1.699  loss_dice_4: 2.817  loss_ce_5: 3.17  loss_mask_5: 1.712  loss_dice_5: 2.815  loss_ce_6: 3.148  loss_mask_6: 1.68  loss_dice_6: 2.829  loss_ce_7: 3.267  loss_mask_7: 1.848  loss_dice_7: 2.661  loss_ce_8: 3.267  loss_mask_8: 1.614  loss_dice_8: 2.795    time: 0.4363  last_time: 0.4282  data_time: 0.0339  last_data_time: 0.0206   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 09:08:08 d2.utils.events]: \u001b[0m eta: 1 day, 19:56:19  iter: 4159  total_loss: 77.37  loss_ce: 3.136  loss_mask: 1.767  loss_dice: 2.8  loss_ce_0: 3.584  loss_mask_0: 1.711  loss_dice_0: 3.025  loss_ce_1: 3.017  loss_mask_1: 1.564  loss_dice_1: 2.668  loss_ce_2: 3.186  loss_mask_2: 1.688  loss_dice_2: 2.728  loss_ce_3: 3.106  loss_mask_3: 1.812  loss_dice_3: 2.929  loss_ce_4: 3.014  loss_mask_4: 1.8  loss_dice_4: 2.965  loss_ce_5: 3.168  loss_mask_5: 1.91  loss_dice_5: 2.925  loss_ce_6: 3.21  loss_mask_6: 1.731  loss_dice_6: 2.819  loss_ce_7: 3.203  loss_mask_7: 1.611  loss_dice_7: 2.838  loss_ce_8: 3.2  loss_mask_8: 1.726  loss_dice_8: 2.921    time: 0.4365  last_time: 0.4309  data_time: 0.0657  last_data_time: 0.0237   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 09:08:17 d2.utils.events]: \u001b[0m eta: 1 day, 19:56:29  iter: 4179  total_loss: 80.7  loss_ce: 3.477  loss_mask: 2.035  loss_dice: 2.516  loss_ce_0: 3.746  loss_mask_0: 1.94  loss_dice_0: 2.6  loss_ce_1: 3.355  loss_mask_1: 1.922  loss_dice_1: 2.408  loss_ce_2: 3.441  loss_mask_2: 1.99  loss_dice_2: 2.449  loss_ce_3: 3.384  loss_mask_3: 2.127  loss_dice_3: 2.677  loss_ce_4: 3.586  loss_mask_4: 1.914  loss_dice_4: 2.347  loss_ce_5: 3.397  loss_mask_5: 2.101  loss_dice_5: 2.476  loss_ce_6: 3.349  loss_mask_6: 2.184  loss_dice_6: 2.847  loss_ce_7: 3.445  loss_mask_7: 2.07  loss_dice_7: 2.595  loss_ce_8: 3.417  loss_mask_8: 2.006  loss_dice_8: 2.664    time: 0.4366  last_time: 0.4314  data_time: 0.0579  last_data_time: 0.0231   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 09:08:26 d2.utils.events]: \u001b[0m eta: 1 day, 19:56:09  iter: 4199  total_loss: 80.14  loss_ce: 3.274  loss_mask: 1.815  loss_dice: 2.593  loss_ce_0: 3.641  loss_mask_0: 1.842  loss_dice_0: 2.809  loss_ce_1: 3.247  loss_mask_1: 1.847  loss_dice_1: 2.663  loss_ce_2: 3.326  loss_mask_2: 1.737  loss_dice_2: 2.539  loss_ce_3: 3.279  loss_mask_3: 1.873  loss_dice_3: 2.591  loss_ce_4: 3.281  loss_mask_4: 1.943  loss_dice_4: 2.624  loss_ce_5: 3.272  loss_mask_5: 1.894  loss_dice_5: 2.638  loss_ce_6: 3.296  loss_mask_6: 1.806  loss_dice_6: 2.633  loss_ce_7: 3.341  loss_mask_7: 1.936  loss_dice_7: 2.798  loss_ce_8: 3.426  loss_mask_8: 1.856  loss_dice_8: 2.584    time: 0.4367  last_time: 0.4227  data_time: 0.0298  last_data_time: 0.0170   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 09:08:35 d2.utils.events]: \u001b[0m eta: 1 day, 19:55:16  iter: 4219  total_loss: 77.64  loss_ce: 2.892  loss_mask: 1.834  loss_dice: 2.727  loss_ce_0: 3.445  loss_mask_0: 1.804  loss_dice_0: 2.793  loss_ce_1: 2.662  loss_mask_1: 1.818  loss_dice_1: 2.859  loss_ce_2: 2.78  loss_mask_2: 1.913  loss_dice_2: 2.617  loss_ce_3: 2.899  loss_mask_3: 2.174  loss_dice_3: 2.773  loss_ce_4: 2.727  loss_mask_4: 2.025  loss_dice_4: 2.771  loss_ce_5: 2.814  loss_mask_5: 2.032  loss_dice_5: 2.935  loss_ce_6: 2.876  loss_mask_6: 2.086  loss_dice_6: 2.889  loss_ce_7: 2.9  loss_mask_7: 1.891  loss_dice_7: 2.808  loss_ce_8: 2.938  loss_mask_8: 1.895  loss_dice_8: 2.691    time: 0.4366  last_time: 0.4335  data_time: 0.0221  last_data_time: 0.0261   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 09:08:44 d2.utils.events]: \u001b[0m eta: 1 day, 19:54:55  iter: 4239  total_loss: 76.08  loss_ce: 3.009  loss_mask: 2.099  loss_dice: 2.644  loss_ce_0: 3.364  loss_mask_0: 1.966  loss_dice_0: 2.618  loss_ce_1: 2.836  loss_mask_1: 2.048  loss_dice_1: 2.399  loss_ce_2: 2.956  loss_mask_2: 1.927  loss_dice_2: 2.429  loss_ce_3: 2.868  loss_mask_3: 2.012  loss_dice_3: 2.702  loss_ce_4: 2.934  loss_mask_4: 2.155  loss_dice_4: 2.602  loss_ce_5: 2.846  loss_mask_5: 2.175  loss_dice_5: 2.857  loss_ce_6: 2.888  loss_mask_6: 2.091  loss_dice_6: 2.792  loss_ce_7: 2.947  loss_mask_7: 2.238  loss_dice_7: 2.747  loss_ce_8: 2.906  loss_mask_8: 1.957  loss_dice_8: 2.694    time: 0.4366  last_time: 0.4408  data_time: 0.0222  last_data_time: 0.0243   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 09:08:52 d2.utils.events]: \u001b[0m eta: 1 day, 19:54:44  iter: 4259  total_loss: 79.77  loss_ce: 3.166  loss_mask: 1.926  loss_dice: 2.754  loss_ce_0: 3.505  loss_mask_0: 1.786  loss_dice_0: 2.854  loss_ce_1: 3.088  loss_mask_1: 1.951  loss_dice_1: 2.603  loss_ce_2: 3.066  loss_mask_2: 1.974  loss_dice_2: 2.683  loss_ce_3: 3.195  loss_mask_3: 1.766  loss_dice_3: 2.818  loss_ce_4: 3.171  loss_mask_4: 1.852  loss_dice_4: 2.704  loss_ce_5: 3.082  loss_mask_5: 1.929  loss_dice_5: 2.818  loss_ce_6: 3.201  loss_mask_6: 1.973  loss_dice_6: 2.809  loss_ce_7: 3.149  loss_mask_7: 1.926  loss_dice_7: 2.858  loss_ce_8: 3.098  loss_mask_8: 1.888  loss_dice_8: 2.792    time: 0.4366  last_time: 0.4288  data_time: 0.0226  last_data_time: 0.0210   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 09:09:01 d2.utils.events]: \u001b[0m eta: 1 day, 19:53:32  iter: 4279  total_loss: 78.13  loss_ce: 2.963  loss_mask: 1.939  loss_dice: 2.852  loss_ce_0: 3.392  loss_mask_0: 1.686  loss_dice_0: 2.917  loss_ce_1: 2.892  loss_mask_1: 1.753  loss_dice_1: 2.8  loss_ce_2: 2.967  loss_mask_2: 1.687  loss_dice_2: 2.76  loss_ce_3: 2.872  loss_mask_3: 1.75  loss_dice_3: 2.715  loss_ce_4: 3.029  loss_mask_4: 1.788  loss_dice_4: 2.891  loss_ce_5: 2.941  loss_mask_5: 1.805  loss_dice_5: 2.908  loss_ce_6: 2.956  loss_mask_6: 1.961  loss_dice_6: 2.91  loss_ce_7: 3.057  loss_mask_7: 1.95  loss_dice_7: 2.932  loss_ce_8: 3.101  loss_mask_8: 1.681  loss_dice_8: 2.834    time: 0.4366  last_time: 0.4272  data_time: 0.0215  last_data_time: 0.0210   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 09:09:10 d2.utils.events]: \u001b[0m eta: 1 day, 19:53:23  iter: 4299  total_loss: 81.85  loss_ce: 3.461  loss_mask: 2.063  loss_dice: 2.856  loss_ce_0: 3.65  loss_mask_0: 1.844  loss_dice_0: 2.883  loss_ce_1: 3.448  loss_mask_1: 1.912  loss_dice_1: 2.879  loss_ce_2: 3.405  loss_mask_2: 1.995  loss_dice_2: 2.769  loss_ce_3: 3.477  loss_mask_3: 1.868  loss_dice_3: 2.927  loss_ce_4: 3.456  loss_mask_4: 1.99  loss_dice_4: 2.841  loss_ce_5: 3.282  loss_mask_5: 1.939  loss_dice_5: 3.014  loss_ce_6: 3.345  loss_mask_6: 1.949  loss_dice_6: 2.867  loss_ce_7: 3.457  loss_mask_7: 2.03  loss_dice_7: 2.935  loss_ce_8: 3.508  loss_mask_8: 2.091  loss_dice_8: 2.818    time: 0.4366  last_time: 0.4359  data_time: 0.0242  last_data_time: 0.0178   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 09:09:18 d2.utils.events]: \u001b[0m eta: 1 day, 19:52:53  iter: 4319  total_loss: 78.7  loss_ce: 3.336  loss_mask: 2.156  loss_dice: 2.592  loss_ce_0: 3.718  loss_mask_0: 1.847  loss_dice_0: 2.495  loss_ce_1: 3.316  loss_mask_1: 1.837  loss_dice_1: 2.288  loss_ce_2: 3.243  loss_mask_2: 1.874  loss_dice_2: 2.242  loss_ce_3: 3.311  loss_mask_3: 2.088  loss_dice_3: 2.423  loss_ce_4: 3.372  loss_mask_4: 1.769  loss_dice_4: 2.43  loss_ce_5: 3.323  loss_mask_5: 1.953  loss_dice_5: 2.41  loss_ce_6: 3.33  loss_mask_6: 1.835  loss_dice_6: 2.504  loss_ce_7: 3.346  loss_mask_7: 1.968  loss_dice_7: 2.556  loss_ce_8: 3.33  loss_mask_8: 1.981  loss_dice_8: 2.472    time: 0.4366  last_time: 0.4340  data_time: 0.0227  last_data_time: 0.0240   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 09:09:27 d2.utils.events]: \u001b[0m eta: 1 day, 19:53:06  iter: 4339  total_loss: 79.62  loss_ce: 3.102  loss_mask: 1.901  loss_dice: 2.739  loss_ce_0: 3.627  loss_mask_0: 1.903  loss_dice_0: 2.853  loss_ce_1: 3.046  loss_mask_1: 1.745  loss_dice_1: 2.651  loss_ce_2: 3.027  loss_mask_2: 1.915  loss_dice_2: 2.718  loss_ce_3: 3.095  loss_mask_3: 1.941  loss_dice_3: 2.684  loss_ce_4: 3.146  loss_mask_4: 1.891  loss_dice_4: 2.743  loss_ce_5: 3.092  loss_mask_5: 1.926  loss_dice_5: 2.825  loss_ce_6: 3.057  loss_mask_6: 2.149  loss_dice_6: 2.995  loss_ce_7: 3.091  loss_mask_7: 2.061  loss_dice_7: 2.981  loss_ce_8: 3.106  loss_mask_8: 1.982  loss_dice_8: 2.744    time: 0.4366  last_time: 0.4373  data_time: 0.0232  last_data_time: 0.0263   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 09:09:36 d2.utils.events]: \u001b[0m eta: 1 day, 19:52:21  iter: 4359  total_loss: 79.13  loss_ce: 2.846  loss_mask: 1.978  loss_dice: 2.974  loss_ce_0: 3.506  loss_mask_0: 1.998  loss_dice_0: 2.839  loss_ce_1: 2.858  loss_mask_1: 1.921  loss_dice_1: 2.582  loss_ce_2: 2.897  loss_mask_2: 1.902  loss_dice_2: 2.756  loss_ce_3: 2.978  loss_mask_3: 1.832  loss_dice_3: 2.775  loss_ce_4: 2.949  loss_mask_4: 2  loss_dice_4: 2.785  loss_ce_5: 3.001  loss_mask_5: 1.98  loss_dice_5: 2.699  loss_ce_6: 3.035  loss_mask_6: 1.991  loss_dice_6: 2.821  loss_ce_7: 2.783  loss_mask_7: 2.026  loss_dice_7: 2.917  loss_ce_8: 2.889  loss_mask_8: 2.177  loss_dice_8: 2.814    time: 0.4366  last_time: 0.4323  data_time: 0.0221  last_data_time: 0.0221   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 09:09:45 d2.utils.events]: \u001b[0m eta: 1 day, 19:52:12  iter: 4379  total_loss: 77.55  loss_ce: 3.345  loss_mask: 1.982  loss_dice: 2.827  loss_ce_0: 3.471  loss_mask_0: 1.711  loss_dice_0: 2.846  loss_ce_1: 3.344  loss_mask_1: 1.675  loss_dice_1: 2.714  loss_ce_2: 3.299  loss_mask_2: 1.879  loss_dice_2: 2.774  loss_ce_3: 3.241  loss_mask_3: 1.717  loss_dice_3: 2.789  loss_ce_4: 3.259  loss_mask_4: 1.82  loss_dice_4: 2.72  loss_ce_5: 3.233  loss_mask_5: 1.853  loss_dice_5: 2.724  loss_ce_6: 3.296  loss_mask_6: 1.867  loss_dice_6: 2.749  loss_ce_7: 3.289  loss_mask_7: 1.786  loss_dice_7: 2.797  loss_ce_8: 3.281  loss_mask_8: 1.784  loss_dice_8: 2.81    time: 0.4366  last_time: 0.4342  data_time: 0.0221  last_data_time: 0.0209   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 09:09:53 d2.utils.events]: \u001b[0m eta: 1 day, 19:51:56  iter: 4399  total_loss: 76.81  loss_ce: 3.262  loss_mask: 2.082  loss_dice: 2.615  loss_ce_0: 3.517  loss_mask_0: 1.801  loss_dice_0: 2.661  loss_ce_1: 3.214  loss_mask_1: 1.892  loss_dice_1: 2.491  loss_ce_2: 3.286  loss_mask_2: 1.976  loss_dice_2: 2.646  loss_ce_3: 3.243  loss_mask_3: 2.061  loss_dice_3: 2.686  loss_ce_4: 3.341  loss_mask_4: 1.87  loss_dice_4: 2.554  loss_ce_5: 3.238  loss_mask_5: 1.892  loss_dice_5: 2.486  loss_ce_6: 3.3  loss_mask_6: 1.67  loss_dice_6: 2.504  loss_ce_7: 3.166  loss_mask_7: 1.968  loss_dice_7: 2.595  loss_ce_8: 3.241  loss_mask_8: 1.877  loss_dice_8: 2.609    time: 0.4366  last_time: 0.4277  data_time: 0.0223  last_data_time: 0.0214   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 09:10:02 d2.utils.events]: \u001b[0m eta: 1 day, 19:51:35  iter: 4419  total_loss: 80.47  loss_ce: 3.503  loss_mask: 1.865  loss_dice: 2.701  loss_ce_0: 3.714  loss_mask_0: 1.861  loss_dice_0: 2.839  loss_ce_1: 3.464  loss_mask_1: 1.902  loss_dice_1: 2.651  loss_ce_2: 3.507  loss_mask_2: 1.887  loss_dice_2: 2.671  loss_ce_3: 3.511  loss_mask_3: 1.89  loss_dice_3: 2.765  loss_ce_4: 3.638  loss_mask_4: 1.932  loss_dice_4: 2.686  loss_ce_5: 3.465  loss_mask_5: 1.926  loss_dice_5: 2.61  loss_ce_6: 3.478  loss_mask_6: 1.949  loss_dice_6: 2.807  loss_ce_7: 3.406  loss_mask_7: 2.07  loss_dice_7: 2.783  loss_ce_8: 3.516  loss_mask_8: 1.882  loss_dice_8: 2.795    time: 0.4366  last_time: 0.4418  data_time: 0.0242  last_data_time: 0.0286   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 09:10:11 d2.utils.events]: \u001b[0m eta: 1 day, 19:51:24  iter: 4439  total_loss: 78.25  loss_ce: 2.879  loss_mask: 1.788  loss_dice: 2.878  loss_ce_0: 3.258  loss_mask_0: 1.677  loss_dice_0: 2.826  loss_ce_1: 2.839  loss_mask_1: 1.567  loss_dice_1: 2.523  loss_ce_2: 2.773  loss_mask_2: 1.781  loss_dice_2: 2.663  loss_ce_3: 2.722  loss_mask_3: 1.685  loss_dice_3: 2.862  loss_ce_4: 2.743  loss_mask_4: 1.841  loss_dice_4: 2.881  loss_ce_5: 2.898  loss_mask_5: 1.628  loss_dice_5: 2.847  loss_ce_6: 2.919  loss_mask_6: 1.956  loss_dice_6: 2.707  loss_ce_7: 2.856  loss_mask_7: 1.877  loss_dice_7: 2.891  loss_ce_8: 2.851  loss_mask_8: 1.974  loss_dice_8: 2.844    time: 0.4366  last_time: 0.4509  data_time: 0.0228  last_data_time: 0.0234   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 09:10:19 d2.utils.events]: \u001b[0m eta: 1 day, 19:51:11  iter: 4459  total_loss: 77.05  loss_ce: 3.229  loss_mask: 1.984  loss_dice: 2.63  loss_ce_0: 3.436  loss_mask_0: 1.824  loss_dice_0: 2.59  loss_ce_1: 3.461  loss_mask_1: 1.68  loss_dice_1: 2.345  loss_ce_2: 3.208  loss_mask_2: 1.844  loss_dice_2: 2.496  loss_ce_3: 3.202  loss_mask_3: 1.949  loss_dice_3: 2.519  loss_ce_4: 3.297  loss_mask_4: 1.891  loss_dice_4: 2.69  loss_ce_5: 3.336  loss_mask_5: 1.93  loss_dice_5: 2.542  loss_ce_6: 3.319  loss_mask_6: 2.033  loss_dice_6: 2.472  loss_ce_7: 3.424  loss_mask_7: 1.832  loss_dice_7: 2.521  loss_ce_8: 3.275  loss_mask_8: 2.023  loss_dice_8: 2.639    time: 0.4366  last_time: 0.4374  data_time: 0.0216  last_data_time: 0.0246   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 09:10:28 d2.utils.events]: \u001b[0m eta: 1 day, 19:51:02  iter: 4479  total_loss: 76.66  loss_ce: 3.246  loss_mask: 1.928  loss_dice: 2.689  loss_ce_0: 3.553  loss_mask_0: 1.904  loss_dice_0: 2.558  loss_ce_1: 3.15  loss_mask_1: 1.729  loss_dice_1: 2.451  loss_ce_2: 3.138  loss_mask_2: 1.96  loss_dice_2: 2.571  loss_ce_3: 3.079  loss_mask_3: 2.116  loss_dice_3: 2.692  loss_ce_4: 3.271  loss_mask_4: 1.992  loss_dice_4: 2.64  loss_ce_5: 3.319  loss_mask_5: 1.995  loss_dice_5: 2.448  loss_ce_6: 3.104  loss_mask_6: 2.099  loss_dice_6: 2.453  loss_ce_7: 3.189  loss_mask_7: 2.01  loss_dice_7: 2.597  loss_ce_8: 3.119  loss_mask_8: 1.875  loss_dice_8: 2.482    time: 0.4366  last_time: 0.4344  data_time: 0.0231  last_data_time: 0.0219   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 09:10:37 d2.utils.events]: \u001b[0m eta: 1 day, 19:51:00  iter: 4499  total_loss: 82.98  loss_ce: 3.347  loss_mask: 2.061  loss_dice: 2.832  loss_ce_0: 3.48  loss_mask_0: 1.836  loss_dice_0: 2.735  loss_ce_1: 3.445  loss_mask_1: 1.752  loss_dice_1: 2.547  loss_ce_2: 3.45  loss_mask_2: 2.012  loss_dice_2: 2.815  loss_ce_3: 3.436  loss_mask_3: 1.871  loss_dice_3: 3.014  loss_ce_4: 3.439  loss_mask_4: 1.859  loss_dice_4: 2.709  loss_ce_5: 3.517  loss_mask_5: 1.871  loss_dice_5: 2.696  loss_ce_6: 3.427  loss_mask_6: 1.964  loss_dice_6: 2.893  loss_ce_7: 3.377  loss_mask_7: 1.953  loss_dice_7: 2.758  loss_ce_8: 3.575  loss_mask_8: 1.988  loss_dice_8: 2.66    time: 0.4366  last_time: 0.4317  data_time: 0.0230  last_data_time: 0.0245   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 09:10:46 d2.utils.events]: \u001b[0m eta: 1 day, 19:50:54  iter: 4519  total_loss: 77.26  loss_ce: 3.278  loss_mask: 1.989  loss_dice: 2.521  loss_ce_0: 3.383  loss_mask_0: 1.86  loss_dice_0: 2.706  loss_ce_1: 3.225  loss_mask_1: 1.879  loss_dice_1: 2.334  loss_ce_2: 3.126  loss_mask_2: 2.111  loss_dice_2: 2.771  loss_ce_3: 3.273  loss_mask_3: 1.989  loss_dice_3: 2.632  loss_ce_4: 3.015  loss_mask_4: 2.161  loss_dice_4: 2.515  loss_ce_5: 3.179  loss_mask_5: 1.882  loss_dice_5: 2.607  loss_ce_6: 3.278  loss_mask_6: 2.007  loss_dice_6: 2.597  loss_ce_7: 3.316  loss_mask_7: 1.845  loss_dice_7: 2.578  loss_ce_8: 3.197  loss_mask_8: 1.859  loss_dice_8: 2.589    time: 0.4366  last_time: 0.4366  data_time: 0.0227  last_data_time: 0.0226   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 09:10:54 d2.utils.events]: \u001b[0m eta: 1 day, 19:50:49  iter: 4539  total_loss: 79.75  loss_ce: 3.386  loss_mask: 1.651  loss_dice: 2.768  loss_ce_0: 3.548  loss_mask_0: 1.714  loss_dice_0: 2.92  loss_ce_1: 3.438  loss_mask_1: 1.635  loss_dice_1: 2.662  loss_ce_2: 3.389  loss_mask_2: 1.99  loss_dice_2: 2.922  loss_ce_3: 3.321  loss_mask_3: 1.9  loss_dice_3: 2.802  loss_ce_4: 3.257  loss_mask_4: 2.072  loss_dice_4: 2.659  loss_ce_5: 3.427  loss_mask_5: 1.677  loss_dice_5: 2.697  loss_ce_6: 3.36  loss_mask_6: 1.78  loss_dice_6: 2.66  loss_ce_7: 3.414  loss_mask_7: 1.712  loss_dice_7: 2.77  loss_ce_8: 3.438  loss_mask_8: 1.777  loss_dice_8: 2.784    time: 0.4366  last_time: 0.4339  data_time: 0.0247  last_data_time: 0.0243   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 09:11:03 d2.utils.events]: \u001b[0m eta: 1 day, 19:50:28  iter: 4559  total_loss: 77.56  loss_ce: 2.76  loss_mask: 2.214  loss_dice: 2.75  loss_ce_0: 3.072  loss_mask_0: 2.038  loss_dice_0: 2.773  loss_ce_1: 2.909  loss_mask_1: 1.917  loss_dice_1: 2.557  loss_ce_2: 2.909  loss_mask_2: 2.051  loss_dice_2: 2.721  loss_ce_3: 2.792  loss_mask_3: 2.071  loss_dice_3: 2.709  loss_ce_4: 2.804  loss_mask_4: 2.119  loss_dice_4: 2.763  loss_ce_5: 2.97  loss_mask_5: 2.173  loss_dice_5: 2.768  loss_ce_6: 2.991  loss_mask_6: 2.173  loss_dice_6: 2.688  loss_ce_7: 2.913  loss_mask_7: 1.969  loss_dice_7: 2.677  loss_ce_8: 2.971  loss_mask_8: 2.106  loss_dice_8: 2.628    time: 0.4365  last_time: 0.4281  data_time: 0.0220  last_data_time: 0.0211   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 09:11:12 d2.utils.events]: \u001b[0m eta: 1 day, 19:50:14  iter: 4579  total_loss: 77.31  loss_ce: 2.752  loss_mask: 1.833  loss_dice: 2.712  loss_ce_0: 3.156  loss_mask_0: 1.688  loss_dice_0: 2.756  loss_ce_1: 2.743  loss_mask_1: 1.727  loss_dice_1: 2.515  loss_ce_2: 2.915  loss_mask_2: 1.76  loss_dice_2: 2.722  loss_ce_3: 2.982  loss_mask_3: 1.741  loss_dice_3: 2.812  loss_ce_4: 2.725  loss_mask_4: 2.076  loss_dice_4: 2.716  loss_ce_5: 2.869  loss_mask_5: 1.932  loss_dice_5: 2.719  loss_ce_6: 2.867  loss_mask_6: 1.987  loss_dice_6: 2.675  loss_ce_7: 2.884  loss_mask_7: 1.893  loss_dice_7: 2.736  loss_ce_8: 2.964  loss_mask_8: 1.984  loss_dice_8: 2.653    time: 0.4365  last_time: 0.4542  data_time: 0.0225  last_data_time: 0.0190   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 09:11:21 d2.utils.events]: \u001b[0m eta: 1 day, 19:49:48  iter: 4599  total_loss: 81.21  loss_ce: 3.31  loss_mask: 1.963  loss_dice: 2.745  loss_ce_0: 3.501  loss_mask_0: 1.849  loss_dice_0: 2.693  loss_ce_1: 3.378  loss_mask_1: 1.935  loss_dice_1: 2.632  loss_ce_2: 3.358  loss_mask_2: 1.899  loss_dice_2: 2.686  loss_ce_3: 3.414  loss_mask_3: 1.881  loss_dice_3: 2.619  loss_ce_4: 3.349  loss_mask_4: 1.973  loss_dice_4: 2.652  loss_ce_5: 3.364  loss_mask_5: 1.935  loss_dice_5: 2.698  loss_ce_6: 3.386  loss_mask_6: 1.835  loss_dice_6: 2.832  loss_ce_7: 3.37  loss_mask_7: 1.908  loss_dice_7: 2.616  loss_ce_8: 3.361  loss_mask_8: 1.654  loss_dice_8: 2.769    time: 0.4365  last_time: 0.4513  data_time: 0.0233  last_data_time: 0.0285   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 09:11:29 d2.utils.events]: \u001b[0m eta: 1 day, 19:49:57  iter: 4619  total_loss: 72.92  loss_ce: 2.879  loss_mask: 1.687  loss_dice: 2.871  loss_ce_0: 3.294  loss_mask_0: 1.71  loss_dice_0: 2.94  loss_ce_1: 2.898  loss_mask_1: 1.634  loss_dice_1: 2.816  loss_ce_2: 2.851  loss_mask_2: 1.816  loss_dice_2: 2.897  loss_ce_3: 2.811  loss_mask_3: 1.674  loss_dice_3: 2.935  loss_ce_4: 2.814  loss_mask_4: 1.733  loss_dice_4: 2.821  loss_ce_5: 2.859  loss_mask_5: 1.771  loss_dice_5: 2.849  loss_ce_6: 2.884  loss_mask_6: 1.646  loss_dice_6: 2.81  loss_ce_7: 2.85  loss_mask_7: 1.782  loss_dice_7: 2.879  loss_ce_8: 2.933  loss_mask_8: 1.625  loss_dice_8: 2.755    time: 0.4365  last_time: 0.4347  data_time: 0.0221  last_data_time: 0.0271   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 09:11:38 d2.utils.events]: \u001b[0m eta: 1 day, 19:49:48  iter: 4639  total_loss: 78.66  loss_ce: 3.048  loss_mask: 2.149  loss_dice: 2.627  loss_ce_0: 3.294  loss_mask_0: 1.888  loss_dice_0: 2.587  loss_ce_1: 3.078  loss_mask_1: 1.656  loss_dice_1: 2.291  loss_ce_2: 2.972  loss_mask_2: 1.991  loss_dice_2: 2.725  loss_ce_3: 2.965  loss_mask_3: 2.121  loss_dice_3: 2.53  loss_ce_4: 2.963  loss_mask_4: 1.938  loss_dice_4: 2.571  loss_ce_5: 3.111  loss_mask_5: 2.024  loss_dice_5: 2.673  loss_ce_6: 3.093  loss_mask_6: 2.018  loss_dice_6: 2.593  loss_ce_7: 2.981  loss_mask_7: 1.952  loss_dice_7: 2.558  loss_ce_8: 3.048  loss_mask_8: 1.939  loss_dice_8: 2.449    time: 0.4365  last_time: 0.4315  data_time: 0.0228  last_data_time: 0.0206   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 09:11:47 d2.utils.events]: \u001b[0m eta: 1 day, 19:49:42  iter: 4659  total_loss: 77.96  loss_ce: 3.221  loss_mask: 2.001  loss_dice: 2.706  loss_ce_0: 3.438  loss_mask_0: 1.957  loss_dice_0: 2.678  loss_ce_1: 3.246  loss_mask_1: 1.923  loss_dice_1: 2.709  loss_ce_2: 3.205  loss_mask_2: 1.96  loss_dice_2: 2.718  loss_ce_3: 3.166  loss_mask_3: 1.999  loss_dice_3: 2.625  loss_ce_4: 3.102  loss_mask_4: 2.058  loss_dice_4: 2.528  loss_ce_5: 3.319  loss_mask_5: 1.875  loss_dice_5: 2.519  loss_ce_6: 3.24  loss_mask_6: 1.936  loss_dice_6: 2.442  loss_ce_7: 3.224  loss_mask_7: 2.085  loss_dice_7: 2.507  loss_ce_8: 3.358  loss_mask_8: 1.933  loss_dice_8: 2.597    time: 0.4365  last_time: 0.4340  data_time: 0.0223  last_data_time: 0.0212   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 09:11:55 d2.utils.events]: \u001b[0m eta: 1 day, 19:49:34  iter: 4679  total_loss: 83.8  loss_ce: 3.665  loss_mask: 1.958  loss_dice: 2.694  loss_ce_0: 3.732  loss_mask_0: 1.815  loss_dice_0: 2.88  loss_ce_1: 3.607  loss_mask_1: 1.959  loss_dice_1: 2.633  loss_ce_2: 3.631  loss_mask_2: 1.886  loss_dice_2: 2.704  loss_ce_3: 3.502  loss_mask_3: 2.009  loss_dice_3: 2.845  loss_ce_4: 3.488  loss_mask_4: 2.107  loss_dice_4: 2.849  loss_ce_5: 3.599  loss_mask_5: 1.967  loss_dice_5: 2.625  loss_ce_6: 3.533  loss_mask_6: 1.837  loss_dice_6: 2.806  loss_ce_7: 3.513  loss_mask_7: 1.945  loss_dice_7: 2.819  loss_ce_8: 3.633  loss_mask_8: 1.921  loss_dice_8: 2.844    time: 0.4365  last_time: 0.4300  data_time: 0.0245  last_data_time: 0.0210   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 09:12:04 d2.utils.events]: \u001b[0m eta: 1 day, 19:49:17  iter: 4699  total_loss: 77.39  loss_ce: 3.329  loss_mask: 1.826  loss_dice: 2.745  loss_ce_0: 3.438  loss_mask_0: 1.946  loss_dice_0: 2.858  loss_ce_1: 3.207  loss_mask_1: 1.76  loss_dice_1: 2.805  loss_ce_2: 3.204  loss_mask_2: 1.917  loss_dice_2: 2.877  loss_ce_3: 3.244  loss_mask_3: 1.888  loss_dice_3: 2.868  loss_ce_4: 3.176  loss_mask_4: 1.743  loss_dice_4: 2.864  loss_ce_5: 3.106  loss_mask_5: 1.948  loss_dice_5: 2.879  loss_ce_6: 3.267  loss_mask_6: 2.067  loss_dice_6: 2.922  loss_ce_7: 3.211  loss_mask_7: 1.905  loss_dice_7: 2.889  loss_ce_8: 3.289  loss_mask_8: 2.025  loss_dice_8: 3.076    time: 0.4365  last_time: 0.4346  data_time: 0.0226  last_data_time: 0.0230   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 09:12:13 d2.utils.events]: \u001b[0m eta: 1 day, 19:49:08  iter: 4719  total_loss: 81.6  loss_ce: 3.464  loss_mask: 1.846  loss_dice: 2.851  loss_ce_0: 3.439  loss_mask_0: 1.786  loss_dice_0: 2.99  loss_ce_1: 3.474  loss_mask_1: 1.735  loss_dice_1: 2.888  loss_ce_2: 3.545  loss_mask_2: 1.806  loss_dice_2: 3.057  loss_ce_3: 3.32  loss_mask_3: 1.734  loss_dice_3: 2.959  loss_ce_4: 3.588  loss_mask_4: 1.962  loss_dice_4: 3.108  loss_ce_5: 3.291  loss_mask_5: 1.931  loss_dice_5: 3.122  loss_ce_6: 3.409  loss_mask_6: 1.926  loss_dice_6: 3.11  loss_ce_7: 3.449  loss_mask_7: 1.768  loss_dice_7: 3.094  loss_ce_8: 3.5  loss_mask_8: 1.638  loss_dice_8: 3.071    time: 0.4365  last_time: 0.4290  data_time: 0.0229  last_data_time: 0.0206   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 09:12:21 d2.utils.events]: \u001b[0m eta: 1 day, 19:48:23  iter: 4739  total_loss: 74.16  loss_ce: 2.892  loss_mask: 2.108  loss_dice: 2.629  loss_ce_0: 3.163  loss_mask_0: 2.012  loss_dice_0: 2.642  loss_ce_1: 2.883  loss_mask_1: 2.085  loss_dice_1: 2.47  loss_ce_2: 2.866  loss_mask_2: 2.057  loss_dice_2: 2.509  loss_ce_3: 2.954  loss_mask_3: 1.848  loss_dice_3: 2.64  loss_ce_4: 2.987  loss_mask_4: 1.963  loss_dice_4: 2.299  loss_ce_5: 3.175  loss_mask_5: 2.021  loss_dice_5: 2.479  loss_ce_6: 3.141  loss_mask_6: 1.883  loss_dice_6: 2.458  loss_ce_7: 2.992  loss_mask_7: 2.101  loss_dice_7: 2.543  loss_ce_8: 2.878  loss_mask_8: 2.073  loss_dice_8: 2.614    time: 0.4365  last_time: 0.4383  data_time: 0.0222  last_data_time: 0.0242   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 09:12:30 d2.utils.events]: \u001b[0m eta: 1 day, 19:48:33  iter: 4759  total_loss: 78.54  loss_ce: 2.953  loss_mask: 1.948  loss_dice: 2.659  loss_ce_0: 3.311  loss_mask_0: 2.031  loss_dice_0: 2.617  loss_ce_1: 2.831  loss_mask_1: 1.918  loss_dice_1: 2.448  loss_ce_2: 3.006  loss_mask_2: 2.078  loss_dice_2: 2.568  loss_ce_3: 2.971  loss_mask_3: 2.042  loss_dice_3: 2.64  loss_ce_4: 3.147  loss_mask_4: 2.041  loss_dice_4: 2.692  loss_ce_5: 2.992  loss_mask_5: 2.018  loss_dice_5: 2.606  loss_ce_6: 2.933  loss_mask_6: 1.854  loss_dice_6: 2.526  loss_ce_7: 2.974  loss_mask_7: 2.202  loss_dice_7: 2.608  loss_ce_8: 2.914  loss_mask_8: 2.102  loss_dice_8: 2.783    time: 0.4365  last_time: 0.4386  data_time: 0.0222  last_data_time: 0.0253   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 09:12:39 d2.utils.events]: \u001b[0m eta: 1 day, 19:47:57  iter: 4779  total_loss: 81.69  loss_ce: 3.292  loss_mask: 1.92  loss_dice: 2.665  loss_ce_0: 3.392  loss_mask_0: 1.924  loss_dice_0: 2.834  loss_ce_1: 3.151  loss_mask_1: 1.871  loss_dice_1: 2.722  loss_ce_2: 3.293  loss_mask_2: 1.928  loss_dice_2: 2.727  loss_ce_3: 3.191  loss_mask_3: 1.921  loss_dice_3: 2.758  loss_ce_4: 3.274  loss_mask_4: 2.038  loss_dice_4: 2.78  loss_ce_5: 3.183  loss_mask_5: 1.837  loss_dice_5: 2.828  loss_ce_6: 3.159  loss_mask_6: 1.928  loss_dice_6: 2.821  loss_ce_7: 3.105  loss_mask_7: 2.04  loss_dice_7: 2.819  loss_ce_8: 3.12  loss_mask_8: 1.917  loss_dice_8: 2.893    time: 0.4365  last_time: 0.4624  data_time: 0.0226  last_data_time: 0.0228   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 09:12:48 d2.utils.events]: \u001b[0m eta: 1 day, 19:47:36  iter: 4799  total_loss: 81.06  loss_ce: 3.224  loss_mask: 1.74  loss_dice: 2.764  loss_ce_0: 3.328  loss_mask_0: 1.826  loss_dice_0: 2.904  loss_ce_1: 3.176  loss_mask_1: 1.761  loss_dice_1: 2.774  loss_ce_2: 3.075  loss_mask_2: 1.778  loss_dice_2: 2.669  loss_ce_3: 3.064  loss_mask_3: 1.835  loss_dice_3: 2.796  loss_ce_4: 3.001  loss_mask_4: 2.03  loss_dice_4: 2.998  loss_ce_5: 3.077  loss_mask_5: 1.894  loss_dice_5: 2.771  loss_ce_6: 3.187  loss_mask_6: 1.753  loss_dice_6: 2.863  loss_ce_7: 3.083  loss_mask_7: 1.753  loss_dice_7: 2.88  loss_ce_8: 3.025  loss_mask_8: 1.986  loss_dice_8: 2.857    time: 0.4364  last_time: 0.4377  data_time: 0.0228  last_data_time: 0.0240   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 09:12:56 d2.utils.events]: \u001b[0m eta: 1 day, 19:47:17  iter: 4819  total_loss: 77.26  loss_ce: 3.079  loss_mask: 1.742  loss_dice: 2.645  loss_ce_0: 3.313  loss_mask_0: 1.855  loss_dice_0: 2.786  loss_ce_1: 2.898  loss_mask_1: 1.806  loss_dice_1: 2.693  loss_ce_2: 2.989  loss_mask_2: 1.838  loss_dice_2: 2.806  loss_ce_3: 3.015  loss_mask_3: 2.158  loss_dice_3: 2.747  loss_ce_4: 2.957  loss_mask_4: 2.014  loss_dice_4: 2.707  loss_ce_5: 3.019  loss_mask_5: 1.937  loss_dice_5: 2.81  loss_ce_6: 3.026  loss_mask_6: 1.926  loss_dice_6: 2.683  loss_ce_7: 2.931  loss_mask_7: 1.985  loss_dice_7: 2.732  loss_ce_8: 2.899  loss_mask_8: 1.786  loss_dice_8: 2.778    time: 0.4364  last_time: 0.4279  data_time: 0.0227  last_data_time: 0.0191   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 09:13:05 d2.utils.events]: \u001b[0m eta: 1 day, 19:47:09  iter: 4839  total_loss: 78.14  loss_ce: 3.313  loss_mask: 1.796  loss_dice: 2.569  loss_ce_0: 3.422  loss_mask_0: 1.781  loss_dice_0: 2.737  loss_ce_1: 3.387  loss_mask_1: 1.697  loss_dice_1: 2.538  loss_ce_2: 3.388  loss_mask_2: 1.91  loss_dice_2: 2.535  loss_ce_3: 3.304  loss_mask_3: 1.853  loss_dice_3: 2.645  loss_ce_4: 3.318  loss_mask_4: 1.824  loss_dice_4: 2.578  loss_ce_5: 3.292  loss_mask_5: 1.779  loss_dice_5: 2.722  loss_ce_6: 3.294  loss_mask_6: 1.939  loss_dice_6: 2.756  loss_ce_7: 3.202  loss_mask_7: 1.787  loss_dice_7: 2.562  loss_ce_8: 3.166  loss_mask_8: 1.839  loss_dice_8: 2.733    time: 0.4364  last_time: 0.4341  data_time: 0.0246  last_data_time: 0.0265   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 09:13:14 d2.utils.events]: \u001b[0m eta: 1 day, 19:46:28  iter: 4859  total_loss: 80.86  loss_ce: 3.53  loss_mask: 1.772  loss_dice: 2.872  loss_ce_0: 3.581  loss_mask_0: 1.911  loss_dice_0: 2.886  loss_ce_1: 3.426  loss_mask_1: 1.707  loss_dice_1: 2.688  loss_ce_2: 3.481  loss_mask_2: 1.812  loss_dice_2: 2.702  loss_ce_3: 3.472  loss_mask_3: 1.694  loss_dice_3: 2.767  loss_ce_4: 3.536  loss_mask_4: 1.892  loss_dice_4: 2.836  loss_ce_5: 3.45  loss_mask_5: 1.855  loss_dice_5: 2.889  loss_ce_6: 3.543  loss_mask_6: 1.842  loss_dice_6: 2.856  loss_ce_7: 3.534  loss_mask_7: 1.933  loss_dice_7: 2.702  loss_ce_8: 3.356  loss_mask_8: 1.845  loss_dice_8: 2.878    time: 0.4364  last_time: 0.4364  data_time: 0.0241  last_data_time: 0.0233   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 09:13:22 d2.utils.events]: \u001b[0m eta: 1 day, 19:46:29  iter: 4879  total_loss: 80.16  loss_ce: 3.592  loss_mask: 1.759  loss_dice: 2.721  loss_ce_0: 3.636  loss_mask_0: 1.702  loss_dice_0: 2.839  loss_ce_1: 3.552  loss_mask_1: 1.766  loss_dice_1: 2.736  loss_ce_2: 3.451  loss_mask_2: 1.713  loss_dice_2: 2.683  loss_ce_3: 3.365  loss_mask_3: 1.733  loss_dice_3: 2.685  loss_ce_4: 3.494  loss_mask_4: 1.717  loss_dice_4: 2.695  loss_ce_5: 3.442  loss_mask_5: 1.757  loss_dice_5: 2.764  loss_ce_6: 3.45  loss_mask_6: 1.854  loss_dice_6: 2.758  loss_ce_7: 3.447  loss_mask_7: 1.866  loss_dice_7: 2.665  loss_ce_8: 3.293  loss_mask_8: 1.855  loss_dice_8: 2.729    time: 0.4364  last_time: 0.4320  data_time: 0.0242  last_data_time: 0.0247   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 09:13:31 d2.utils.events]: \u001b[0m eta: 1 day, 19:46:43  iter: 4899  total_loss: 78.81  loss_ce: 3.624  loss_mask: 1.825  loss_dice: 2.375  loss_ce_0: 3.568  loss_mask_0: 2.045  loss_dice_0: 2.618  loss_ce_1: 3.442  loss_mask_1: 1.851  loss_dice_1: 2.367  loss_ce_2: 3.465  loss_mask_2: 1.888  loss_dice_2: 2.438  loss_ce_3: 3.544  loss_mask_3: 1.974  loss_dice_3: 2.414  loss_ce_4: 3.552  loss_mask_4: 1.922  loss_dice_4: 2.481  loss_ce_5: 3.472  loss_mask_5: 1.989  loss_dice_5: 2.38  loss_ce_6: 3.521  loss_mask_6: 1.933  loss_dice_6: 2.285  loss_ce_7: 3.429  loss_mask_7: 2.063  loss_dice_7: 2.622  loss_ce_8: 3.401  loss_mask_8: 2.081  loss_dice_8: 2.548    time: 0.4364  last_time: 0.4376  data_time: 0.0263  last_data_time: 0.0271   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 09:13:40 d2.utils.events]: \u001b[0m eta: 1 day, 19:46:50  iter: 4919  total_loss: 78.2  loss_ce: 3.424  loss_mask: 1.736  loss_dice: 2.779  loss_ce_0: 3.485  loss_mask_0: 1.726  loss_dice_0: 2.971  loss_ce_1: 3.529  loss_mask_1: 1.628  loss_dice_1: 2.709  loss_ce_2: 3.493  loss_mask_2: 1.681  loss_dice_2: 2.838  loss_ce_3: 3.431  loss_mask_3: 1.716  loss_dice_3: 2.859  loss_ce_4: 3.484  loss_mask_4: 1.788  loss_dice_4: 2.911  loss_ce_5: 3.433  loss_mask_5: 1.852  loss_dice_5: 2.722  loss_ce_6: 3.457  loss_mask_6: 1.838  loss_dice_6: 2.881  loss_ce_7: 3.429  loss_mask_7: 1.689  loss_dice_7: 2.867  loss_ce_8: 3.432  loss_mask_8: 1.571  loss_dice_8: 2.832    time: 0.4364  last_time: 0.4314  data_time: 0.0227  last_data_time: 0.0240   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 09:13:49 d2.utils.events]: \u001b[0m eta: 1 day, 19:46:48  iter: 4939  total_loss: 83.04  loss_ce: 3.626  loss_mask: 1.92  loss_dice: 2.671  loss_ce_0: 3.736  loss_mask_0: 1.914  loss_dice_0: 2.748  loss_ce_1: 3.522  loss_mask_1: 1.936  loss_dice_1: 2.64  loss_ce_2: 3.559  loss_mask_2: 1.957  loss_dice_2: 2.588  loss_ce_3: 3.606  loss_mask_3: 2.062  loss_dice_3: 2.851  loss_ce_4: 3.64  loss_mask_4: 2.002  loss_dice_4: 2.67  loss_ce_5: 3.569  loss_mask_5: 2.038  loss_dice_5: 2.704  loss_ce_6: 3.685  loss_mask_6: 1.948  loss_dice_6: 2.61  loss_ce_7: 3.615  loss_mask_7: 1.959  loss_dice_7: 2.746  loss_ce_8: 3.558  loss_mask_8: 1.855  loss_dice_8: 2.648    time: 0.4364  last_time: 0.4393  data_time: 0.0240  last_data_time: 0.0210   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 09:13:57 d2.utils.events]: \u001b[0m eta: 1 day, 19:46:51  iter: 4959  total_loss: 76.63  loss_ce: 3.341  loss_mask: 1.817  loss_dice: 2.383  loss_ce_0: 3.451  loss_mask_0: 1.67  loss_dice_0: 2.686  loss_ce_1: 3.229  loss_mask_1: 1.735  loss_dice_1: 2.482  loss_ce_2: 3.277  loss_mask_2: 1.745  loss_dice_2: 2.436  loss_ce_3: 3.135  loss_mask_3: 2.031  loss_dice_3: 2.737  loss_ce_4: 3.284  loss_mask_4: 1.932  loss_dice_4: 2.635  loss_ce_5: 3.354  loss_mask_5: 1.777  loss_dice_5: 2.577  loss_ce_6: 3.394  loss_mask_6: 1.962  loss_dice_6: 2.541  loss_ce_7: 3.304  loss_mask_7: 2.06  loss_dice_7: 2.67  loss_ce_8: 3.237  loss_mask_8: 1.845  loss_dice_8: 2.775    time: 0.4364  last_time: 0.4302  data_time: 0.0225  last_data_time: 0.0228   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 09:14:06 d2.utils.events]: \u001b[0m eta: 1 day, 19:46:58  iter: 4979  total_loss: 77.16  loss_ce: 3.051  loss_mask: 2.047  loss_dice: 2.629  loss_ce_0: 3.197  loss_mask_0: 1.87  loss_dice_0: 2.639  loss_ce_1: 3.139  loss_mask_1: 1.812  loss_dice_1: 2.471  loss_ce_2: 3.039  loss_mask_2: 1.99  loss_dice_2: 2.576  loss_ce_3: 3.016  loss_mask_3: 1.901  loss_dice_3: 2.67  loss_ce_4: 2.978  loss_mask_4: 2.149  loss_dice_4: 2.659  loss_ce_5: 3.013  loss_mask_5: 1.888  loss_dice_5: 2.593  loss_ce_6: 3.055  loss_mask_6: 2.048  loss_dice_6: 2.586  loss_ce_7: 2.988  loss_mask_7: 2  loss_dice_7: 2.605  loss_ce_8: 2.903  loss_mask_8: 1.966  loss_dice_8: 2.664    time: 0.4364  last_time: 0.4310  data_time: 0.0223  last_data_time: 0.0212   lr: 1e-05  max_mem: 6731M\n",
      "\u001b[32m[07/17 09:14:16 d2.data.build]: \u001b[0mMaking batched data loader with batch_size=1\n",
      "\u001b[4m\u001b[5m\u001b[31mERROR\u001b[0m \u001b[32m[07/17 09:14:16 d2.engine.train_loop]: \u001b[0mException during training:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/stefan.steinheber/.conda/envs/pytorch/lib/python3.12/site-packages/detectron2/engine/train_loop.py\", line 156, in train\n",
      "    self.after_step()\n",
      "  File \"/home/stefan.steinheber/.conda/envs/pytorch/lib/python3.12/site-packages/detectron2/engine/train_loop.py\", line 190, in after_step\n",
      "    h.after_step()\n",
      "  File \"/home/stefan.steinheber/.conda/envs/pytorch/lib/python3.12/site-packages/detectron2/engine/hooks.py\", line 556, in after_step\n",
      "    self._do_eval()\n",
      "  File \"/home/stefan.steinheber/.conda/envs/pytorch/lib/python3.12/site-packages/detectron2/engine/hooks.py\", line 529, in _do_eval\n",
      "    results = self._func()\n",
      "              ^^^^^^^^^^^^\n",
      "  File \"/home/stefan.steinheber/.conda/envs/pytorch/lib/python3.12/site-packages/detectron2/engine/defaults.py\", line 457, in test_and_save_results\n",
      "    self._last_eval_results = self.test(self.cfg, self.model)\n",
      "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/stefan.steinheber/.conda/envs/pytorch/lib/python3.12/site-packages/detectron2/engine/defaults.py\", line 613, in test\n",
      "    evaluator = cls.build_evaluator(cfg, dataset_name)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/stefan.steinheber/bachelor/processing/leaf_segmentation/lib/Mask2Former/train_net.py\", line 91, in build_evaluator\n",
      "    evaluator_list.append(COCOEvaluator(dataset_name, output_dir=output_folder))\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/stefan.steinheber/.conda/envs/pytorch/lib/python3.12/site-packages/detectron2/evaluation/coco_evaluation.py\", line 146, in __init__\n",
      "    self._coco_api = COCO(json_file)\n",
      "                     ^^^^^^^^^^^^^^^\n",
      "  File \"/home/stefan.steinheber/.conda/envs/pytorch/lib/python3.12/site-packages/pycocotools/coco.py\", line 81, in __init__\n",
      "    with open(annotation_file, 'r') as f:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'datasets/coco/annotations/instances_val2017.json'\n",
      "\u001b[32m[07/17 09:14:16 d2.engine.hooks]: \u001b[0mOverall training speed: 4997 iterations in 0:36:21 (0.4365 s / it)\n",
      "\u001b[32m[07/17 09:14:16 d2.engine.hooks]: \u001b[0mTotal training time: 0:36:25 (0:00:04 on hooks)\n",
      "\u001b[32m[07/17 09:14:16 d2.utils.events]: \u001b[0m eta: 1 day, 19:46:18  iter: 4999  total_loss: 74.05  loss_ce: 2.626  loss_mask: 1.956  loss_dice: 2.58  loss_ce_0: 2.864  loss_mask_0: 2.265  loss_dice_0: 2.803  loss_ce_1: 2.506  loss_mask_1: 1.982  loss_dice_1: 2.541  loss_ce_2: 2.396  loss_mask_2: 2.097  loss_dice_2: 2.719  loss_ce_3: 2.573  loss_mask_3: 2.074  loss_dice_3: 2.844  loss_ce_4: 2.52  loss_mask_4: 1.919  loss_dice_4: 2.794  loss_ce_5: 2.517  loss_mask_5: 2.178  loss_dice_5: 2.703  loss_ce_6: 2.497  loss_mask_6: 1.967  loss_dice_6: 2.703  loss_ce_7: 2.527  loss_mask_7: 2.184  loss_dice_7: 2.728  loss_ce_8: 2.617  loss_mask_8: 1.911  loss_dice_8: 2.549    time: 0.4364  last_time: 0.4472  data_time: 0.0215  last_data_time: 0.0171   lr: 1e-05  max_mem: 6731M\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'datasets/coco/annotations/instances_val2017.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m mask2former\u001b[38;5;241m.\u001b[39madd_maskformer2_config(cfg)\n\u001b[1;32m      4\u001b[0m cfg\u001b[38;5;241m.\u001b[39mmerge_from_file(CONFIG)\n\u001b[0;32m----> 6\u001b[0m launch(get_trainer, \u001b[38;5;241m1\u001b[39m, args\u001b[38;5;241m=\u001b[39m(cfg,))\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.12/site-packages/detectron2/engine/launch.py:84\u001b[0m, in \u001b[0;36mlaunch\u001b[0;34m(main_func, num_gpus_per_machine, num_machines, machine_rank, dist_url, args, timeout)\u001b[0m\n\u001b[1;32m     69\u001b[0m     mp\u001b[38;5;241m.\u001b[39mstart_processes(\n\u001b[1;32m     70\u001b[0m         _distributed_worker,\n\u001b[1;32m     71\u001b[0m         nprocs\u001b[38;5;241m=\u001b[39mnum_gpus_per_machine,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     81\u001b[0m         daemon\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     82\u001b[0m     )\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 84\u001b[0m     main_func(\u001b[38;5;241m*\u001b[39margs)\n",
      "Cell \u001b[0;32mIn[6], line 4\u001b[0m, in \u001b[0;36mget_trainer\u001b[0;34m(cfg)\u001b[0m\n\u001b[1;32m      2\u001b[0m trainer \u001b[38;5;241m=\u001b[39m LeavesTrainer(cfg)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#trainer.resume_or_load(resume=args.resume)\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.12/site-packages/detectron2/engine/defaults.py:488\u001b[0m, in \u001b[0;36mDefaultTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    481\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    482\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    483\u001b[0m \u001b[38;5;124;03m    Run training.\u001b[39;00m\n\u001b[1;32m    484\u001b[0m \n\u001b[1;32m    485\u001b[0m \u001b[38;5;124;03m    Returns:\u001b[39;00m\n\u001b[1;32m    486\u001b[0m \u001b[38;5;124;03m        OrderedDict of results, if evaluation is enabled. Otherwise None.\u001b[39;00m\n\u001b[1;32m    487\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 488\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mtrain(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart_iter, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_iter)\n\u001b[1;32m    489\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mTEST\u001b[38;5;241m.\u001b[39mEXPECTED_RESULTS) \u001b[38;5;129;01mand\u001b[39;00m comm\u001b[38;5;241m.\u001b[39mis_main_process():\n\u001b[1;32m    490\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\n\u001b[1;32m    491\u001b[0m             \u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_last_eval_results\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    492\u001b[0m         ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo evaluation results obtained during training!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.12/site-packages/detectron2/engine/train_loop.py:156\u001b[0m, in \u001b[0;36mTrainerBase.train\u001b[0;34m(self, start_iter, max_iter)\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbefore_step()\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_step()\n\u001b[0;32m--> 156\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mafter_step()\n\u001b[1;32m    157\u001b[0m \u001b[38;5;66;03m# self.iter == max_iter can be used by `after_train` to\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;66;03m# tell whether the training successfully finished or failed\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;66;03m# due to exceptions.\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.12/site-packages/detectron2/engine/train_loop.py:190\u001b[0m, in \u001b[0;36mTrainerBase.after_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mafter_step\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m h \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hooks:\n\u001b[0;32m--> 190\u001b[0m         h\u001b[38;5;241m.\u001b[39mafter_step()\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.12/site-packages/detectron2/engine/hooks.py:556\u001b[0m, in \u001b[0;36mEvalHook.after_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_period \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m next_iter \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_period \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    554\u001b[0m     \u001b[38;5;66;03m# do the last eval in after_train\u001b[39;00m\n\u001b[1;32m    555\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m next_iter \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mmax_iter:\n\u001b[0;32m--> 556\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_eval()\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.12/site-packages/detectron2/engine/hooks.py:529\u001b[0m, in \u001b[0;36mEvalHook._do_eval\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_do_eval\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 529\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_func()\n\u001b[1;32m    531\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m results:\n\u001b[1;32m    532\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m    533\u001b[0m             results, \u001b[38;5;28mdict\u001b[39m\n\u001b[1;32m    534\u001b[0m         ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEval function must return a dict. Got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(results)\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.12/site-packages/detectron2/engine/defaults.py:457\u001b[0m, in \u001b[0;36mDefaultTrainer.build_hooks.<locals>.test_and_save_results\u001b[0;34m()\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtest_and_save_results\u001b[39m():\n\u001b[0;32m--> 457\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_eval_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[1;32m    458\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_eval_results\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.12/site-packages/detectron2/engine/defaults.py:613\u001b[0m, in \u001b[0;36mDefaultTrainer.test\u001b[0;34m(cls, cfg, model, evaluators)\u001b[0m\n\u001b[1;32m    611\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    612\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 613\u001b[0m         evaluator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mbuild_evaluator(cfg, dataset_name)\n\u001b[1;32m    614\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m:\n\u001b[1;32m    615\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    616\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    617\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor implement its `build_evaluator` method.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    618\u001b[0m         )\n",
      "File \u001b[0;32m~/bachelor/processing/leaf_segmentation/lib/Mask2Former/train_net.py:91\u001b[0m, in \u001b[0;36mTrainer.build_evaluator\u001b[0;34m(cls, cfg, dataset_name, output_folder)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# instance segmentation\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m evaluator_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoco\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 91\u001b[0m     evaluator_list\u001b[38;5;241m.\u001b[39mappend(COCOEvaluator(dataset_name, output_dir\u001b[38;5;241m=\u001b[39moutput_folder))\n\u001b[1;32m     92\u001b[0m \u001b[38;5;66;03m# panoptic segmentation\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m evaluator_type \u001b[38;5;129;01min\u001b[39;00m [\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoco_panoptic_seg\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124made20k_panoptic_seg\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcityscapes_panoptic_seg\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmapillary_vistas_panoptic_seg\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     98\u001b[0m ]:\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.12/site-packages/detectron2/evaluation/coco_evaluation.py:146\u001b[0m, in \u001b[0;36mCOCOEvaluator.__init__\u001b[0;34m(self, dataset_name, tasks, distributed, output_dir, max_dets_per_image, use_fast_impl, kpt_oks_sigmas, allow_cached_coco)\u001b[0m\n\u001b[1;32m    144\u001b[0m json_file \u001b[38;5;241m=\u001b[39m PathManager\u001b[38;5;241m.\u001b[39mget_local_path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metadata\u001b[38;5;241m.\u001b[39mjson_file)\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mredirect_stdout(io\u001b[38;5;241m.\u001b[39mStringIO()):\n\u001b[0;32m--> 146\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_coco_api \u001b[38;5;241m=\u001b[39m COCO(json_file)\n\u001b[1;32m    148\u001b[0m \u001b[38;5;66;03m# Test set json files do not contain annotations (evaluation must be\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;66;03m# performed using the COCO evaluation server).\u001b[39;00m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_evaluation \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mannotations\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_coco_api\u001b[38;5;241m.\u001b[39mdataset\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.12/site-packages/pycocotools/coco.py:81\u001b[0m, in \u001b[0;36mCOCO.__init__\u001b[0;34m(self, annotation_file)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloading annotations into memory...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     80\u001b[0m tic \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 81\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(annotation_file, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     82\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(dataset)\u001b[38;5;241m==\u001b[39m\u001b[38;5;28mdict\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mannotation file format \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m not supported\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mtype\u001b[39m(dataset))\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'datasets/coco/annotations/instances_val2017.json'"
     ]
    }
   ],
   "source": [
    "cfg = get_cfg()\n",
    "add_deeplab_config(cfg)\n",
    "mask2former.add_maskformer2_config(cfg)\n",
    "cfg.merge_from_file(CONFIG)\n",
    "\n",
    "launch(get_trainer, 1, args=(cfg,))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
