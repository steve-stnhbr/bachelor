{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "tags": []
            },
            "outputs": [],
            "source": [
                "! pip install kaggle &> /dev/null\n",
                "! pip install torch torchvision &> /dev/null\n",
                "! pip install opencv-python pycocotools matplotlib onnxruntime onnx &> /dev/null\n",
                "! pip install git+https://github.com/facebookresearch/segment-anything.git &> /dev/null\n",
                "! wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth &> /dev/null"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {
                "tags": []
            },
            "outputs": [],
            "source": [
                "from pathlib import Path\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import cv2"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {
                "tags": []
            },
            "outputs": [],
            "source": [
                "MASK_DIR = \"_data/combined/train/leaf_instances\"\n",
                "RGB_DIR = \"_data/combined/train/images\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {
                "tags": []
            },
            "outputs": [],
            "source": [
                "model_type = 'vit_b'\n",
                "checkpoint = 'sam_vit_b_01ec64.pth'\n",
                "device = 'cuda:0'\n",
                "num_classes = 2"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {
                "tags": []
            },
            "outputs": [],
            "source": [
                "\n",
                "import torch\n",
                "import torchvision\n",
                "from segment_anything import sam_model_registry, SamAutomaticMaskGenerator\n",
                "# Custom SAM with ResNet50 classifier\n",
                "class CustomSAM(torch.nn.Module):\n",
                "    def __init__(self, sam, mask_generator, num_classes):\n",
                "        super().__init__()\n",
                "        self.sam = sam\n",
                "        self.mask_generator = mask_generator\n",
                "        self.classifier = torchvision.models.resnet50(pretrained=False)\n",
                "        self.classifier.fc = torch.nn.Linear(self.classifier.fc.in_features, num_classes)\n",
                "\n",
                "    def forward(self, image):\n",
                "        # Generate masks\n",
                "        masks = self.mask_generator.generate(image)\n",
                "        \n",
                "        # Get image embeddings from SAM\n",
                "        with torch.no_grad():\n",
                "            image_embeddings = self.sam.image_encoder(image)\n",
                "        \n",
                "        # Use classifier on image embeddings\n",
                "        class_output = self.classifier(image_embeddings)\n",
                "        \n",
                "        return masks, class_output"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "\n",
                "sam = sam_model_registry[model_type](checkpoint=checkpoint)\n",
                "# Create mask generator\n",
                "mask_generator = SamAutomaticMaskGenerator(sam)\n",
                "custom_sam = CustomSAM(sam, mask_generator, num_classes)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 57,
            "metadata": {
                "tags": []
            },
            "outputs": [],
            "source": [
                "from torch.utils.data import Dataset\n",
                "# Preprocess the images\n",
                "from collections import defaultdict\n",
                "\n",
                "import torch\n",
                "\n",
                "from segment_anything.utils.transforms import ResizeLongestSide\n",
                "import os\n",
                "\n",
                "transform = ResizeLongestSide(sam.image_encoder.img_size)\n",
                "\n",
                "class LeafInstanceDataset(Dataset):\n",
                "    def __init__(self, path):\n",
                "        self.path = path\n",
                "        self.files = os.listdir(os.path.join(path, \"images\"))\n",
                "        \n",
                "    def __len__(self):\n",
                "        return len(self.files)\n",
                "    \n",
                "    def __getitem__(self, idx):\n",
                "        img_path = os.path.join(self.path, \"images\", self.files[idx])\n",
                "        mask_path =  os.path.join(self.path, \"leaf_instances\", self.files[idx])\n",
                "        mask_im = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
                "        mask = np.array(mask_im)\n",
                "\n",
                "        unique_categories = np.unique(mask)\n",
                "        unique_categories = unique_categories[unique_categories > 0]  # Exclude background (assumed to be 0)\n",
                "        \n",
                "        image = cv2.imread(img_path)\n",
                "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
                "        input_image = transform.apply_image(image)\n",
                "        input_image_torch = torch.as_tensor(input_image, device=device)\n",
                "        transformed_image = input_image_torch.permute(2, 0, 1).contiguous()[None, :, :, :]\n",
                "\n",
                "        input_image = sam_model.preprocess(transformed_image)\n",
                "        original_image_size = image.shape[:2]\n",
                "        input_size = tuple(transformed_image.shape[-2:])\n",
                "        \n",
                "        data = {}\n",
                "        data[\"image\"] = input_image\n",
                "        data[\"original_image_size\"] = original_image_size\n",
                "        data[\"input_size\"] = input_size\n",
                "        data[\"bboxes\"] = []\n",
                "        data[\"masks\"] = []\n",
                "        data[\"bboxes_transformed\"]\n",
                "\n",
                "        for category_id in unique_categories:\n",
                "            y, x = np.nonzero(mask)\n",
                "            x_min = np.min(x)\n",
                "            y_min = np.min(y)\n",
                "            x_max = np.max(x)\n",
                "            y_max = np.max(y)\n",
                "            bboxes = np.array([x_min, y_min, x_max, y_max])\n",
                "            mask = (mask == category_id).squeeze()\n",
                "            data[\"bboxes\"].append(bboxes)\n",
                "            data[\"bboxes_transformed\"].append(transform.apply_boxes(bboxes, original_image_size))\n",
                "            data[\"masks\"].append(mask)\n",
                "            data[\"masks_transformed\"].append(transform.apply_image(mask, original_image_size))\n",
                "            \n",
                "        return data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 44,
            "metadata": {
                "tags": []
            },
            "outputs": [],
            "source": [
                "# Set up the optimizer, hyperparameter tuning will improve performance here\n",
                "lr = 1e-4\n",
                "wd = 0\n",
                "optimizer = torch.optim.Adam(sam.mask_decoder.parameters(), lr=lr, weight_decay=wd)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from torchvision.ops import box_iou\n",
                "\n",
                "def custom_loss(generated_masks, class_output, true_masks, true_bboxes, true_classes):\n",
                "    # Class loss\n",
                "    class_loss = torch.nn.functional.cross_entropy(class_output, true_classes)\n",
                "    \n",
                "    # Mask and bbox loss\n",
                "    mask_loss = 0\n",
                "    iou_threshold = 0.5\n",
                "    \n",
                "    for gen_mask in generated_masks:\n",
                "        gen_bbox = torch.tensor(gen_mask['bbox'])\n",
                "        ious = box_iou(gen_bbox.unsqueeze(0), true_bboxes)\n",
                "        best_iou, best_idx = ious.max(dim=1)\n",
                "        \n",
                "        if best_iou > iou_threshold:\n",
                "            # Calculate mask IoU\n",
                "            gen_mask_tensor = torch.tensor(gen_mask['segmentation'])\n",
                "            true_mask_tensor = true_masks[best_idx]\n",
                "            mask_iou = (gen_mask_tensor & true_mask_tensor).sum() / (gen_mask_tensor | true_mask_tensor).sum()\n",
                "            mask_loss += 1 - mask_iou\n",
                "\n",
                "    mask_loss = mask_loss / len(generated_masks) if generated_masks else torch.tensor(0.)\n",
                "    \n",
                "    total_loss = class_loss + mask_loss\n",
                "    return total_loss"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "tags": []
            },
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Loss: 73.42185974121094:   0%|          | 39/10010 [00:19<1:17:59,  2.13it/s]"
                    ]
                }
            ],
            "source": [
                "from statistics import mean\n",
                "\n",
                "from tqdm import tqdm\n",
                "from torch.nn.functional import threshold, normalize\n",
                "\n",
                "from torch.utils.data import DataLoader\n",
                "\n",
                "num_epochs = 100\n",
                "losses = []\n",
                "data_loader = DataLoader(LeafInstanceDataset(\"_data/combined/train\"))\n",
                "\n",
                "for epoch in range(num_epochs):\n",
                "    epoch_losses = []\n",
                "    p_bar = tqdm(data_loader)\n",
                "    for data in p_bar:\n",
                "        optimizer.zero_grad()\n",
                "        \n",
                "        image = data[\"image\"]\n",
                "        true_masks = torch.stack(data[\"masks\"])\n",
                "        true_bboxes = torch.tensor(data[\"bboxes_transformed\"])\n",
                "        true_classes = torch.tensor([your_class_mapping[mask.sum() > 0] for mask in data[\"masks\"]])\n",
                "        \n",
                "        generated_masks, class_output = custom_sam(image)\n",
                "        loss = custom_loss(generated_masks, class_output, true_masks, true_bboxes, true_classes)\n",
                "        loss.backward()\n",
                "        optimizer.step()\n",
                "    losses.append(epoch_losses)\n",
                "    print(f'EPOCH: {epoch}')\n",
                "    print(f'Mean loss: {mean(epoch_losses)}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "mean_losses = [mean(x) for x in losses]\n",
                "mean_losses\n",
                "\n",
                "plt.plot(list(range(len(mean_losses))), mean_losses)\n",
                "plt.title('Mean epoch loss')\n",
                "plt.xlabel('Epoch Number')\n",
                "plt.ylabel('Loss')\n",
                "\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Set up predictors for both tuned and original models\n",
                "from segment_anything import sam_model_registry, SamPredictor\n",
                "predictor_tuned = SamPredictor(sam_model)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import random\n",
                "image_file = random.sample(os.listdir(\"_data/combined/test\"))\n",
                "image_file = os.path.join(\"_data/combined/test\", image_file)\n",
                "image = cv2.imread(image_file)\n",
                "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "predictor_tuned.set_image(image)\n",
                "\n",
                "masks_tuned, _, _ = predictor_tuned.predict(\n",
                "    point_coords=None,\n",
                "    box=None,\n",
                "    multimask_output=True,\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Helper functions provided in https://github.com/facebookresearch/segment-anything/blob/9e8f1309c94f1128a6e5c047a10fdcb02fc8d651/notebooks/predictor_example.ipynb\n",
                "def show_mask(mask, ax, random_color=False):\n",
                "    if random_color:\n",
                "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
                "    else:\n",
                "        color = np.array([30/255, 144/255, 255/255, 0.6])\n",
                "    h, w = mask.shape[-2:]\n",
                "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
                "    ax.imshow(mask_image)\n",
                "\n",
                "def show_box(box, ax):\n",
                "    x0, y0 = box[0], box[1]\n",
                "    w, h = box[2] - box[0], box[3] - box[1]\n",
                "    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0,0,0,0), lw=2))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "axs[0].imshow(image)\n",
                "show_mask(masks_tuned, axs[0])\n",
                "show_box(input_bbox, axs[0])\n",
                "axs[0].set_title('Mask with Tuned Model', fontsize=26)\n",
                "axs[0].axis('off')\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "PyTorch w/ CUDA 12.4",
            "language": "python",
            "name": "pytorch_cuda_12.4"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.4"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
