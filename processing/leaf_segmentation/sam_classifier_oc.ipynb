{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from IPython.display import Image, display\n",
    "import numpy as np\n",
    "import os\n",
    "from os.path import join\n",
    "from PIL import ImageFile\n",
    "import pandas as pd\n",
    "from matplotlib import cm\n",
    "import seaborn as sns\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense, Flatten, GlobalAveragePooling2D\n",
    "from tensorflow.python.keras.applications.resnet50 import preprocess_input\n",
    "from tensorflow.python.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, roc_auc_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn import svm\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "import re\n",
    "\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "plt.style.use('fivethirtyeight')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare images for resnet50\n",
    "image_size = 224\n",
    "\n",
    "def read_and_prep_images(img_paths, img_height=image_size, img_width=image_size):\n",
    "    imgs = [load_img(img_path, target_size=(img_height, img_width)) for img_path in img_paths]\n",
    "    img_array = np.array([img_to_array(img) for img in imgs])\n",
    "    #output = img_array\n",
    "    output = preprocess_input(img_array)\n",
    "    return(output)\n",
    "\n",
    "X_train = read_and_prep_images(train_img_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get features from resnet50 \n",
    "\n",
    "resnet_weights_path = '../input/resnet50/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
    "\n",
    "# X : images numpy array\n",
    "resnet_model = ResNet50(input_shape=(image_size, image_size, 3), weights=resnet_weights_path, include_top=False, pooling='avg')  # Since top layer is the fc layer used for predictions\n",
    "\n",
    "X_train = resnet_model.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply standard scaler to output from resnet50\n",
    "ss = StandardScaler()\n",
    "ss.fit(X_train)\n",
    "X_train = ss.transform(X_train)\n",
    "\n",
    "# Take PCA to reduce feature space dimensionality\n",
    "pca = PCA(n_components=512, whiten=True)\n",
    "pca = pca.fit(X_train)\n",
    "print('Explained variance percentage = %0.2f' % sum(pca.explained_variance_ratio_))\n",
    "X_train = pca.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train classifier and obtain predictions for OC-SVM\n",
    "oc_svm_clf = svm.OneClassSVM(gamma=0.001, kernel='rbf', nu=0.08)  # Obtained using grid search\n",
    "if_clf = IsolationForest(contamination=0.08, max_features=1.0, max_samples=1.0, n_estimators=40)  # Obtained using grid search\n",
    "\n",
    "oc_svm_clf.fit(X_train)\n",
    "if_clf.fit(X_train)\n",
    "\n",
    "oc_svm_preds = oc_svm_clf.predict(X_train) #TODO: add test data\n",
    "\n",
    "# Further compute accuracy, precision and recall for the two predictions sets obtained"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
