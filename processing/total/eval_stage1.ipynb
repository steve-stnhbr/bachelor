{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2cd75e20-cb2a-4d4c-a2d8-8b153aebd377",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import tqdm\n",
    "import random\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fd41159c-82b5-46f9-a56d-9bd811efca3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "EVAL_AMOUNT = 10000\n",
    "DATASET_DIR = \"_data/plant_pathology\"\n",
    "INT_S1_DIR = \"_intermediate/stage1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "852b632b-b013-41b1-ba62-3b24af56d684",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.makedirs(INT_S1_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "77b2c724-47cb-48b7-bd21-dcc4dcc37cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(os.path.join(DATASET_DIR, \"train.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "669cf64e-5b47-4d53-8b84-527a7e1723f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if EVAL_AMOUNT > len(train_data.index):\n",
    "    indices = list(train_data.index)\n",
    "else:\n",
    "    indices = random.sample(list(train_data.index), k=EVAL_AMOUNT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ee8233ad-0a77-4c1e-ad7e-35c55969c608",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_patches(masks, image, apply_mask=False, padding=0):\n",
    "    result = []\n",
    "    \n",
    "    for mask in masks:\n",
    "        if apply_mask:\n",
    "            image_tmp = image * (mask[\"segmentation\"][:, :, np.newaxis])\n",
    "        else:\n",
    "            image_tmp = image\n",
    "        \n",
    "        bbox = mask[\"bbox\"]\n",
    "        x0 = bbox[1]-padding\n",
    "        if x0 < 0:\n",
    "            x0 = 0\n",
    "        x1 = bbox[1]+bbox[3]+padding\n",
    "        if x1 >= image.shape[0]:\n",
    "            x1 = image.shape[0] - 1\n",
    "        y0 = bbox[0]-padding\n",
    "        if y0 < 0:\n",
    "            y0 = 0\n",
    "        y1 = bbox[0]+bbox[2]+padding\n",
    "        if y1 >= image.shape[1]:\n",
    "            y1 = image.shape[1] - 1\n",
    "        \n",
    "        patch = image_tmp[x0:x1, y0:y1]\n",
    "        #mask['patch'] = patch\n",
    "        \n",
    "        if 0 in patch.shape:\n",
    "            continue\n",
    "        result.append(patch)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "bad5d090-c53b-4987-8f94-cd6cca19dc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "10ed62de-2bf4-4558-93cb-7b1c6df5da39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from segment_anything import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor\n",
    "\n",
    "sam_checkpoint = \"sam_vit_h_4b8939.pth\"\n",
    "model_type = \"vit_h\"\n",
    "\n",
    "sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n",
    "sam.to(device=device)\n",
    "\n",
    "def sam_generate_mask(image):\n",
    "    mask_generator = SamAutomaticMaskGenerator(sam)\n",
    "    masks = mask_generator.generate(image)\n",
    "    return masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "78bdf896-ee62-4f44-b5a3-571204f9a7bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stefan.steinheber/.conda/envs/pt_12.4/lib/python3.12/site-packages/torchvision/transforms/v2/_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "class BinaryResnetClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=1):\n",
    "        super(BinaryResnetClassifier, self).__init__()\n",
    "        # Load a pre-trained ResNet model\n",
    "        self.resnet = resnet50(ResNet50_Weights.IMAGENET1K_V1) \n",
    "        # Modify the last fully connected layer\n",
    "        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, num_classes)\n",
    "        nn.init.xavier_normal_(self.resnet.fc.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass the input through the ResNet\n",
    "        x = self.resnet(x)\n",
    "        return x\n",
    "    \n",
    "import torchvision.transforms.v2 as transforms\n",
    "\n",
    "tf = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.ToDtype(torch.float32, scale=True),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  \n",
    "])\n",
    "\n",
    "\n",
    "resnet = torch.load(\"../leaf_segmentation/out/leaf_classifier/resnet/resnet_latest.pth\")\n",
    "resnet = resnet.to(device)\n",
    "\n",
    "def s1_sam_resnet(image):\n",
    "    masks = sam_generate_mask(image)\n",
    "    patches = get_patches(masks, image)\n",
    "    from PIL import Image\n",
    "    results = []\n",
    "    with torch.no_grad():\n",
    "        for i, patch in enumerate(patches):\n",
    "            input = tf(Image.fromarray(patch)).unsqueeze(0).to(device)\n",
    "            result = torch.sigmoid(resnet(input)).cpu().item()\n",
    "\n",
    "            results.append(result)\n",
    "            masks[i][\"patch\"] = patch\n",
    "            masks[i][\"leaf_probability\"] = result\n",
    "    PROBABILITY_THRESHOLD = .5\n",
    "    masks_filtered = [mask for mask, result in zip(masks,results) if result > PROBABILITY_THRESHOLD]\n",
    "    return masks_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "1259a2bf-896f-4787-8f2f-1099d4d3cb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO, checks\n",
    "model = YOLO(\"../leaf_segmentation/out/yolo_urban_street/train/weights/best.pt\")\n",
    "\n",
    "def s1_sam_yolo(image):\n",
    "    masks = sam_generate_mask(image)\n",
    "    patches = get_patches(masks, image)\n",
    "    results_yolo = []\n",
    "    for i, patch in enumerate(patches):\n",
    "        result = model.predict(patch, verbose=False)\n",
    "        # retrieve leaf (class 1) porbability\n",
    "        prob = result[0].boxes.conf\n",
    "        if len(prob) == 1:\n",
    "            prob = prob.item()\n",
    "        else:\n",
    "            prob = 0\n",
    "        results_yolo.append(prob)\n",
    "        masks[i][\"patch\"] = patch\n",
    "        # TODO: update probability assignment\n",
    "        masks[i][\"leaf_probability\"] = prob\n",
    "    masks_filtered = [mask for mask, result in zip(masks,results_yolo) if result > .8]\n",
    "    return masks_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "83ca455c-920a-4a8b-8b61-3f0d631392e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "stage1_dict = {\n",
    "    \"SAM + YOLOv8\": s1_sam_yolo,\n",
    "    \"SAM + ResNet\": s1_sam_resnet,\n",
    "#    \"Mask R-CNN\": s1_mask_rcnn\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "eca72354-fcaf-4bd5-ba49-e6b0f3f8ee3c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model SAM + YOLOv8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SAM + YOLOv8:   6%|â–Œ         | 105/1821 [13:54<3:47:19,  7.95s/it]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 18.06 MiB is free. Including non-PyTorch memory, this process has 6.88 GiB memory in use. Process 2607513 has 32.58 GiB memory in use. Of the allocated memory 6.20 GiB is allocated by PyTorch, and 170.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[78], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m img \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(DATASET_DIR, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m\"\u001b[39m, train_data\u001b[38;5;241m.\u001b[39mloc[index][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage_id\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 13\u001b[0m     leaf_masks \u001b[38;5;241m=\u001b[39m stage1_model(img)\n\u001b[1;32m     14\u001b[0m     stage1_results[stage1_name][index][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmasks\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m leaf_masks\n\u001b[1;32m     15\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n",
      "Cell \u001b[0;32mIn[76], line 5\u001b[0m, in \u001b[0;36ms1_sam_yolo\u001b[0;34m(image)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21ms1_sam_yolo\u001b[39m(image):\n\u001b[0;32m----> 5\u001b[0m     masks \u001b[38;5;241m=\u001b[39m sam_generate_mask(image)\n\u001b[1;32m      6\u001b[0m     patches \u001b[38;5;241m=\u001b[39m get_patches(masks, image)\n\u001b[1;32m      7\u001b[0m     results_yolo \u001b[38;5;241m=\u001b[39m []\n",
      "Cell \u001b[0;32mIn[65], line 11\u001b[0m, in \u001b[0;36msam_generate_mask\u001b[0;34m(image)\u001b[0m\n\u001b[1;32m      7\u001b[0m model_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvit_h\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     10\u001b[0m sam \u001b[38;5;241m=\u001b[39m sam_model_registry[model_type](checkpoint\u001b[38;5;241m=\u001b[39msam_checkpoint)\n\u001b[0;32m---> 11\u001b[0m sam\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m     13\u001b[0m mask_generator \u001b[38;5;241m=\u001b[39m SamAutomaticMaskGenerator(sam)\n\u001b[1;32m     14\u001b[0m masks \u001b[38;5;241m=\u001b[39m mask_generator\u001b[38;5;241m.\u001b[39mgenerate(image)\n",
      "File \u001b[0;32m~/.conda/envs/pt_12.4/lib/python3.12/site-packages/torch/nn/modules/module.py:1320\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1317\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1318\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1320\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply(convert)\n",
      "File \u001b[0;32m~/.conda/envs/pt_12.4/lib/python3.12/site-packages/torch/nn/modules/module.py:899\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    897\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    898\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 899\u001b[0m         module\u001b[38;5;241m.\u001b[39m_apply(fn)\n\u001b[1;32m    901\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    902\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    903\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    904\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    909\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    910\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/pt_12.4/lib/python3.12/site-packages/torch/nn/modules/module.py:926\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    922\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    923\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    924\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    925\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 926\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m fn(param)\n\u001b[1;32m    927\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    929\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/pt_12.4/lib/python3.12/site-packages/torch/nn/modules/module.py:1306\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1299\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1300\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1301\u001b[0m             device,\n\u001b[1;32m   1302\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1303\u001b[0m             non_blocking,\n\u001b[1;32m   1304\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1305\u001b[0m         )\n\u001b[0;32m-> 1306\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1307\u001b[0m         device,\n\u001b[1;32m   1308\u001b[0m         dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1309\u001b[0m         non_blocking,\n\u001b[1;32m   1310\u001b[0m     )\n\u001b[1;32m   1311\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1312\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 18.06 MiB is free. Including non-PyTorch memory, this process has 6.88 GiB memory in use. Process 2607513 has 32.58 GiB memory in use. Of the allocated memory 6.20 GiB is allocated by PyTorch, and 170.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "stage1_results = {}\n",
    "for stage1_name, stage1_model in stage1_dict.items():\n",
    "    print(f\"Running model {stage1_name}\")\n",
    "    stage1_results[stage1_name] = {}\n",
    "    for index in tqdm.tqdm(indices, desc=stage1_name):\n",
    "        gt_healthy = bool(train_data.loc[index][\"healthy\"])\n",
    "        stage1_results[stage1_name][index] = {\n",
    "            'healthy': gt_healthy,\n",
    "            'masks': []\n",
    "        }\n",
    "        img = cv2.imread(os.path.join(DATASET_DIR, \"images\", train_data.loc[index][\"image_id\"] + \".jpg\"))\n",
    "        with torch.no_grad():\n",
    "            leaf_masks = stage1_model(img)\n",
    "            stage1_results[stage1_name][index]['masks'] = leaf_masks\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9a2d1dbf-935e-48f2-9d81-d20eddc1357f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "for stage1_name, stage1_result in stage1_results.items():\n",
    "    os.makedirs(os.path.join(INT_S1_DIR, stage1_name), exist_ok=True)\n",
    "    with open(os.path.join(INT_S1_DIR, stage1_name, \"data.pkl\"), \"wb+\") as file:\n",
    "        pickle.dump(stage1_result, file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open(os.path.join(INT_S1_DIR, \"total_data.pkl\"), \"wb+\") as file:\n",
    "    pickle.dump(stage1_results, file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6b12117c-d155-4d35-a421-8af546a66751",
   "metadata": {},
   "outputs": [],
   "source": [
    "for stage1_name, stage1_result in stage1_results.items():\n",
    "    patches_dir = os.path.join(INT_S1_DIR, stage1_name, \"patches\")\n",
    "    os.makedirs(patches_dir, exist_ok=True)\n",
    "    for index, data in stage1_result.items():\n",
    "        for i, leaf_mask in enumerate(data['masks']):\n",
    "            cv2.imwrite(os.path.join(patches_dir, f\"patch_{index}_{i}.png\"), leaf_mask['patch'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99933272-214a-44bb-820d-21cdcf01bcba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch w/ CUDA 12.4",
   "language": "python",
   "name": "pytorch_cuda_12.4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
