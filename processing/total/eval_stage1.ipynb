{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2cd75e20-cb2a-4d4c-a2d8-8b153aebd377",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import tqdm\n",
    "import glob\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd41159c-82b5-46f9-a56d-9bd811efca3d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_intermediate/stage1_plantdoc_pil_masked/2024_12_12_11_45_16\n"
     ]
    }
   ],
   "source": [
    "now = datetime.now()\n",
    "\n",
    "USE_BEFORE = False\n",
    "USE_GENERATED = False\n",
    "\n",
    "EVAL_AMOUNT = 512\n",
    "#DATASET_DIR = \"_data/plant_pathology\"\n",
    "DATASET_DIR = \"_data/plantdoc_csv\"\n",
    "SUBDIR = \"stage1_plantdoc_pil_masked\"\n",
    "INT_S1_DIR = f\"_intermediate/{SUBDIR}/{now.strftime('%Y_%m_%d_%H_%M_%S')}\"\n",
    "if USE_GENERATED:\n",
    "    INT_S1_DIR = sorted(glob.glob(f\"_intermediate/{SUBDIR}/*\"))[-1]\n",
    "PATCHES_DIR = os.path.join(INT_S1_DIR, \"patches\")\n",
    "print(INT_S1_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "852b632b-b013-41b1-ba62-3b24af56d684",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.makedirs(INT_S1_DIR, exist_ok=True)\n",
    "os.makedirs(PATCHES_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77b2c724-47cb-48b7-bd21-dcc4dcc37cf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>healthy</th>\n",
       "      <th>file_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>image_0</td>\n",
       "      <td>True</td>\n",
       "      <td>train_grape_leaf_22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>image_1</td>\n",
       "      <td>True</td>\n",
       "      <td>train_grape_leaf_30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>image_2</td>\n",
       "      <td>True</td>\n",
       "      <td>train_grape_leaf_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>image_3</td>\n",
       "      <td>True</td>\n",
       "      <td>train_grape_leaf_11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>image_4</td>\n",
       "      <td>True</td>\n",
       "      <td>train_grape_leaf_59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2917</th>\n",
       "      <td>image_2917</td>\n",
       "      <td>False</td>\n",
       "      <td>test_Tomato_leaf_bacterial_spot_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2918</th>\n",
       "      <td>image_2918</td>\n",
       "      <td>False</td>\n",
       "      <td>test_Tomato_leaf_bacterial_spot_8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2919</th>\n",
       "      <td>image_2919</td>\n",
       "      <td>False</td>\n",
       "      <td>test_Tomato_leaf_bacterial_spot_7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2920</th>\n",
       "      <td>image_2920</td>\n",
       "      <td>False</td>\n",
       "      <td>test_Tomato_leaf_bacterial_spot_4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2921</th>\n",
       "      <td>image_2921</td>\n",
       "      <td>False</td>\n",
       "      <td>test_Tomato_leaf_bacterial_spot_1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2922 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        image_id  healthy                          file_name\n",
       "0        image_0     True                train_grape_leaf_22\n",
       "1        image_1     True                train_grape_leaf_30\n",
       "2        image_2     True                 train_grape_leaf_1\n",
       "3        image_3     True                train_grape_leaf_11\n",
       "4        image_4     True                train_grape_leaf_59\n",
       "...          ...      ...                                ...\n",
       "2917  image_2917    False  test_Tomato_leaf_bacterial_spot_2\n",
       "2918  image_2918    False  test_Tomato_leaf_bacterial_spot_8\n",
       "2919  image_2919    False  test_Tomato_leaf_bacterial_spot_7\n",
       "2920  image_2920    False  test_Tomato_leaf_bacterial_spot_4\n",
       "2921  image_2921    False  test_Tomato_leaf_bacterial_spot_1\n",
       "\n",
       "[2922 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv(os.path.join(DATASET_DIR, \"data.csv\"))\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6aa3063-f345-4b4a-bf25-07dc075c14e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if USE_GENERATED:\n",
    "    files = os.listdir(PATCHES_DIR)\n",
    "    if EVAL_AMOUNT > len(files):\n",
    "        indices = train_data.index[train_data[\"image_id\"].isin(files)].tolist()\n",
    "    else:\n",
    "        file_samples = random.sample(files, k=EVAL_AMOUNT)\n",
    "        indices = train_data.index[train_data[\"image_id\"].isin(file_samples)].tolist()\n",
    "else:\n",
    "    if EVAL_AMOUNT > len(train_data.index):\n",
    "        indices = list(train_data.index)\n",
    "    else:\n",
    "        indices = random.sample(list(train_data.index), k=EVAL_AMOUNT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee8233ad-0a77-4c1e-ad7e-35c55969c608",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_patches(masks, image, apply_mask=False, padding=0):\n",
    "    result = []\n",
    "    \n",
    "    for mask in masks:\n",
    "        if apply_mask:\n",
    "            image_tmp = image * (mask[\"segmentation\"][:, :, np.newaxis])\n",
    "        else:\n",
    "            image_tmp = image\n",
    "        \n",
    "        bbox = mask[\"bbox\"]\n",
    "        x0 = bbox[1]-padding\n",
    "        if x0 < 0:\n",
    "            x0 = 0\n",
    "        x1 = bbox[1]+bbox[3]+padding\n",
    "        if x1 >= image.shape[0]:\n",
    "            x1 = image.shape[0] - 1\n",
    "        y0 = bbox[0]-padding\n",
    "        if y0 < 0:\n",
    "            y0 = 0\n",
    "        y1 = bbox[0]+bbox[2]+padding\n",
    "        if y1 >= image.shape[1]:\n",
    "            y1 = image.shape[1] - 1\n",
    "   \n",
    "        x0 = int(x0)\n",
    "        x1 = int(x1)\n",
    "        y0 = int(y0)\n",
    "        y1 = int(y1)\n",
    "\n",
    "        try:\n",
    "            patch = image_tmp[x0:x1, y0:y1]\n",
    "        except:\n",
    "            print(x0, x1, y0, y1, type(x0), type(x1), type(y0), type(y1)) \n",
    "\n",
    "        #mask['patch'] = patch\n",
    "        \n",
    "        if 0 in patch.shape:\n",
    "            continue\n",
    "        result.append(patch)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b0ce753-bb50-42ea-9033-75339c36e5a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_patches_file(image_id):\n",
    "    patches = []\n",
    "    for file in glob.glob(os.path.join(PATCHES_DIR, image_id, \"*.png\")):\n",
    "        #patches.append(cv2.imread(file))\n",
    "        patches.append(Image.open(file))\n",
    "    return patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8e319a4-cc32-4559-8093-134660e7a3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_masks_file(image_id):\n",
    "    with open(os.path.join(PATCHES_DIR, image_id, \"data.pkl\"), 'rb') as file:\n",
    "        data = pickle.load(file)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bad5d090-c53b-4987-8f94-cd6cca19dc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "10ed62de-2bf4-4558-93cb-7b1c6df5da39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stefan.steinheber/.conda/envs/pt_12.4/lib/python3.12/site-packages/segment_anything/build_sam.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(f)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from segment_anything import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor\n",
    "\n",
    "sam_checkpoint = \"sam_vit_h_4b8939.pth\"\n",
    "model_type = \"vit_h\"\n",
    "\n",
    "sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n",
    "sam.to(device=device)\n",
    "\n",
    "def sam_generate_mask(image):\n",
    "    mask_generator = SamAutomaticMaskGenerator(sam)\n",
    "    masks = mask_generator.generate(image)\n",
    "    return masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8154a416-92ad-4925-ae01-8842029f33e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BinaryResnetClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=1):\n",
    "        super(BinaryResnetClassifier, self).__init__()\n",
    "        # Load a pre-trained ResNet model\n",
    "        self.resnet = resnet50(ResNet50_Weights.IMAGENET1K_V1)  # You can choose any ResNet variant\n",
    "        # Modify the last fully connected layer\n",
    "        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, num_classes)\n",
    "        nn.init.xavier_normal_(self.resnet.fc.weight)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Pass the input through the ResNet\n",
    "        x = self.resnet(x)\n",
    "        # Apply the sigmoid activation function\n",
    "#        x = torch.sigmoid(x)  # Output will be between 0 and 1\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ff8ed842-e974-4252-b405-9efaa27cc23d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BinaryInceptionClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=1):\n",
    "        super(BinaryInceptionClassifier, self).__init__()\n",
    "        # Load a pre-trained ResNet model\n",
    "        self.inception = torch.hub.load('pytorch/vision:v0.10.0', 'inception_v3', pretrained=True) # You can choose any ResNet variant\n",
    "        # Modify the last fully connected layer\n",
    "        self.inception.fc = nn.Linear(self.inception.fc.in_features, num_classes)\n",
    "        nn.init.xavier_normal_(self.inception.fc.weight)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Pass the input through the ResNet\n",
    "        x = self.inception(x)\n",
    "        # Apply the sigmoid activation function\n",
    "#        x = torch.sigmoid(x)  # Output will be between 0 and 1\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4ab6e2a6-6615-4c5d-8cd9-00df3378a16a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=16, latent_dim=200, act_fn=nn.ReLU()):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, padding=1, stride=2),  # 112x112\n",
    "            act_fn,\n",
    "            nn.Conv2d(out_channels, 2*out_channels, 3, padding=1, stride=2),  # 56x56\n",
    "            act_fn,\n",
    "            nn.Conv2d(2*out_channels, 4*out_channels, 3, padding=1, stride=2),  # 28x28\n",
    "            act_fn,\n",
    "            nn.Conv2d(4*out_channels, 8*out_channels, 3, padding=1, stride=2),  # 14x14\n",
    "            act_fn,\n",
    "            nn.Conv2d(8*out_channels, 16*out_channels, 3, padding=1, stride=2),  # 7x7\n",
    "            act_fn,\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(16*out_channels*7*7, latent_dim),\n",
    "            act_fn\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=16, latent_dim=200, act_fn=nn.ReLU()):\n",
    "        super().__init__()\n",
    "        self.out_channels = out_channels\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 16*out_channels*7*7),\n",
    "            act_fn\n",
    "        )\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.ConvTranspose2d(16*out_channels, 8*out_channels, 3, stride=2, padding=1, output_padding=1),  # 14x14\n",
    "            act_fn,\n",
    "            nn.ConvTranspose2d(8*out_channels, 4*out_channels, 3, stride=2, padding=1, output_padding=1),  # 28x28\n",
    "            act_fn,\n",
    "            nn.ConvTranspose2d(4*out_channels, 2*out_channels, 3, stride=2, padding=1, output_padding=1),  # 56x56\n",
    "            act_fn,\n",
    "            nn.ConvTranspose2d(2*out_channels, out_channels, 3, stride=2, padding=1, output_padding=1),  # 112x112\n",
    "            act_fn,\n",
    "            nn.ConvTranspose2d(out_channels, in_channels, 3, stride=2, padding=1, output_padding=1),  # 224x224\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.linear(x)\n",
    "        output = output.view(-1, 16*self.out_channels, 7, 7)\n",
    "        output = self.conv(output)\n",
    "        return output\n",
    "\n",
    "#  defining autoencoder\n",
    "class BigAutoencoder(nn.Module):\n",
    "    def __init__(self, encoder=Encoder(), decoder=Decoder()):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "#        self.encoder.to(device)\n",
    "\n",
    "        self.decoder = decoder\n",
    "#        self.decoder.to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "78bdf896-ee62-4f44-b5a3-571204f9a7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from segment_anything import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor\n",
    "from ultralytics import YOLO\n",
    "import torchvision.transforms.v2 as transforms\n",
    "\n",
    "yolo = YOLO(\"../leaf_segmentation/out/yolo_urban_street/train/weights/best.pt\")\n",
    "yolo_syn = YOLO(\"../leaf_segmentation/out/yolo_synthetic/train4/weights/best.pt\")\n",
    "\n",
    "resnet = torch.load(\"../leaf_segmentation/out/leaf_classifier/resnet_masked_cos/resnet_best.pth\")\n",
    "resnet = resnet.to(device)\n",
    "resnet.eval()\n",
    "\n",
    "resnet_transform = transforms.Compose([\n",
    "    transforms.ToImage(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToDtype(torch.float32, scale=True),\n",
    "    #transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  \n",
    "])\n",
    "\n",
    "inception = torch.load(\"../leaf_segmentation/out/leaf_classifier/inception/inception_best.pth\")\n",
    "inception = inception.to(device)\n",
    "inception.eval()\n",
    "\n",
    "inception_transform = transforms.Compose([\n",
    "    transforms.ToImage(),\n",
    "    transforms.Resize((320, 320)),\n",
    "    transforms.ToDtype(torch.float32, scale=True),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  \n",
    "])\n",
    "\n",
    "sam_checkpoint = \"sam_vit_h_4b8939.pth\"\n",
    "model_type = \"vit_h\"\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n",
    "sam.to(device=device)\n",
    "\n",
    "mask_generator = SamAutomaticMaskGenerator(sam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5f7a9af2-acd7-4352-8350-c587ee4ef378",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict_sam(img, pred, image_id):\n",
    "    masks = get_masks_file(image_id)\n",
    "    mask_result = []\n",
    "    \n",
    "    for i, mask in enumerate(masks):\n",
    "        patch = mask['patch']\n",
    "        if 0 in patch.shape:\n",
    "            del masks[i]\n",
    "        _, prob = pred(patch)\n",
    "        mask['leaf_probability'] = float(prob)\n",
    "    return masks\n",
    "    \n",
    "\n",
    "def pred_resnet(x):\n",
    "    x = resnet_transform(x).to(device).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        out = resnet(x)\n",
    "        sig = torch.sigmoid(out).item()\n",
    "        return sig < 0.025, 1 - sig    \n",
    "\n",
    "def s1_sam_resnet(img, image_id):\n",
    "    return predict_sam(img, pred_resnet, image_id)\n",
    "\n",
    "\n",
    "def pred_inception(x):\n",
    "    x = inception_transform(x).to(device).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        out = inception(x)\n",
    "        sig = torch.sigmoid(out).item()\n",
    "        return sig < 0.01, 1 - sig\n",
    "\n",
    "def s1_sam_inception(img, image_id):\n",
    "    return predict_sam(img, pred_inception, image_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1259a2bf-896f-4787-8f2f-1099d4d3cb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO, checks\n",
    "model = YOLO(\"../leaf_segmentation/out/yolo_urban_street/train/weights/best.pt\")\n",
    "\n",
    "def s1_sam_yolo(image, image_id):\n",
    "#    masks = sam_generate_mask(image)\n",
    "#    patches = get_patches(masks, image)\n",
    "#    patches = get_patches_file(image_id)\n",
    "    results_yolo = []\n",
    "    masks = get_masks_file(image_id)\n",
    "    for i, mask in enumerate(masks):\n",
    "        result = model.predict(mask['patch'], verbose=False)\n",
    "        # retrieve leaf (class 1) porbability\n",
    "        prob = result[0].boxes.conf\n",
    "        if len(prob) == 1:\n",
    "            prob = prob.item()\n",
    "        else:\n",
    "            prob = 0\n",
    "        results_yolo.append(prob)\n",
    "        mask['leaf_probability'] = prob\n",
    "    masks_filtered = [mask for mask in masks if mask['leaf_probability'] > .8]\n",
    "    return masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "42d46fc7-81c8-47d6-873a-ea7eec5cff30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_seg = YOLO(\"../leaf_segmentation/out/yolo_synthetic/train4/weights/best.pt\")\n",
    "\n",
    "def s1_yolo(image, image_id):\n",
    "    masks_result = []\n",
    "    result = model.predict(image, verbose=False, retina_masks=True)[0]\n",
    "    if result.masks is None:\n",
    "        return []\n",
    "    masks = result.masks.data\n",
    "    boxes = result.boxes.data\n",
    "    names = list(result.names.values())\n",
    "    \n",
    "    classes = boxes[:, 5]\n",
    "    \n",
    "    for i, name in enumerate(names):\n",
    "        obj_indices = torch.where(classes == i)\n",
    "        obj_masks = masks[obj_indices]\n",
    "        obj_masks = torch.nn.functional.interpolate(obj_masks.unsqueeze(0), size=image.shape[:2], mode='bilinear', align_corners=False).squeeze(0)\n",
    "        prob = result[0].boxes.conf\n",
    "        \n",
    "        segmentations = [seg.cpu().numpy() for seg in torch.unbind(obj_masks)]\n",
    "        \n",
    "        for i, seg in enumerate(segmentations):\n",
    "            patch = image * seg[:, :, np.newaxis].astype(np.uint8)\n",
    "            coords = cv2.findNonZero(seg)  # Returns all non-zero points\n",
    "            x, y, w, h = cv2.boundingRect(coords)  # Get bounding box\n",
    "            \n",
    "            patch = patch[y:y+h, x:x+w]\n",
    "            masks_result.append({\n",
    "                \"segmentation\": seg,\n",
    "                \"leaf_probability\": result.boxes.conf.cpu().numpy()[i],\n",
    "                \"patch\": patch\n",
    "            })\n",
    "    return masks_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154d3172-31ea-4dc8-a01e-84a052199ed3",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating patches:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 285/512 [19:58<18:16,  4.83s/it]"
     ]
    }
   ],
   "source": [
    "for index in tqdm.tqdm(indices, desc=\"Generating patches\"):\n",
    "    img_id = train_data.loc[index][\"image_id\"]\n",
    "    try:\n",
    "        os.makedirs(os.path.join(PATCHES_DIR, img_id))\n",
    "    except:\n",
    "        if USE_GENERATED:\n",
    "            continue\n",
    "#    img = cv2.imread(os.path.join(DATASET_DIR, \"images\", img_id + \".jpg\"))\n",
    "#    img = cv2.resize(img, (640, 640))\n",
    "    img = Image.open(os.path.join(DATASET_DIR, \"images\", img_id + \".jpg\"))\n",
    "    img = img.resize((640, 640))\n",
    "    img = np.array(img)\n",
    "    masks = sam_generate_mask(img)\n",
    "    patches = get_patches(masks, img, apply_mask=True, padding=10)\n",
    "    \n",
    "    for d, item in zip(masks, patches):\n",
    "        d['patch'] = item\n",
    "    with open(os.path.join(PATCHES_DIR, img_id, \"data.pkl\"), 'wb+') as file:\n",
    "        pickle.dump(masks, file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    #for i, patch in enumerate(patches):\n",
    "    #    cv2.imwrite(os.path.join(PATCHES_DIR, img_id, f\"patch{i}.png\"), patch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "83ca455c-920a-4a8b-8b61-3f0d631392e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "stage1_dict = {\n",
    "    \"SAM + ResNet\": s1_sam_resnet,\n",
    "    \"SAM + YOLOv8\": s1_sam_yolo,\n",
    "    \"SAM + Inception\": s1_sam_inception\n",
    "    #\"YOLOv8\": s1_yolo,\n",
    "    #\"Mask R-CNN\": s1_mask_rcnn\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "eca72354-fcaf-4bd5-ba49-e6b0f3f8ee3c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model SAM + ResNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SAM + ResNet:   0%|          | 2/512 [00:02<12:10,  1.43s/it]\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stage1_results = {}\n",
    "\n",
    "for stage1_name, stage1_model in stage1_dict.items():\n",
    "    print(f\"Running model {stage1_name}\")\n",
    "    if USE_BEFORE and os.path.exists(os.path.join(INT_S1_DIR, stage1_name, \"data.pkl\")):\n",
    "        print(\"Data already exists, skipping!\")\n",
    "        continue\n",
    "    stage1_results[stage1_name] = {}\n",
    "    for index in tqdm.tqdm(indices, desc=stage1_name):\n",
    "        gt_healthy = bool(train_data.loc[index][\"healthy\"])\n",
    "        stage1_results[stage1_name][index] = {\n",
    "            'healthy': gt_healthy,\n",
    "            'masks': []\n",
    "        }\n",
    "        img = cv2.imread(os.path.join(DATASET_DIR, \"images\", train_data.loc[index][\"image_id\"] + \".jpg\"))\n",
    "        with torch.no_grad():\n",
    "            leaf_masks = stage1_model(img, train_data.loc[index][\"image_id\"])\n",
    "            stage1_results[stage1_name][index]['masks'] = leaf_masks\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    os.makedirs(os.path.join(INT_S1_DIR, stage1_name), exist_ok=True)\n",
    "    with open(os.path.join(INT_S1_DIR, stage1_name, \"data.pkl\"), \"wb+\") as file:\n",
    "        pickle.dump(stage1_results[stage1_name], file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107bf11f-9256-4e67-b86b-933ec47bbc9f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(os.path.join(INT_S1_DIR, \"total_data.pkl\"), \"wb+\") as file:\n",
    "    pickle.dump(stage1_results, file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2d1dbf-935e-48f2-9d81-d20eddc1357f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "for stage1_name, stage1_result in stage1_results.items():\n",
    "    os.makedirs(os.path.join(INT_S1_DIR, stage1_name), exist_ok=True)\n",
    "    with open(os.path.join(INT_S1_DIR, stage1_name, \"data.pkl\"), \"wb+\") as file:\n",
    "        pickle.dump(stage1_result, file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b12117c-d155-4d35-a421-8af546a66751",
   "metadata": {},
   "outputs": [],
   "source": [
    "for stage1_name, stage1_result in stage1_results.items():\n",
    "    patches_dir = os.path.join(INT_S1_DIR, stage1_name, \"patches\")\n",
    "    for index, data in stage1_result.items():\n",
    "        image_dir = os.path.join(patches_dir, str(index))\n",
    "        os.makedirs(image_dir, exist_ok=True)\n",
    "        for i, leaf_mask in enumerate(data['masks']):\n",
    "            cv2.imwrite(os.path.join(image_dir, f\"patch_{i}.png\"), leaf_mask['patch'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99933272-214a-44bb-820d-21cdcf01bcba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch w/ CUDA 12.4",
   "language": "python",
   "name": "pytorch_cuda_12.4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
