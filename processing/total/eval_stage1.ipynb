{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2cd75e20-cb2a-4d4c-a2d8-8b153aebd377",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import tqdm\n",
    "import glob\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd41159c-82b5-46f9-a56d-9bd811efca3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "now = datetime.now()\n",
    "\n",
    "EVAL_AMOUNT = 512\n",
    "#DATASET_DIR = \"_data/plant_pathology\"\n",
    "DATASET_DIR = \"_data/plantdoc_csv\"\n",
    "INT_S1_DIR = f\"_intermediate/stage1_plantdoc/{now.strftime('%Y_%m_%d_%H_%M_%S')}\"\n",
    "PATCHES_DIR = os.path.join(INT_S1_DIR, \"patches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "852b632b-b013-41b1-ba62-3b24af56d684",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.makedirs(INT_S1_DIR, exist_ok=True)\n",
    "os.makedirs(PATCHES_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77b2c724-47cb-48b7-bd21-dcc4dcc37cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(os.path.join(DATASET_DIR, \"data.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "669cf64e-5b47-4d53-8b84-527a7e1723f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if EVAL_AMOUNT > len(train_data.index):\n",
    "    indices = list(train_data.index)\n",
    "else:\n",
    "    indices = random.sample(list(train_data.index), k=EVAL_AMOUNT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee8233ad-0a77-4c1e-ad7e-35c55969c608",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_patches(masks, image, apply_mask=False, padding=0):\n",
    "    result = []\n",
    "    \n",
    "    for mask in masks:\n",
    "        if apply_mask:\n",
    "            image_tmp = image * (mask[\"segmentation\"][:, :, np.newaxis])\n",
    "        else:\n",
    "            image_tmp = image\n",
    "        \n",
    "        bbox = mask[\"bbox\"]\n",
    "        x0 = bbox[1]-padding\n",
    "        if x0 < 0:\n",
    "            x0 = 0\n",
    "        x1 = bbox[1]+bbox[3]+padding\n",
    "        if x1 >= image.shape[0]:\n",
    "            x1 = image.shape[0] - 1\n",
    "        y0 = bbox[0]-padding\n",
    "        if y0 < 0:\n",
    "            y0 = 0\n",
    "        y1 = bbox[0]+bbox[2]+padding\n",
    "        if y1 >= image.shape[1]:\n",
    "            y1 = image.shape[1] - 1\n",
    "   \n",
    "        x0 = int(x0)\n",
    "        x1 = int(x1)\n",
    "        y0 = int(y0)\n",
    "        y1 = int(y1)\n",
    "\n",
    "        try:\n",
    "            patch = image_tmp[x0:x1, y0:y1]\n",
    "        except:\n",
    "            print(x0, x1, y0, y1, type(x0), type(x1), type(y0), type(y1)) \n",
    "\n",
    "        #mask['patch'] = patch\n",
    "        \n",
    "        if 0 in patch.shape:\n",
    "            continue\n",
    "        result.append(patch)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b0ce753-bb50-42ea-9033-75339c36e5a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_patches_file(image_id):\n",
    "    patches = []\n",
    "    for file in glob.glob(os.path.join(PATCHES_DIR, image_id, \"*.png\")):\n",
    "        patches.append(cv2.imread(file))\n",
    "    return patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bad5d090-c53b-4987-8f94-cd6cca19dc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10ed62de-2bf4-4558-93cb-7b1c6df5da39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stefan.steinheber/.conda/envs/pt_12.4/lib/python3.12/site-packages/segment_anything/build_sam.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(f)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from segment_anything import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor\n",
    "\n",
    "sam_checkpoint = \"sam_vit_h_4b8939.pth\"\n",
    "model_type = \"vit_h\"\n",
    "\n",
    "sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n",
    "sam.to(device=device)\n",
    "\n",
    "def sam_generate_mask(image):\n",
    "    mask_generator = SamAutomaticMaskGenerator(sam)\n",
    "    masks = mask_generator.generate(image)\n",
    "    return masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "78bdf896-ee62-4f44-b5a3-571204f9a7bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stefan.steinheber/.conda/envs/pt_12.4/lib/python3.12/site-packages/torchvision/transforms/v2/_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_1130605/2377007833.py:32: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  resnet = torch.load(\"../leaf_segmentation/out/leaf_classifier/resnet/resnet_latest.pth\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "class BinaryResnetClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=1):\n",
    "        super(BinaryResnetClassifier, self).__init__()\n",
    "        # Load a pre-trained ResNet model\n",
    "        self.resnet = resnet50(ResNet50_Weights.IMAGENET1K_V1) \n",
    "        # Modify the last fully connected layer\n",
    "        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, num_classes)\n",
    "        nn.init.xavier_normal_(self.resnet.fc.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass the input through the ResNet\n",
    "        x = self.resnet(x)\n",
    "        return x\n",
    "    \n",
    "import torchvision.transforms.v2 as transforms\n",
    "\n",
    "tf = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.ToDtype(torch.float32, scale=True),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  \n",
    "])\n",
    "\n",
    "\n",
    "resnet = torch.load(\"../leaf_segmentation/out/leaf_classifier/resnet/resnet_latest.pth\")\n",
    "resnet = resnet.to(device)\n",
    "\n",
    "def s1_sam_resnet(image, image_id):\n",
    "    #masks = sam_generate_mask(image)\n",
    "    #patches = get_patches(masks, image)\n",
    "    patches = get_patches_file(image_id)\n",
    "    from PIL import Image\n",
    "    results = []\n",
    "    masks = [{}] * len(patches)\n",
    "    with torch.no_grad():\n",
    "        for i, patch in enumerate(patches):\n",
    "            input = tf(Image.fromarray(patch)).unsqueeze(0).to(device)\n",
    "            result = torch.sigmoid(resnet(input)).cpu().item()\n",
    "\n",
    "            results.append(result)\n",
    "            masks[i][\"patch\"] = patch\n",
    "            masks[i][\"leaf_probability\"] = result\n",
    "    PROBABILITY_THRESHOLD = .5\n",
    "    masks_filtered = [mask for mask, result in zip(masks,results) if result > PROBABILITY_THRESHOLD]\n",
    "    return masks_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1259a2bf-896f-4787-8f2f-1099d4d3cb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO, checks\n",
    "model = YOLO(\"../leaf_segmentation/out/yolo_urban_street/train/weights/best.pt\")\n",
    "\n",
    "def s1_sam_yolo(image, image_id):\n",
    "#    masks = sam_generate_mask(image)\n",
    "#    patches = get_patches(masks, image)\n",
    "    patches = get_patches_file(image_id)\n",
    "    results_yolo = []\n",
    "    masks = []\n",
    "    for i, patch in enumerate(patches):\n",
    "        result = model.predict(patch, verbose=False)\n",
    "        # retrieve leaf (class 1) porbability\n",
    "        prob = result[0].boxes.conf\n",
    "        if len(prob) == 1:\n",
    "            prob = prob.item()\n",
    "        else:\n",
    "            prob = 0\n",
    "        results_yolo.append(prob)\n",
    "        masks.append({ 'patch':patch, 'leaf_probability': prob})\n",
    "    masks_filtered = [mask for mask in masks if mask['leaf_probability'] > .8]\n",
    "    return masks_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "42d46fc7-81c8-47d6-873a-ea7eec5cff30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_seg = YOLO(\"../leaf_segmentation/out/yolo_synthetic/train4/weights/best.pt\")\n",
    "\n",
    "def s1_yolo(image, image_id):\n",
    "    result = model.predict(image)[0]\n",
    "    print(result)\n",
    "    print(result.masks)\n",
    "    return masks_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "154d3172-31ea-4dc8-a01e-84a052199ed3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating patches: 100%|██████████| 512/512 [23:23<00:00,  2.74s/it]\n"
     ]
    }
   ],
   "source": [
    "for index in tqdm.tqdm(indices, desc=\"Generating patches\"):\n",
    "    img_id = train_data.loc[index][\"image_id\"]\n",
    "    img = cv2.imread(os.path.join(DATASET_DIR, \"images\", img_id + \".jpg\"))\n",
    "    img = cv2.resize(img, (640, 640))\n",
    "    masks = sam_generate_mask(img)\n",
    "    patches = get_patches(masks, img, apply_mask=False)\n",
    "    os.makedirs(os.path.join(PATCHES_DIR, img_id), exist_ok=True)\n",
    "    for i, patch in enumerate(patches):\n",
    "        cv2.imwrite(os.path.join(PATCHES_DIR, img_id, f\"patch{i}.png\"), patch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "83ca455c-920a-4a8b-8b61-3f0d631392e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "stage1_dict = {\n",
    "    \"SAM + YOLOv8\": s1_sam_yolo,\n",
    "    \"SAM + ResNet\": s1_sam_resnet,\n",
    "#    \"YOLOv8\": s1_yolo,\n",
    "#    \"Mask R-CNN\": s1_mask_rcnn\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca72354-fcaf-4bd5-ba49-e6b0f3f8ee3c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model SAM + YOLOv8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SAM + YOLOv8: 100%|██████████| 512/512 [04:29<00:00,  1.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model SAM + ResNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SAM + ResNet:  40%|███▉      | 204/512 [02:34<05:58,  1.16s/it]"
     ]
    }
   ],
   "source": [
    "stage1_results = {}\n",
    "\n",
    "for stage1_name, stage1_model in stage1_dict.items():\n",
    "    print(f\"Running model {stage1_name}\")\n",
    "    stage1_results[stage1_name] = {}\n",
    "    for index in tqdm.tqdm(indices, desc=stage1_name):\n",
    "        gt_healthy = bool(train_data.loc[index][\"healthy\"])\n",
    "        stage1_results[stage1_name][index] = {\n",
    "            'healthy': gt_healthy,\n",
    "            'masks': []\n",
    "        }\n",
    "        img = cv2.imread(os.path.join(DATASET_DIR, \"images\", train_data.loc[index][\"image_id\"] + \".jpg\"))\n",
    "        with torch.no_grad():\n",
    "            leaf_masks = stage1_model(img, train_data.loc[index][\"image_id\"])\n",
    "            stage1_results[stage1_name][index]['masks'] = leaf_masks\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "107bf11f-9256-4e67-b86b-933ec47bbc9f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(os.path.join(INT_S1_DIR, \"total_data.pkl\"), \"wb+\") as file:\n",
    "    pickle.dump(stage1_results, file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2d1dbf-935e-48f2-9d81-d20eddc1357f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "for stage1_name, stage1_result in stage1_results.items():\n",
    "    os.makedirs(os.path.join(INT_S1_DIR, stage1_name), exist_ok=True)\n",
    "    with open(os.path.join(INT_S1_DIR, stage1_name, \"data.pkl\"), \"wb+\") as file:\n",
    "        pickle.dump(stage1_result, file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b12117c-d155-4d35-a421-8af546a66751",
   "metadata": {},
   "outputs": [],
   "source": [
    "for stage1_name, stage1_result in stage1_results.items():\n",
    "    patches_dir = os.path.join(INT_S1_DIR, stage1_name, \"patches\")\n",
    "    for index, data in stage1_result.items():\n",
    "        image_dir = os.path.join(patches_dir, str(index))\n",
    "        os.makedirs(image_dir, exist_ok=True)\n",
    "        for i, leaf_mask in enumerate(data['masks']):\n",
    "            cv2.imwrite(os.path.join(image_dir, f\"patch_{i}.png\"), leaf_mask['patch'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99933272-214a-44bb-820d-21cdcf01bcba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch w/ CUDA 12.4",
   "language": "python",
   "name": "pytorch_cuda_12.4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
