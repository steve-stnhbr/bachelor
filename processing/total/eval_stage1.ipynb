{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2cd75e20-cb2a-4d4c-a2d8-8b153aebd377",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import tqdm\n",
    "import glob\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fd41159c-82b5-46f9-a56d-9bd811efca3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "now = datetime.now()\n",
    "\n",
    "USE_BEFORE = True\n",
    "USE_GENERATED = True\n",
    "\n",
    "EVAL_AMOUNT = 512\n",
    "DATASET_DIR = \"_data/plant_pathology\"\n",
    "#DATASET_DIR = \"_data/plantdoc_csv\"\n",
    "INT_S1_DIR = f\"_intermediate/stage1_pathology_masked/{now.strftime('%Y_%m_%d_%H_%M_%S')}\"\n",
    "if USE_GENERATED:\n",
    "    INT_S1_DIR = sorted(glob.glob(\"_intermediate/stage1_pathology_masked/*\"))[-1]\n",
    "PATCHES_DIR = os.path.join(INT_S1_DIR, \"patches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "852b632b-b013-41b1-ba62-3b24af56d684",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.makedirs(INT_S1_DIR, exist_ok=True)\n",
    "os.makedirs(PATCHES_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77b2c724-47cb-48b7-bd21-dcc4dcc37cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(os.path.join(DATASET_DIR, \"data.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "669cf64e-5b47-4d53-8b84-527a7e1723f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if EVAL_AMOUNT > len(train_data.index):\n",
    "    indices = list(train_data.index)\n",
    "else:\n",
    "    indices = random.sample(list(train_data.index), k=EVAL_AMOUNT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee8233ad-0a77-4c1e-ad7e-35c55969c608",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_patches(masks, image, apply_mask=False, padding=0):\n",
    "    result = []\n",
    "    \n",
    "    for mask in masks:\n",
    "        if apply_mask:\n",
    "            image_tmp = image * (mask[\"segmentation\"][:, :, np.newaxis])\n",
    "        else:\n",
    "            image_tmp = image\n",
    "        \n",
    "        bbox = mask[\"bbox\"]\n",
    "        x0 = bbox[1]-padding\n",
    "        if x0 < 0:\n",
    "            x0 = 0\n",
    "        x1 = bbox[1]+bbox[3]+padding\n",
    "        if x1 >= image.shape[0]:\n",
    "            x1 = image.shape[0] - 1\n",
    "        y0 = bbox[0]-padding\n",
    "        if y0 < 0:\n",
    "            y0 = 0\n",
    "        y1 = bbox[0]+bbox[2]+padding\n",
    "        if y1 >= image.shape[1]:\n",
    "            y1 = image.shape[1] - 1\n",
    "   \n",
    "        x0 = int(x0)\n",
    "        x1 = int(x1)\n",
    "        y0 = int(y0)\n",
    "        y1 = int(y1)\n",
    "\n",
    "        try:\n",
    "            patch = image_tmp[x0:x1, y0:y1]\n",
    "        except:\n",
    "            print(x0, x1, y0, y1, type(x0), type(x1), type(y0), type(y1)) \n",
    "\n",
    "        #mask['patch'] = patch\n",
    "        \n",
    "        if 0 in patch.shape:\n",
    "            continue\n",
    "        result.append(patch)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b0ce753-bb50-42ea-9033-75339c36e5a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_patches_file(image_id):\n",
    "    patches = []\n",
    "    for file in glob.glob(os.path.join(PATCHES_DIR, image_id, \"*.png\")):\n",
    "        patches.append(cv2.imread(file))\n",
    "    return patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8e319a4-cc32-4559-8093-134660e7a3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_masks_file(image_id):\n",
    "    with open(os.path.join(PATCHES_DIR, image_id, \"data.pkl\"), 'rb') as file:\n",
    "        data = pickle.load(file)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bad5d090-c53b-4987-8f94-cd6cca19dc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "10ed62de-2bf4-4558-93cb-7b1c6df5da39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stefan.steinheber/.conda/envs/pt_12.4/lib/python3.12/site-packages/segment_anything/build_sam.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(f)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from segment_anything import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor\n",
    "\n",
    "sam_checkpoint = \"sam_vit_h_4b8939.pth\"\n",
    "model_type = \"vit_h\"\n",
    "\n",
    "sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n",
    "sam.to(device=device)\n",
    "\n",
    "def sam_generate_mask(image):\n",
    "    mask_generator = SamAutomaticMaskGenerator(sam)\n",
    "    masks = mask_generator.generate(image)\n",
    "    return masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8154a416-92ad-4925-ae01-8842029f33e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BinaryResnetClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=1):\n",
    "        super(BinaryResnetClassifier, self).__init__()\n",
    "        # Load a pre-trained ResNet model\n",
    "        self.resnet = resnet50(ResNet50_Weights.IMAGENET1K_V1)  # You can choose any ResNet variant\n",
    "        # Modify the last fully connected layer\n",
    "        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, num_classes)\n",
    "        nn.init.xavier_normal_(self.resnet.fc.weight)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Pass the input through the ResNet\n",
    "        x = self.resnet(x)\n",
    "        # Apply the sigmoid activation function\n",
    "#        x = torch.sigmoid(x)  # Output will be between 0 and 1\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ff8ed842-e974-4252-b405-9efaa27cc23d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BinaryInceptionClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=1):\n",
    "        super(BinaryInceptionClassifier, self).__init__()\n",
    "        # Load a pre-trained ResNet model\n",
    "        self.inception = torch.hub.load('pytorch/vision:v0.10.0', 'inception_v3', pretrained=True) # You can choose any ResNet variant\n",
    "        # Modify the last fully connected layer\n",
    "        self.inception.fc = nn.Linear(self.inception.fc.in_features, num_classes)\n",
    "        nn.init.xavier_normal_(self.inception.fc.weight)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Pass the input through the ResNet\n",
    "        x = self.inception(x)\n",
    "        # Apply the sigmoid activation function\n",
    "#        x = torch.sigmoid(x)  # Output will be between 0 and 1\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4ab6e2a6-6615-4c5d-8cd9-00df3378a16a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=16, latent_dim=200, act_fn=nn.ReLU()):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, padding=1, stride=2),  # 112x112\n",
    "            act_fn,\n",
    "            nn.Conv2d(out_channels, 2*out_channels, 3, padding=1, stride=2),  # 56x56\n",
    "            act_fn,\n",
    "            nn.Conv2d(2*out_channels, 4*out_channels, 3, padding=1, stride=2),  # 28x28\n",
    "            act_fn,\n",
    "            nn.Conv2d(4*out_channels, 8*out_channels, 3, padding=1, stride=2),  # 14x14\n",
    "            act_fn,\n",
    "            nn.Conv2d(8*out_channels, 16*out_channels, 3, padding=1, stride=2),  # 7x7\n",
    "            act_fn,\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(16*out_channels*7*7, latent_dim),\n",
    "            act_fn\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=16, latent_dim=200, act_fn=nn.ReLU()):\n",
    "        super().__init__()\n",
    "        self.out_channels = out_channels\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 16*out_channels*7*7),\n",
    "            act_fn\n",
    "        )\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.ConvTranspose2d(16*out_channels, 8*out_channels, 3, stride=2, padding=1, output_padding=1),  # 14x14\n",
    "            act_fn,\n",
    "            nn.ConvTranspose2d(8*out_channels, 4*out_channels, 3, stride=2, padding=1, output_padding=1),  # 28x28\n",
    "            act_fn,\n",
    "            nn.ConvTranspose2d(4*out_channels, 2*out_channels, 3, stride=2, padding=1, output_padding=1),  # 56x56\n",
    "            act_fn,\n",
    "            nn.ConvTranspose2d(2*out_channels, out_channels, 3, stride=2, padding=1, output_padding=1),  # 112x112\n",
    "            act_fn,\n",
    "            nn.ConvTranspose2d(out_channels, in_channels, 3, stride=2, padding=1, output_padding=1),  # 224x224\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.linear(x)\n",
    "        output = output.view(-1, 16*self.out_channels, 7, 7)\n",
    "        output = self.conv(output)\n",
    "        return output\n",
    "\n",
    "#  defining autoencoder\n",
    "class BigAutoencoder(nn.Module):\n",
    "    def __init__(self, encoder=Encoder(), decoder=Decoder()):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "#        self.encoder.to(device)\n",
    "\n",
    "        self.decoder = decoder\n",
    "#        self.decoder.to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "78bdf896-ee62-4f44-b5a3-571204f9a7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from segment_anything import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor\n",
    "from ultralytics import YOLO\n",
    "import torchvision.transforms.v2 as transforms\n",
    "\n",
    "yolo = YOLO(\"../leaf_segmentation/out/yolo_urban_street/train/weights/best.pt\")\n",
    "yolo_syn = YOLO(\"../leaf_segmentation/out/yolo_synthetic/train4/weights/best.pt\")\n",
    "\n",
    "resnet = torch.load(\"../leaf_segmentation/out/leaf_classifier/resnet_masked_cos/resnet_best.pth\")\n",
    "resnet = resnet.to(device)\n",
    "resnet.eval()\n",
    "\n",
    "resnet_transform = transforms.Compose([\n",
    "    transforms.ToImage(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToDtype(torch.float32, scale=True),\n",
    "    #transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  \n",
    "])\n",
    "\n",
    "inception = torch.load(\"../leaf_segmentation/out/leaf_classifier/inception/inception_best.pth\")\n",
    "inception = inception.to(device)\n",
    "inception.eval()\n",
    "\n",
    "inception_transform = transforms.Compose([\n",
    "    transforms.ToImage(),\n",
    "    transforms.Resize((320, 320)),\n",
    "    transforms.ToDtype(torch.float32, scale=True),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  \n",
    "])\n",
    "\n",
    "sam_checkpoint = \"sam_vit_h_4b8939.pth\"\n",
    "model_type = \"vit_h\"\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n",
    "sam.to(device=device)\n",
    "\n",
    "mask_generator = SamAutomaticMaskGenerator(sam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5f7a9af2-acd7-4352-8350-c587ee4ef378",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict_sam(img, pred, image_id, threshold):\n",
    "    masks = get_masks_file(image_id)\n",
    "    result = []\n",
    "    mask_result = []\n",
    "    \n",
    "    for i, mask in enumerate(masks):\n",
    "        patch = mask['patch']\n",
    "        if 0 in patch.shape:\n",
    "            continue\n",
    "        include, prob = pred(patch)\n",
    "        if include:\n",
    "            result.append(mask[\"segmentation\"] > 0)\n",
    "        \n",
    "        mask['leaf_probability'] = prob\n",
    "        if prob < threshold:\n",
    "            del masks[i]\n",
    "    return result, mask_result\n",
    "    \n",
    "\n",
    "def pred_resnet(x):\n",
    "    x = resnet_transform(x).to(device).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        out = resnet(x)\n",
    "        sig = torch.sigmoid(out).item()\n",
    "        return sig < 0.025, 1 - sig    \n",
    "\n",
    "def s1_sam_resnet(img, image_id):\n",
    "    return predict_sam(img, pred_resnet, image_id, .9)\n",
    "\n",
    "\n",
    "def pred_inception(x):\n",
    "    x = inception_transform(x).to(device).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        out = inception(x)\n",
    "        sig = torch.sigmoid(out).item()\n",
    "        return sig < 0.01, 1 - sig\n",
    "\n",
    "def s1_sam_inception(img, image_id):\n",
    "    return predict_sam(img, pred_inception, image_id, .95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1259a2bf-896f-4787-8f2f-1099d4d3cb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO, checks\n",
    "model = YOLO(\"../leaf_segmentation/out/yolo_urban_street/train/weights/best.pt\")\n",
    "\n",
    "def s1_sam_yolo(image, image_id):\n",
    "#    masks = sam_generate_mask(image)\n",
    "#    patches = get_patches(masks, image)\n",
    "#    patches = get_patches_file(image_id)\n",
    "    results_yolo = []\n",
    "    masks = get_masks_file(image_id)\n",
    "    for i, mask in enumerate(masks):\n",
    "        result = model.predict(mask['patch'], verbose=False)\n",
    "        # retrieve leaf (class 1) porbability\n",
    "        prob = result[0].boxes.conf\n",
    "        if len(prob) == 1:\n",
    "            prob = prob.item()\n",
    "        else:\n",
    "            prob = 0\n",
    "        results_yolo.append(prob)\n",
    "        mask['leaf_probability'] = prob\n",
    "    masks_filtered = [mask for mask in masks if mask['leaf_probability'] > .8]\n",
    "    return masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "42d46fc7-81c8-47d6-873a-ea7eec5cff30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_seg = YOLO(\"../leaf_segmentation/out/yolo_synthetic/train4/weights/best.pt\")\n",
    "\n",
    "def s1_yolo(image, image_id):\n",
    "    masks_result = []\n",
    "    result = model.predict(image, verbose=False, retina_masks=True)[0]\n",
    "    if result.masks is None:\n",
    "        return []\n",
    "    masks = result.masks.data\n",
    "    boxes = result.boxes.data\n",
    "    names = list(result.names.values())\n",
    "    \n",
    "    classes = boxes[:, 5]\n",
    "    \n",
    "    for i, name in enumerate(names):\n",
    "        obj_indices = torch.where(classes == i)\n",
    "        obj_masks = masks[obj_indices]\n",
    "        obj_masks = torch.nn.functional.interpolate(obj_masks.unsqueeze(0), size=image.shape[:2], mode='bilinear', align_corners=False).squeeze(0)\n",
    "        prob = result[0].boxes.conf\n",
    "        \n",
    "        segmentations = [seg.cpu().numpy() for seg in torch.unbind(obj_masks)]\n",
    "        \n",
    "        for i, seg in enumerate(segmentations):\n",
    "            patch = image * seg[:, :, np.newaxis].astype(np.uint8)\n",
    "            coords = cv2.findNonZero(seg)  # Returns all non-zero points\n",
    "            x, y, w, h = cv2.boundingRect(coords)  # Get bounding box\n",
    "            \n",
    "            patch = patch[y:y+h, x:x+w]\n",
    "            masks_result.append({\n",
    "                \"segmentation\": seg,\n",
    "                \"leaf_probability\": result.boxes.conf.cpu().numpy()[i],\n",
    "                \"patch\": patch\n",
    "            })\n",
    "    return masks_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "154d3172-31ea-4dc8-a01e-84a052199ed3",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating patches: 100%|██████████| 512/512 [11:53<00:00,  1.39s/it]\n"
     ]
    }
   ],
   "source": [
    "for index in tqdm.tqdm(indices, desc=\"Generating patches\"):\n",
    "    img_id = train_data.loc[index][\"image_id\"]\n",
    "    try:\n",
    "        os.makedirs(os.path.join(PATCHES_DIR, img_id))\n",
    "    except:\n",
    "        continue\n",
    "    img = cv2.imread(os.path.join(DATASET_DIR, \"images\", img_id + \".jpg\"))\n",
    "    img = cv2.resize(img, (640, 640))\n",
    "    masks = sam_generate_mask(img)\n",
    "    patches = get_patches(masks, img, apply_mask=True)\n",
    "    \n",
    "    for d, item in zip(masks, patches):\n",
    "        d['patch'] = item\n",
    "    with open(os.path.join(PATCHES_DIR, img_id, \"data.pkl\"), 'wb+') as file:\n",
    "        pickle.dump(masks, file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    #for i, patch in enumerate(patches):\n",
    "    #    cv2.imwrite(os.path.join(PATCHES_DIR, img_id, f\"patch{i}.png\"), patch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "83ca455c-920a-4a8b-8b61-3f0d631392e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "stage1_dict = {\n",
    "    #\"YOLOv8\": s1_yolo,\n",
    "    \"SAM + ResNet\": s1_sam_resnet,\n",
    "    \"SAM + YOLOv8\": s1_sam_yolo,\n",
    "    \"SAM + Inception\": s1_sam_inception\n",
    "#    \"Mask R-CNN\": s1_mask_rcnn\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca72354-fcaf-4bd5-ba49-e6b0f3f8ee3c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model SAM + ResNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SAM + ResNet: 100%|██████████| 512/512 [02:07<00:00,  4.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model SAM + YOLOv8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SAM + YOLOv8:  62%|██████▏   | 319/512 [02:31<01:13,  2.62it/s]"
     ]
    }
   ],
   "source": [
    "stage1_results = {}\n",
    "\n",
    "for stage1_name, stage1_model in stage1_dict.items():\n",
    "    print(f\"Running model {stage1_name}\")\n",
    "    stage1_results[stage1_name] = {}\n",
    "    for index in tqdm.tqdm(indices, desc=stage1_name):\n",
    "        gt_healthy = bool(train_data.loc[index][\"healthy\"])\n",
    "        stage1_results[stage1_name][index] = {\n",
    "            'healthy': gt_healthy,\n",
    "            'masks': []\n",
    "        }\n",
    "        img = cv2.imread(os.path.join(DATASET_DIR, \"images\", train_data.loc[index][\"image_id\"] + \".jpg\"))\n",
    "        with torch.no_grad():\n",
    "            leaf_masks = stage1_model(img, train_data.loc[index][\"image_id\"])\n",
    "            stage1_results[stage1_name][index]['masks'] = leaf_masks\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "107bf11f-9256-4e67-b86b-933ec47bbc9f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(os.path.join(INT_S1_DIR, \"total_data.pkl\"), \"wb+\") as file:\n",
    "    pickle.dump(stage1_results, file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9a2d1dbf-935e-48f2-9d81-d20eddc1357f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "for stage1_name, stage1_result in stage1_results.items():\n",
    "    os.makedirs(os.path.join(INT_S1_DIR, stage1_name), exist_ok=True)\n",
    "    with open(os.path.join(INT_S1_DIR, stage1_name, \"data.pkl\"), \"wb+\") as file:\n",
    "        pickle.dump(stage1_result, file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6b12117c-d155-4d35-a421-8af546a66751",
   "metadata": {},
   "outputs": [],
   "source": [
    "for stage1_name, stage1_result in stage1_results.items():\n",
    "    patches_dir = os.path.join(INT_S1_DIR, stage1_name, \"patches\")\n",
    "    for index, data in stage1_result.items():\n",
    "        image_dir = os.path.join(patches_dir, str(index))\n",
    "        os.makedirs(image_dir, exist_ok=True)\n",
    "        for i, leaf_mask in enumerate(data['masks']):\n",
    "            cv2.imwrite(os.path.join(image_dir, f\"patch_{i}.png\"), leaf_mask['patch'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99933272-214a-44bb-820d-21cdcf01bcba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch w/ CUDA 12.4",
   "language": "python",
   "name": "pytorch_cuda_12.4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
