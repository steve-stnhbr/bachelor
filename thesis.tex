% Copyright (C) 2014-2023 by Thomas Auzinger <thomas@auzinger.name>

\documentclass[draft,final]{vutinfth} % Remove option 'final' to obtain debug information.

% Load packages to allow in- and output of non-ASCII characters.
\usepackage{lmodern}        % Use an extension of the original Computer Modern font to minimize the use of bitmapped letters.
\usepackage[T1]{fontenc}    % Determines font encoding of the output. Font packages have to be included before this line.
\usepackage[utf8]{inputenc} % Determines encoding of the input. All input files have to use UTF8 encoding.

% Extended LaTeX functionality is enables by including packages with \usepackage{...}.
\usepackage{amsmath}    % Extended typesetting of mathematical expression.
\usepackage{amssymb}    % Provides a multitude of mathematical symbols.
\usepackage{mathtools}  % Further extensions of mathematical typesetting.
\usepackage{microtype}  % Small-scale typographic enhancements.
\usepackage[inline]{enumitem} % User control over the layout of lists (itemize, enumerate, description).
\usepackage{multirow}   % Allows table elements to span several rows.
\usepackage{booktabs}   % Improves the typesetting of tables.
\usepackage{subcaption} % Allows the use of subfigures and enables their referencing.
\usepackage[ruled,linesnumbered,algochapter]{algorithm2e} % Enables the writing of pseudo code.
\usepackage[usenames,dvipsnames,table]{xcolor} % Allows the definition and use of colors. This package has to be included before tikz.
\usepackage[style=alphabetic,natbib=true]{biblatex}
\usepackage{makecell}
\usepackage{nag}       % Issues warnings when best practices in writing LaTeX documents are violated.
\usepackage{todonotes} % Provides tooltip-like todo notes.
\usepackage{hyperref}  % Enables hyperlinking in the electronic document version. This package has to be included second to last.

\usepackage[english,german]{babel}
\usepackage[acronym,toc]{glossaries} % Enables the generation of glossaries and lists of acronyms. This package has to be included last.
\addbibresource{refs.bib}

% Define convenience functions to use the author name and the thesis title in the PDF document properties.
\newcommand{\authorname}{Stefan Steinheber} % The author name without titles.
\newcommand{\thesistitle}{Observation and Tracking of Plant Health using
Computer Vision} % The title of the thesis. The English version should be used, if it exists.

% Set PDF document properties
\hypersetup{
    pdfpagelayout   = TwoPageRight,           % How the document is shown in PDF viewers (optional).
    linkbordercolor = {Melon},                % The color of the borders of boxes around hyperlinks (optional).
    pdfauthor       = {\authorname},          % The author's name in the document properties (optional).
    pdftitle        = {\thesistitle},         % The document's title in the document properties (optional).
    pdfsubject      = {Subject},              % The document's subject in the document properties (optional).
    pdfkeywords     = {a, list, of, keywords} % The document's keywords in the document properties (optional).
}

\setpnumwidth{2.5em}        % Avoid overfull hboxes in the table of contents (see memoir manual).
\setsecnumdepth{subsection} % Enumerate subsections.

\nonzeroparskip             % Create space between paragraphs (optional).
\setlength{\parindent}{0pt} % Remove paragraph indentation (optional).

\makeindex      % Use an optional index.
\makeglossaries % Use an optional glossary.
%\glstocfalse   % Remove the glossaries from the table of contents.

% Set persons with 4 arguments:
%  {title before name}{name}{title after name}{gender}
%  where both titles are optional (i.e. can be given as empty brackets {}).
\setauthor{}{\authorname}{}{male}
\setadvisor{Univ.Prof. Dipl.-Ing. Dipl.-Ing. Dr.techn.}{Michael Wimmer}{PhD}{male}

% For bachelor and master theses:
\setfirstassistant{Projektass. Mag.rer.soc.oec.}{Stefan Ohrhallinger}{PhD}{male}
%\setsecondassistant{Pretitle}{Forename Surname}{Posttitle}{male}
%\setthirdassistant{Pretitle}{Forename Surname}{Posttitle}{male}

% For dissertations:
\setfirstreviewer{Pretitle}{Forename Surname}{Posttitle}{male}
\setsecondreviewer{Pretitle}{Forename Surname}{Posttitle}{male}

% For dissertations at the PhD School and optionally for dissertations:
\setsecondadvisor{Pretitle}{Forename Surname}{Posttitle}{male} % Comment to remove.

% Required data.
\setregnumber{12022506}
\setdate{01}{01}{2024} % Set date with 3 arguments: {day}{month}{year}.
\settitle{\thesistitle}{Erfassung von Pflanzengesundheit mittels Computer Visison} % Sets English and German version of the title (both can be English or German). If your title contains commas, enclose it with additional curvy brackets (i.e., {{your title}}) or define it as a macro as done with \thesistitle.
%\setsubtitle{Optional Subtitle of the Thesis}{Optionaler Untertitel der Arbeit} % Sets English and German version of the subtitle (both can be English or German).

% Select the thesis type: bachelor / master / doctor.
% Bachelor:
\setthesis{bachelor}
%
% Master:
%\setthesis{master}
%\setmasterdegree{dipl.} % dipl. / rer.nat. / rer.soc.oec. / master
%
% Doctor:
%\setthesis{doctor}
%\setdoctordegree{rer.soc.oec.}% rer.nat. / techn. / rer.soc.oec.

% For bachelor and master:
\setcurriculum{Media Informatics and Visual Computing}{Medieninformatik und Visual Computing} % Sets the English and German name of the curriculum.

% Optional reviewer data:
\setfirstreviewerdata{Affiliation, Country}
\setsecondreviewerdata{Affiliation, Country}

% render without figures and tables
%\usepackage{environ}
%\RenewEnviron{figure}{}% Gobble figure environment
%\RenewEnviron{table}{}% Gobble table environment

\begin{document}
\newcommand{\aitool}[3]{
\textbf{\underline{#1:}}\vspace{.8em}\newline\textbf{Input:}\vspace{.3em}\newline#2\vspace{.6em}\newline \textbf{Output:}\vspace{.3em}\newline#3}


\frontmatter % Switches to roman numbering.
% The structure of the thesis has to conform to the guidelines at
%  https://informatics.tuwien.ac.at/study-services

\addtitlepage{naustrian} % German title page.
\addtitlepage{english} % English title page.
\addstatementpage

\begin{danksagung*}
Vielen Dank an das DataLab der Technischen Universität Wien und die Bereitstellung der Cluster-Ressourcen. Alle Berechnungen wurden in einer Jupyter-Umgebung (Version 3.6.7) auf einer Compute-Node mit einem AMD EPYC 7742 64-Core Processor, 64 GB RAM und einer NVIDIA A100 40 GB ausgeführt.

Als Schreibhilfe wurde Writefull (Version 2.2.0) verwendet, das mithilfe von KI Verbesserungsvorschläge zum Text erstellt. Hier wurde sowohl das Modell von Writefull als auch die GPT Integration, die ebenfalls von Writefull angeboten wird, verwendet.

\end{danksagung*}

\begin{acknowledgements*}
Thank you to the Datalab at the Technical University of Vienna for providing their Cluster Resources. All computations were executed in a Jupyter environment (Version 3.6.7) on a Compute Node equipped with an AMD EPYC 7742 64-core processor, 64 GB of RAM, and an NVIDIA A100 40 GB GPU.

The Writefull AI tool (Version 2.2.0) was used as a writing aid using Writefull's model and the GPT-based integration provided by Writefull.

In some cases, AI tools were used to improve the writing style:

\aitool{DeepL Writing (Academic Style)}{The novelty of this architecture is the introduction of Inception blocks, which process their input with different-sized convolutions for different number of times}{The distinctive feature of this architectural design is the incorporation of Inception blocks, which facilitate the processing of input through the application of varying convolutional operations, depending on the input size.}

\aitool{DeepL Translation}{The domain of detecting plant diseases holds many benefits for society, increasing farmers' crop yields, and thus increasing food security. The current state of research addresses the identification of diseases in single leaves in a lab context using Computer Vision, achieving very reliable results, however, when applied to real-world conditions like agricultural fields the proposed methods show their limitations. To address this issue, this work proposes a methodology that is designed to be used in these conditions, employing multiple stages for segmentation and disease detection to generate its predictions. The comparison of various models in each stage showed that a combination of Meta's SegmentAnythingModel (SAM) and YOLOv8 for detecting leaves in an image and AlexNet for discerning healthy and diseased leaf regions, showed most suitable for this task, achieving an accuracy of 71.01\%. This baseline performance underscores the potential of multi-stage approaches in agricultural computer vision and provides a foundation for future research aimed at enhancing disease detection precision in challenging field conditions.}{Die Erkennung von Pflanzenkrankheiten ist für die Gesellschaft von großem Nutzen, da sie die Ernteerträge der Landwirte erhöht und damit die Ernährungssicherheit verbessert. Der derzeitige Stand der Forschung befasst sich mit der Identifizierung von Krankheiten auf einzelnen Blättern im Labor unter Verwendung von Computer Vision und erzielt dabei sehr zuverlässige Ergebnisse, doch bei der Anwendung unter realen Bedingungen wie auf landwirtschaftlichen Feldern zeigen die vorgeschlagenen Methoden ihre Grenzen. Um dieses Problem anzugehen, wird in dieser Arbeit eine Methodik vorgeschlagen, die für den Einsatz unter diesen Bedingungen konzipiert ist und mehrere Stufen für die Segmentierung und Krankheitserkennung verwendet, um Vorhersagen zu erstellen. Der Vergleich verschiedener Modelle in jeder Phase zeigte, dass eine Kombination aus Metas SegmentAnythingModel (SAM) und YOLOv8 für die Erkennung von Blättern in einem Bild und AlexNet für die Unterscheidung gesunder und kranker Blattregionen am besten für diese Aufgabe geeignet war und eine Genauigkeit von 71,01\% erreichte. Diese grundlegende Leistung unterstreicht das Potenzial von mehrstufigen Ansätzen in der landwirtschaftlichen Bildverarbeitung und bietet eine Grundlage für künftige Forschungen, die darauf abzielen, die Genauigkeit der Krankheitserkennung unter schwierigen Feldbedingungen zu verbessern.}

\end{acknowledgements*}

\begin{kurzfassung}
Die Erkennung von Pflanzenkrankheiten ist für die Gesellschaft von großem Nutzen, da sie die Ernteerträge der Landwirte erhöht und damit die Ernährungssicherheit verbessern kann. Der derzeitige Stand der Forschung befasst sich mit der Identifizierung von Krankheiten auf einzelnen Blättern im Labor unter Verwendung von Computer Vision und erzielt dabei sehr zuverlässige Ergebnisse, doch bei der Anwendung unter realen Bedingungen wie auf landwirtschaftlichen Feldern zeigen die vorgeschlagenen Methoden ihre Grenzen. Um dieses Problem anzugehen, wird in dieser Arbeit eine Methodik vorgeschlagen, die für den Einsatz unter diesen Bedingungen konzipiert ist und mehrere Stufen für die Isolierung von Blättern und die anschließende Krankheitserkennung verwendet, um Vorhersagen zu erstellen. Der Vergleich verschiedener Modelle in jeder Phase zeigte, dass eine Kombination aus Metas SegmentAnythingModel (SAM) und YOLOv8 für die Erkennung und Isolation von Blättern in einem Bild und AlexNet für die Unterscheidung gesunder und kranker Blattregionen am besten für diese Aufgabe geeignet war und eine Genauigkeit von 69,71\% erreichte. Diese grundlegende Leistung unterstreicht das Potenzial von mehrstufigen Ansätzen in der landwirtschaftlichen Bildverarbeitung und bietet eine Grundlage für künftige Forschungen, die darauf abzielen, die Genauigkeit der Krankheitserkennung unter schwierigen Feldbedingungen zu verbessern.
\end{kurzfassung}

\begin{abstract}
The domain of detecting plant diseases holds many benefits for society, enabling increases in farmers' crop yields, and thus food security. The current state of research addresses the identification of diseases in single leaves in a lab context using Computer Vision, achieving very reliable results, however, when applied to real-world conditions like agricultural fields the proposed methods show their limitations. To address this issue, this work proposes a methodology that is designed to be used in these conditions, employing multiple stages for isolation of plant leaf regions in the image and subsequent disease detection to generate its predictions. The comparison of various models in each stage showed that a combination of Meta's SegmentAnythingModel (SAM) and YOLOv8 for detecting and isolating leaves in an image and AlexNet for discerning healthy and diseased leaf regions, proved most suitable for this task, achieving an accuracy of 69.71\%. This baseline performance underscores the potential of multi-stage approaches in agricultural computer vision and provides a foundation for future research aimed at enhancing disease detection precision in challenging field conditions.
\end{abstract}

% Select the language of the thesis, e.g., english or naustrian.
\selectlanguage{english}

% Add a table of contents (toc).
\tableofcontents % Starred version, i.e., \tableofcontents*, removes the self-entry.

% Switch to arabic numbering and start the enumeration of chapters in the table of content.
\mainmatter

\chapter{Introduction}
Plant health is a critical factor in agriculture, especially regarding food security. Effective monitoring and assessment of plant health can prevent crop loss, optimize crop yield, and reduce the need for chemical fertilizers and pesticides. Traditionally, monitoring plant health relied on manual human inspection of crops or sensor-based methods detecting the moisture in the soil or nutrients in individual plants. However, these methods can be labor-intensive and difficult to scale across a large farm field.

Approaches to the use of computer vision in a crop-farming context have been around for a long time but are more rudimentary than the approaches presented in this work \cite{cunha_application_2003}. These approaches only had a limited range of use cases and were more manually involved. 

With the advancement of Machine Learning, particularly in Computer Vision, the capability to monitor plant health is increasingly accessible to small farms and individual cultivators. Computer Vision encompasses the automated extraction, analysis, and interpretation of information derived from images or videos. Within plant health monitoring, Computer Vision analyzes captured images of plant leaves to identify specific characteristics or patterns that signify the plant's health status or potential issues therein.

Most approaches presented in existing literature utilize given datasets, like PlantVillage or PlantDoc (\cite{hughes_open_2016, singh_plantdoc_2020}). These datasets mainly provide images of single leaves before a neutral background. However, this is rarely the case in a practical environment. 
In a practical environment, you often have to deal with many leaves, which can overlap or be entirely obstructed by other objects in the scene. The background can also vary significantly in complexity and contrast to the leaves. 
This work will evaluate the ability to transmit the results of this method to training and evaluation models. This involves the introduction of an image processing pipeline. To combat the problems of overlapping leaves and varying backgrounds, the pipeline will include an image segmentation step, which aims to separate the leaf from the background as well as possible, followed by the disease detection step. 
Subsequently, the image processing pipeline will be applied to a set of images captured in a real-world scenario, containing multiple overlapping instances of leaves with both healthy and diseased plants. Based on that image set, the proposed method's efficacy in detecting diseases in the presented plants will be evaluated.

\chapter{Related Work} \label{sec:related_work}
A substantial body of research is dedicated to monitoring plant health. Much research has been conducted, especially in regions of high agricultural importance, such as India and China. I will focus on the two areas that are important for the thesis, i.e., Leaf Segmentation (Section \ref{sec:leaf_seg}) and Disease Detection (Section \ref{sec:disease_detection}).

\section{Plant/Leaf Segmentation} \label{sec:leaf_seg}

Many different segmentation algorithms exist in image processing (IP) and Machine Learning (ML), each pertaining to different use cases. Ranging from the most basic approach of selecting similar colors like global threshold \cite{lu_chapter_2024}, to more sophisticated methods like k-means clustering involving shallow learning methods for segmentation but is also based on color \cite{dhanachandra_image_2015}.

\subsection{Shallow Learning methods}
In the beginning of this domain, researchers utilized more traditional classifiers and shallow learning to determine an object from the background in an image. \cite{lowe_distinctive_2004} presented an approach using the histogram of the image in a method called Histogram Oriented Gradients (HOG). 

\cite{gao_method_2018} proposed a semi-automated image processing method for segmenting leaves. Their methodology requires the manual input of points lying on the area of the leaf in a presented image. Through further processing, they determine the perimeter points of the leaf and use these in the marker-based watershed segmentation \cite{kornilov_review_2022}. The proposed methodology outperformed state-of-the-art segmentation algorithms like Otsu, GrabCut, and regular Watershed segmentation with a Jaccard index of 96.97\%.

\subsection{Deep Learning methods}
The problem with color-based segmentation algorithms is the high color uniformity when processing plant leaf images. Especially when examining overlapping plant leaves, there often is little color contrast when differentiating individual leaf instances, which leads to the algorithms' low efficacy.
With recent developments in hardware, namely the progress in Graphics and Tensor processing units, and the developments in software development in Machine Learning, the approach of utilizing Deep Learning Neural Networks (DLNN) has become more and more viable for use in image segmentation. For this reason, multiple Deep Learning (DL) based segmentation algorithms have been proposed. The benefit of using Artificial Neural Networks (ANN) in Deep Learning is that they can be trained to learn different characteristics and patterns of plant leaves, especially when utilizing Convolutional Layers \cite{patil_convolutional_2021}, so they can recognize leaves not solely on color information but also based on their traits, like shape and color, and additionally can be more resilient towards capturing imperfections like lighting conditions, reflections, and overlapping objects. 

\cite{yang_leaf_2020} performed a study examining the performance of the Mask R-CNN model in a leaf segmentation task with a complicated background. \citeauthor{yang_leaf_2020} argued that other datasets like the Aberystwyth Leaf Evaluation Dataset \cite{bell_aberystwyth_2016, scharr_leaf_2016, minervini_finely-grained_2016} only provide images with a relatively uniform and contrasting background compared to the leaves. Thus, they prepared 4000 images of 15 different plant species with a non-uniform and more complicated background and manually created masks for individual leaves. The model was compared to non-deep-learning segmentation approaches such as Grabcut and Otsu segmentation. The DL approach managed to perform significantly better than the others with a Misclassification Error (ME) of 1.15\% compared to 28.74\% (Grabcut) and 29.80\% (Otsu).

 \cite{guo_leafmask_2021} proposed a model called LeafMask. This model consists of two main components: the mask assembly module and the mask refining module. The mask assembly module merges position-sensitive bases from each predicted bounding box after non-maximum suppression (NMS) with corresponding coefficients to generate initial masks. This is followed by the mask refining module, which improves leaf boundaries through a point selection strategy and predictor, ensuring precise delineation of leaf edges. Additionally, LeafMask incorporates a multi-scale attention module within its dual attention-guided mask (DAG-Mask) branch, enhancing information representation and producing more accurate bases. By integrating these modules under an anchor-free instance segmentation paradigm, LeafMask effectively addresses the challenges of leaf occlusion, overlap, and varying shapes and sizes. Validated through extensive experiments on the Leaf Segmentation Challenge (LSC) dataset, LeafMask achieved a 90.09\% BestDice score, outperforming existing state-of-the-art methods.

A hierarchical approach to segmenting semantics, plants, and individual leaves was proposed in
\cite{roggiolani_hierarchical_2023}. This model uses an encoder-decoder architecture, where a single ERFNet encoder (a semantic segmentation network designed for real-time usage \cite{romera_erfnet_2018}) and three different ERFNet decoders, one for semantic segmentation (differentiating between plant and soil), plant segmentation (differentiating individual plants), and leaf segmentation (differentiating individual leaves). The decoders are also connected by skip connections that provide unencoded data from the previous level. They used two different datasets captured from a top-down perspective on an agricultural field: GrowliFlower \cite{kierdorf_growliflower_2023} \& Sugar Beets \cite{chebrolu_agricultural_2017}.
They scored a Panoptic Quality (PQ) score of 76.2\% \& 89.2\%, whereas other architectures performed significantly worse.  

In \cite{kirillov_segment_2023} a novel general-purpose model (Segment Anything Model (SAM)) for panoptic segmentation was proposed. This new proposed model outperformed other state-of-the-art models like ViTDet and shows that upon human evaluation most of the masks generated by SAM achieve a rating of 9 out of 10.

\section{Disease Detection} \label{sec:disease_detection}

\subsection{Shallow Learning methods}
Several studies exist that use shallow learning to classify and recognize plant diseases, utilizing Support Vector Machines \cite{kirti_black_2020}, while others use k Nearest Neighbor classifiers to great success \cite{bharate_classification_2020}.

\cite{kaur_semi-automatic_2018} proposed a method based on the k-means clustering algorithm, which in image processing creates clusters of similarly colored regions in the image. In their image processing pipeline, they first use the number of these clusters to identify if the plant in the given image is infected with a disease, based on the color and texture properties of the individual clusters they then classified the disease present in the given leaf image. Using this method, a mean accuracy of 85.65\% was achieved on a soybean subset of PlantVillage \cite{hughes_open_2016}.

Another way of linearly classifying is by utilizing a support vector machine (SVM). \cite{prakash_detection_2017} proposed a method that first extracts various features from a given citrus plant leaf image using statistical Gray-Level Co-Occurrence Matrix (GLCM), which subsequently are input into an SVM to classify the leaf as healthy or infested by a disease. They achieved an accuracy of 90\% across a test set of 60 images. 

\cite{suresha_recognition_2017} proposed a method for detecting diseases in paddy (rice plant) leaves using the geometric features of visual indicators. First, they used the Otsu (\cite{otsu_threshold_1979} segmentation with a global threshold after converting the input image to the HSV color space to segment the image by color. The resulting regions were then analyzed for their geometric features, which were then used to classify the disease using the k-nearest-neighbor shallow learning method. They achieved an accuracy of 76.6\% in classifying three different types of diseases on paddy leaves.

Compared to Shallow Learning, Deep Learning approaches, which utilize Deep Learning Neural Networks (DLNN) to classify plant diseases, show a more accurate performance \cite{yao_machine_2023, sujatha_performance_2021}.

\subsection{Deep Learning methods}
However, as \cite{yao_machine_2023} as well as \cite{sujatha_performance_2021} pointed out, Deep Learning is much more popular and also more effective due to its higher accuracy and flexibility.

Using deep learning \cite{khalid_real-time_2023} utilized a fine-tuned version of the general-purpose object classification model \textit{YOLO} to classify unhealthy regions in pictures of individual leaves. They managed to achieve an accuracy of 95.12\%.

Many other approaches use general object detection networks like AlexNet, GoogLeNet, and VGGNet, which are fine-tuned to detect illness spots in plant leaves. \cite{applalanaidu_review_2021}

\cite{brahimi_deep_2018} used various Deep Learning models with 3 different learning/training techniques on the PlantVillage dataset \cite{hughes_open_2016}. They found that fine-tuning all layers of a model trained with ImageNet \cite{deng_imagenet_2009} produced the best results for disease detection with an optimal accuracy of 99.76\% achieved by InceptionV3, while all other models discussed achieved accuracies over 98\% using this training approach.

Data augmentation is a method for increasing the variability of the training data and thus increasing the model's adaptability to different inputs. Generally, this task aims to change an existing image to discourage the model from learning unimportant properties. For example, only trained with a dataset consisting of pictures of leaves with the leaf's stem facing downwards, the model could not classify an image of a rotated leaf as effectively. That is what data augmentation tries to tackle \cite{perez_effectiveness_2017}. \cite{wongbongkotpaisan_plant_2021} proposed a two-fold data augmentation method. They utilize local augmentation to identify diseased regions in the image using Otsu global thresholding in the L*a*b* color space to increase the ratio of diseased regions to the leaf area. They performed probabilistic rotation, brightness, and blurriness changes in global data augmentation. By utilizing this data augmentation, they accomplished greater accuracy in different model architectures and faster loss- \& accuracy-convergence.

A study by \cite{xu_research_2022} proposes an alternative methodology that first involves segmenting images of individual leaves. Subsequently, it assesses the shape and any deviations from a standard leaf morphology to infer the health status of the corresponding plant.

\chapter{Methodology}

This work proposes a pipeline designed to detect diseases in an image captured by a camera that contains multiple leaves of different plants.
This pipeline will consist of two stages that will handle different subtasks to achieve the general goal of detecting diseases in plant leaves. The multiple stages are shown in Figure \ref{fig:method_pipeline}.

\begin{figure}
    \centering
    \includegraphics[width=0.75\linewidth]{graphics/pipeline_structure.png}
    \caption{The makeup of the proposed pipeline methodology}
    \label{fig:method_pipeline}
\end{figure}


\textbf{Leaf segmentation} (Section \ref{sec:method_segmentation}) is used to detect individual leaves in the image. This is necessary as the Disease Detection stage cannot distinguish between multiple leaves in different states. Furthermore, the image background, which this stage seeks to eliminate, may bring unwanted noise into the disease detection step, potentially skewing its results. This stage will consist of two approaches, which will be compared in terms of performance. One approach involves first segmenting the image panoptically, i.e., extracting all regions belonging to an object and subsequently classifying each resulting region (see Section \ref{sec:meth_panoptic}). The other approach is instance segmentation, in which a model outputs only the areas of a specific class within an image (see Section \ref{sec:meth_instance}).

The \textbf{Disease detection} stage (Section \ref{sec:method_disease}) utilizes the leaf region outputs generated from the preceding Leaf segmentation stage. Subsequently, it classifies each leaf into healthy and diseased categories based on these inputs.

This two-stage approach has been chosen for several reasons: The model developed through this work should be usable in various scenarios, including those with multiple plants in the captured image. A segmentation step is necessary to detect diseases in each individual plant. In most cases, datasets designed to differentiate between diseased and healthy plants present individual leaves, making it unsuitable to classify multiple leaves directly. Similar inputs must be produced to leverage the performance of models trained on such datasets. Thus, the segmentation stage is employed, which produces single-leaf areas. This should also increase the accuracy of the model and its resilience to noise in the background, which may contribute to false classifications. Additionally, this approach enables the location of a leaf that has been classified as diseased and can thus produce more significant results for a given user.

\section{Leaf Segmentation} \label{sec:method_segmentation}

In this work, the subject of Leaf Segmentation will be examined using two different approaches. Each of the two variants utilizes various methods to accomplish the same task. Panoptic segmentation will use a model that generates panoptic segmentation data for an input image. These segments are void of class attribution and represent individual regions in an image. An example output can be seen in Figure \ref{fig:panoptic_example}, where each region generated by the panoptic segmentation is colored individually. After segmentation, classification will filter relevant (i.e., areas containing a leaf) from irrelevant regions, which can then be passed to the next stage.
In instance segmentation, the segmentation and classification steps are combined. The model is trained to output only relevant regions while differentiating between instances of areas with the same semantic label. This means the model can distinguish between the background and multiple individual leaf instances and generate a region mask for each.

\begin{figure}
    \centering
    \includegraphics[width=0.5\linewidth]{graphics/panoptic_example.png}
    \caption{Example output of panoptic segmentation. Each region generated by the PS is colored individually}
    \label{fig:panoptic_example}
\end{figure}

\subsection{Panoptic Segmentation} \label{sec:meth_panoptic}
The panoptic segmentation (PS) technique combines semantic and instance segmentation. This approach allows for dividing an image into regions of varying semantic significance while concurrently distinguishing between different instances within the same semantic category.

This technique will be employed for leaf segmentation as a two-stage process. First, the panoptic segmentation model will segment the image, yielding a comprehensive set of all the semantic regions within the image. To focus exclusively on the areas representing leaves, each identified region will be processed through a binary classification model to ascertain whether it is a leaf.

In this case, the Segment Anything Model (SAM) \cite{kirillov_segment_2023} published by Meta will be used as the panoptic segmentation backbone. As this model is trained to generate segmentations from input prompts, such as bounding boxes or points lying around or inside the object area, the \textit{AutomaticMaskGenerator}, provided in the SAM python library, will be used to automatically generate the necessary input prompts and allow the image to be panoptically segmented without manual input. An example output of SAM can be seen in Figure \ref{fig:panoptic_example}.

Using PS for segmentation avoids the need to identify leaves directly with a custom-trained model. It considers all semantic regions and determines their importance, allowing a pre-trained panoptic segmentation model to be used. This pre-trained model handles the complex segmentation task, which usually requires extensive data for training.

The second stage, i.e., the classification of regions, will consist of a custom classification model and require actual training and/or finetuning. For this stage, two approaches will be discussed.

\subsubsection{Autoencoder} \label{sec:method_autoencoder}
An autoencoder is a convolutional neural network consisting of an encoder, a bottleneck, and a decoder. The encoder converts the model's input into a latent representation, which is then passed through the bottleneck to the decoder, which tries to reconstruct the original model input from the latent space representation (as depicted in Figure \ref{fig:autoencoders}). 

The autoencoder is trained unsupervised, enabling it to handle single-class data effectively during training and alleviating the necessity of defining a negative (non-leaf) class for training. Throughout training, the model's weights are fine-tuned to minimize the deviation between the output and the input, leveraging the latent representation within the network. 
The concept of the \textit{anomaly detection} process is that the model learns to recreate images of the presented class more accurately than images that do not belong to that class.
The conformity to the presented class is determined by the divergence (the pixel-wise mean square error) of the model's output from the input. By thresholding this divergence, a single-class classification can be achieved \cite{bank_autoencoders_2021}.

\begin{figure}
    \centering
    \includegraphics[width=0.5\linewidth]{graphics/autoencoder.png}
    \caption{Visual representation of the autoencoder architecture \cite{kumawat_everything_2023}}
    \label{fig:autoencoders}
\end{figure}

The autoencoder utilized in this work comprises a symmetric encoder-decoder architecture. The encoder is constructed with five convolutional layers, which reduce the spatial dimensions of the input from its initial 224x224 to successive sizes of 112x112, 56x56, 28x28, 14x14, and ultimately 7x7 pixels. This output is flattened and fed into the bottleneck layer, consisting of a feedforward neural network (FFNN) with 200 dimensions. Following this, the decoder performs deconvolution on the bottleneck's output, mirroring the encoder's process in reverse order. The overall architecture of the autoencoder is also depicted in Figure \ref{fig:autoencoder_arch}.
\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{graphics/autoencoder_arch.png}
    \caption{The architecture of the presented Autoencoder}
    \label{fig:autoencoder_arch}
\end{figure}

The training process implemented on the autoencoder involved 80 epochs utilizing the UrbanStreet leaves dataset \cite{yang_urban_2023}. The training was conducted specifically with the masked versions of the leaves, using the provided segmentation mask. The leaves were then cropped to that mask and resized to 224x224 pixels, resembling the data resulting from the segmentation stage. An example can be seen in Figure \ref{fig:leaf_example}

\begin{figure}
    \centering
    \includegraphics[width=0.5\linewidth]{graphics/leaf_dataset_example.png}
    \caption{An example picture from the dataset used for training the autoencoder}
    \label{fig:leaf_example}
\end{figure}

\subsubsection{Convolutional Neural Network classification}
A convolutional neural network (CNN), such as ResNet \cite{he_deep_2015}, can also be used as a classification model. Like an Autoencoder, the network generates a latent representation of an image, referred to as a feature map, which is subsequently processed by a Feed-Forward network (FFN). This FFN is responsible for deriving the likelihood that the input belongs to a specific class. In contrast to the autoencoder, this approach requires supervised learning, i.e., labeled and multiclass data. In the particular use case, a leaf class and a non-leaf class need to be defined for the model to discern features for the various classes \cite{he_deep_2015}.

For the leaf classification task, ResNet-50 was chosen as the specific architecture (illustrated in more detail in Section \ref{sec:meth_resnet}). It differs from pure CNNs in that residual blocks are used instead of purely convolutional blocks (multiple layers, including convolutional layers). Additionally to convolutional blocks, residual blocks contain skip connections. These skip connections add the input of the block directly to the output of the block, allowing the network to learn if the convolutional layers of a given block derive meaningful information (layer output is preferred) or not (the skip connection prevails) \cite{he_deep_2015, choudhary_comprehensive_2023}.
The specific model used was ResNet-50 with weights pre-trained on the ImageNet dataset \cite{deng_imagenet_2009} (\verb |ResNet50_Weights.IMAGENET1K_V2|) sourced from PyTorch's model library \cite{paszke_pytorch_2019}. 

Another CNN selected to tackle the leaf classification task is InceptionV3. Its architecture is highlighted in Section \ref{sec:meth_disease_inception}. This network will be trained the same way as ResNet and is also initialized with pre-trained weights from PyTorch hub \cite{paszke_pytorch_2019} that were obtained by training on the ImageNet \cite{deng_imagenet_2009} dataset.

The fine-tuning of ResNet and Inception comprises 10 epochs, for which the Urban Street Leaves dataset \cite{yang_urban_2023} with the provided segmentation mask applied to match the output of SAM closely will serve as the positive leaf class, and 6,221 objects from the Open Images V7 dataset \cite{kuznetsova_open_2020} with their mask applied as well, will represent the negative non-leaf class. The learning rate throughout the training was scheduled using the one-cycle policy proposed by \citeauthor{smith_super-convergence_2018}. For this, a scheduler combining cosines increases the learning rate from 0.005 to 0.01 in the first quarter of training and decreases it to 0.00001 throughout the rest of the training loop. This type of scheduling for the learning rate aims to reduce overfitting of the model and faster convergence \cite{smith_super-convergence_2018}.

The third CNN employed for leaf classification is YOLOv8 \cite{yao_hp-yolov8_2024}. This network, similar to ResNet, is also formulated for object detection but uses a different internal architecture to produce its predictions, as elaborated in Section \ref{sec:yolo_arch}. The model delineated in Section \ref{sec:yolo_arch} is distinct from the object detection variant in that the latter lacks a segmentation head tasked with generating a mask output. This variant comprises exclusively a classification head and a detection head, which are responsible for generating class and bounding box predictions, respectively.
YOLOv8 for leaf classification will be trained on the Urban Street Leaves \cite{yang_urban_2023} dataset, comprised of 9,763 single leaf images spanning 39 leaf classes, along with a segmentation annotation for each leaf. Training will be carried out for 100 epochs using the training loop in the \textit{ultralytics} Python library \cite{jocher_ultralytics_2023}.

\subsection{Instance Segmentation} \label{sec:meth_instance}

Instance segmentation differs from panoptic segmentation in focusing exclusively on segmenting specific regions within an image. In this context, the targeted segmentation will be limited to the areas containing leaves. This methodology offers a distinct advantage over panoptic segmentation, as a classification stage is rendered unnecessary; instance segmentation inherently encompasses the classification aspect.

The three architectures, for instance segmentation utilized in this work are the aforementioned YOLOv8 \cite{yao_hp-yolov8_2024, jocher_ultralytics_2023}, the Mask R-CNN model \cite{he_mask_2018}, and RetinaNet \cite{lin_focal_2018}. 
It should be noted that the outputs of the presented models differ in design. The Mask R-CNN and the YOLOv8-Segmentation segmentation architecture (YOLOv8seg), a variant of the YOLOv8 that encompasses an additional segmentation head, produce not only bounding boxes but also a binary mask for each detected object. In contrast, RetinaNet only generates bounding boxes for each detected object.

\subsubsection{YOLOv8} \label{sec:yolo_arch}
The architecture of the YOLOv8seg model comprises three main stages: backbone, neck, and head. In the backbone stage, the convolutional layers are organized to detect various features across different spatial resolutions, gaining a robust understanding of the input images and their objects in different sizes. The neck stage integrates feature maps from the backbone to create a comprehensive representation of the detected objects across different scales. This is achieved through feature pyramids, which aid in maintaining an accurate detection rate irrespective of the object's size. In the head stage, the architecture employs multiple heads specializing in distinct prediction tasks: one for bounding-box localization, another for generating segmentation masks, and a third for class prediction. This multi-head configuration allows for a more effective interpretation of the features processed by the previous stages. In general, the structured approach of the YOLOv8seg model contributes to its efficacy in handling object detection and segmentation challenges in real-time applications \cite{pedro_detailed_2023, timilsina_yolov8_2024}. An illustration of the architecture of YOLOv8 can be seen in Figure \ref{fig:yolov8_architecture}.

YOLOv8seg will be trained utilizing the predefined training loop supplied by \textit{ultralytics} integrated within their Python library. The training process will take place over YOLOv8's default training duration of 100 epochs, during each of which the entire synthetic dataset (as discussed in Section \ref{sec:data_aug}) is processed once, followed by a comprehensive evaluation with the validation set. 

\begin{figure}
    \centering
    \includegraphics[width=0.5\linewidth]{graphics/yolov8_architecture.png}
    \caption{Illustration of the building blocks that make up the YOLOv8 architecture \cite{yao_hp-yolov8_2024}}
    \label{fig:yolov8_architecture}
\end{figure}

\subsubsection{Mask R-CNN} \label{sec:method_maskrcnn}
Mask R-CNN is an instance segmentation architecture based on the Faster R-CNN object detection framework \cite{ren_faster_2016}. It consists of the following components \cite{he_mask_2018, potrimba_what_2023}:

\begin{itemize}
    \item \textbf{Backbone Network:} The backbone network is usually a pre-trained object detection network in the likes of \textit{ResNet} \cite{he_deep_2015} and \textit{MobileNet} \cite{howard_mobilenets_2017}. It is responsible for detecting regions in the image that can represent objects.
    \item \textbf{Feature Pyramid Network (FPN):} An FPN generates a Feature Pyramid from the proposed features of the Backbone Network. The subsequent components will utilize this Feature Pyramid to enable the detection of features in different variations of size and scale.
    \item \textbf{Region Proposal Network (RPN):} From the multiscale feature representations of the FPN, the RPN generates proposals for regions of interest (ROI), that is, regions that are likely to contain an object of interest.
    \item \textbf{ROI-Align:} The ROI-Align Stage combines the generated features of the backbone network and the regional proposals of the FPN by dividing the region proposals into a grid and attributing the features of the Backbone Network to the ROI.
    \item \textbf{Mask Head:} This is the main feature that differentiates Mask R-CNN from Faster R-CNN and allows the architecture to produce the segmentation of objects. The Mask Head is a Convolutional Neural Network that produces binary masks for each class from the features obtained by the ROIAlign stage.
\end{itemize}

Mask R-CNN training will be performed using the TensorFlow ModelGarden \cite{yu_tensorflow_2020} python library. This project provides several model configurations and a framework for training and evaluating these models. For Mask R-CNN, a ResNet object detection backbone was used. The training process was configured for 750,000 steps with stochastic gradient descent (SGD) with a momentum of 0.9, utilizing a stepwise decaying learning rate of:
\[
    lr(step)= 
\begin{cases}
    0.12,& \text{if } step \leq 15000\\
    0.012,& \text{if } step > 15000 \land step \leq 20000\\
    0.0054,& \text{if } step > 20000
\end{cases}
\]
Additionally, a linear learning rate warmup phase of 500 steps with a warmup rate of 0.0067 was employed.
This learning rate scheduling aims to achieve an optimal minimum in the model's loss by taking incremental steps toward the smallest loss.

\subsubsection{RetinaNet}
In contrast to the two-stage approach of Mask R-CNN, which involves the separate region proposal- and classification \& bounding box regression stage, RetinaNet proposes a one-stage approach. The differences in the architecture of the one-stage and two-stage architectures can be seen in Figure \ref{fig:maskrcnn_retinanet_arch}.

The RetinaNet architecture is composed of the following components \cite{lin_focal_2018}:

\begin{itemize}
    \item \textbf{Backbone Network:} Similar to Mask R-CNN, RetinaNet utilizes an object detection network as the backbone for instance segmentation. This Backbone network is then combined with a
    \item \textbf{Feature Pyramid Network (FPN)} which creates multiscale representations of the generated features of the Backbone Network. Instead of feeding its output to an RPN, RetinaNet generates $A$ anchors consisting of a class vector indicating the probability that a class is present at this anchor and a 4-element vector describing the bounding box of a predicted object.
    \item \textbf{Classification Subnet:} This Subnet calculates an object class probability vector for each of the $A$ anchors it is given. 
    \item \textbf{Box Regression Subnet:} Parallel to the Classification subnet, the Box Regression Subnet calculates the regression from the predicted Anchor's bounding box vector to the ground truth bounding boxes. This bounding box prediction, in contrast to Mask R-CNN, is class agnostic.
\end{itemize}

RetinaNet training was conducted equally to the training of Mask R-CNN described in Section \ref{sec:method_maskrcnn}.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{graphics/maskrccn_retinanet.png}
    \caption{Difference in architecture between two-stage and one-stage instance segmentation models, shown exemplarily with Faster R-CNN (the version of Mask R-CNN without segmentation) and RetinaNet \cite{carranza-garcia_performance_2021}}
    \label{fig:maskrcnn_retinanet_arch}
\end{figure}

\subsection{Data Augmentation} \label{sec:data_aug}
Many of the datasets in the leaf segmentation domain provide multiple limiting factors to train a segmentation model:
\begin{itemize}
    \item \textbf{Single Instances:} The UrbanStreet dataset \cite{yang_urban_2023} only provides images of single leaves and by that only contains semantic segmentation information, and is thus not suited for training a model on instance segmentation.
    \item \textbf{Low Variability:} The instance segmentation datasets (GrowliFlower \cite{kierdorf_growliflower_2023}, Phenobench \cite{weyler_phenobench_2023} and the Leaf Segmentation Challenge (LSC) dataset \cite{minervini_finely-grained_2016}) have a high specialization, with only a narrow range of different plant species. Each data set provides segmentation data only for a single species or, at most, two species. This will impact the ability of the model to work with generic input data and various plant species.
    \item \textbf{Perspective:} The images are solely captured from a top-down perspective, providing clear visibility and a uniform flat view of plant leaves. This also provides a clear visibility of the arrangement of plants around the stem, which is not always given in real-world usage.
    \item \textbf{Background:} As the images are not densely populated with plants and are captured in a field with soil as a substrate, the background, i.e., the part of the picture that is not a plant, is visually clearly separable from the plant leaves themselves. This does not provide viable data for distinguishing leaf regions from various background conditions present in images captured in real-world settings.
    \item \textbf{Data quality:} The Phenobench dataset \cite{weyler_phenobench_2023} does not present the quality of segmentation data necessary to train a segmentation model reliably. The annotations contained in this dataset are supposed to represent single-leaf instances. However, upon inspection, it is evident that most of the masks that should only contain single leaf regions wrongly contain multiple leaf regions. This severely limits a model's ability to learn features identifying single-leaf regions.
\end{itemize}

Due to these limitations, existing datasets present challenges regarding their applicability in real-world scenarios and suitability for training. Consequently, a data augmentation technique will be employed to produce synthetic datasets for training, validating, and assessing the models.
In this context, a synthetic dataset will be created that contains multiple leaves in front of various backgrounds with their corresponding segmentation data. To achieve this, an approach similar to the Na"ive Collage technique proposed by \citeauthor{kuznichov_data_2019} will be utilized \cite{kuznichov_data_2019}.
This approach is based on the UrbanStreet \cite{yang_urban_2023} dataset. The dataset consists of 9,763 leaf images across 39 plant species captured in city scenes, provided in conjunction with a segmentation mask. Each image provides a single leaf and the respective leaf segmentation mask, providing optimal data for generating a synthetic dataset.

From this dataset, a random amount of images $n \in_R [N_{min};N_{max}]$ is selected, and then individual leaves are masked from the original RGB image using the provided mask, applying a pixel-wise Boolean operation. Subsequently, a set of random transformations is applied:
\begin{itemize}
    \item Rotation with an angle $\theta \in_R [-\theta_r; \theta_r]$
    \item Translation with offset values $x \in_R [-\mathcal{O}_X; \mathcal{O}_X]$ and $y \in_R [-\mathcal{O}_Y; \mathcal{O}_Y]$
    \item Scaling with a factor of $s \in_R [s_l; s_u]$ after resizing to the resulting image size
\end{itemize}

This generated leaf overlay is then combined with a randomly sampled image from the City Street View Dataset \cite{stealth_username_city_2022}, a dataset that comprises 50,000 images obtained from Google's Street View API across five cities: San Francisco, Detroit, Chicago, Washington, and New York City. These background images present diverse scenes and provide various background conditions for the dataset.

Using this method, a dataset consisting of 8,000 training images, 2,000 validation images, and 2,000 test/evaluation images will be created.
In this specific case, the following parameters for the dataset generation are used: $N_{min} = 4$, $N_{max} = 10$, $\theta_r = 45\deg$, $\mathcal{O}_x = \mathcal{O}_y = 100px$, $s_l = 0.2$ and $s_u = 0.7$.
Figure \ref{fig:data_aug_example} shows a sample image created with the data augmentation.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\linewidth]{graphics/data_aug_example.png}
    \caption{An example picture generated by the presented Na\"ive Data Augmentation algorithm}
    \label{fig:data_aug_example}
\end{figure}

\section{Disease Detection} \label{sec:method_disease}
A considerable amount of research has already been conducted on classification tasks similar to disease detection. Traditionally, the models employed in classification are designed to differentiate between multiple distinct classes, such as distinguishing between an image of a cat and one of a dog. The two classes in this case do not possess such a high distinctiveness. The healthy class shares numerous features with the diseased class, including shape, color, structure, and texture; the primary distinctions lie in a few minor features, which are the regions indicative of the disease on the leaf. The minuscule differences between healthy and diseased plants can be seen in \ref{fig:healthy_diseased_arjun}, illustrating the intricacies of differentiating between healthy and diseased leaves.


\begin{figure}
    \centering
    \includegraphics[width=0.75\linewidth]{graphics/healthy_diseased_arjuna.jpg}
    \caption{Picture of a healthy (left) and diseased (right) leaf of the Terminalia arjuna plant from the PlantLeaves dataset \cite{chouhan_database_2019}}
    \label{fig:healthy_diseased_arjun}
\end{figure}

For the disease detection stage, eight different models will be discussed (see Subsections below). The models in discussion will be trained using a combination of the PlantaeK \cite{kour_plantaek_2019}, PlantVillage \cite{hughes_open_2016}, and PlantLeaves \cite{chouhan_database_2019} datasets. The datasets contain 2,153, 54,303, and 4,502 images of plant leaves, respectively, which are healthy or diseased, with a different set of diseases for each dataset. As this work aims to solely classify whether plants are healthy or diseased and not classify the disease itself, all images in any unhealthy categories are merged into a single \textit{diseased} category. The train, validation, and test sets are split from this resultant aggregated dataset by an 80\%, 10\% \& 10\% ratio. The resulting training, validation, and test sets consist of 19,918, 2,489, \& 2,489 and 41,954, 5,244 \& 5,244 healthy and diseased leaf images, respectively.

The training loop for all models discussed will be run for 36 epochs, during each of which the whole dataset is processed once. The training will be halted if no decrease in the loss of the validation set is detected. Specifically, training will stop after the trainer does not detect any change in validation loss in the range of $\delta = 0.1$ throughout five epochs. This will prevent the models from overfitting, as no improvement in the validation loss indicates a lack of progress in generalization.
The training loop incorporates a cosine annealing/decay learning rate scheduler, with a quarter epoch warm-up phase, using an initial learning rate of 0.001, increasing to 0.01 during warmup and decaying to $1 \cdot 10^{-9}$ as the final learning rate throughout the training. The course of the learning rate is shown in Figure \ref{fig:disease_detection_lr}. This learning rate scheduling is designed to achieve faster and more optimal convergence compared to static or stepwise learning rate schedules as described in \cite{liu_super_2022}.

To closely match the output of the segmentation stage, which is a single leaf that fills the image in front of a black background (see Figure \ref{fig:leaf_example}), for every image in the dataset the previously discussed panoptic segmentation approach will be used to create a segmented version of the image. For this, SAM will be used again with a leaf classifier, namely ResNet, to determine the leaf region in the image. SAM will be prompted with the image's center point, as upon visual inspection most of the images in the dataset are centered over the leaf. From the SAM-generated region proposals $p$ the one with the highest score $S_{mask}$ (see Equation \eqref{eqn:mask_score}, where $C_{SAM}$ denotes the mask confidence generated by SAM and $C_{leaf}^{ResNet}$ is the probability that the region $m$ is a leaf generated by the ResNet leaf classifier discussed in Section \ref{sec:meth_panoptic}).

\begin{equation} \label{eqn:mask_score}
    S_{mask}(m) = C_{SAM}(m) \cdot C_{leaf}^{ResNet}(m)
\end{equation}

\begin{figure}
    \centering
    \includegraphics[width=0.5\linewidth]{graphics/disease_detection_training_lr.png}
    \caption{Learning rate over epochs utilizing cosine annealing learning rate scheduler}
    \label{fig:disease_detection_lr}
\end{figure}

\subsection{AlexNet}
The AlexNet architecture is a fundamental model proposed by \citeauthor{krizhevsky_imagenet_2012}. It comprises three convolutional blocks, with the first two featuring a single convolutional layer each, while the third block contains three convolutional layers, all utilizing the Rectified Linear Unit (ReLU) activation function. Each convolutional block is concluded with a MaxPooling layer. Subsequently, the convolutional blocks are followed by three fully connected feedforward layers (FF) that produce the classification probabilities \cite{krizhevsky_imagenet_2012}.
The AlexNet model was sourced from the deep-cv library of \citeauthor{weill_keras-deepcv_2017} \cite{weill_keras-deepcv_2017} implemented in Keras \cite{chollet_keras_2015}.

\subsection{VGG-19}
The Visual Geometry Group (VGG) networks expand the AlexNet architecture. The VGG-19 model uses similar convolutional blocks, with 19 indicating the presence of 19 convolutional layers (compared to the five present in AlexNet) \cite{simonyan_very_2015}. 
Contrary to AlexNet, in the VGG networks, a varying number (between 2 and 3) of convolutional layers are grouped into a block, which is concluded by a MaxPooling layer.
The structure of a smaller VGG-16 network can be seen in Figure \ref{fig:vgg16}.
The specific VGG-19 implementation model was sourced from the deep-cv library by \citeauthor{weill_keras-deepcv_2017} \cite{weill_keras-deepcv_2017} implemented in Keras \cite{chollet_keras_2015}.

\begin{figure}
    \centering
    \includegraphics[width=0.87\linewidth]{graphics/vgg_16.png}
    \caption{Architecture of the VGG-16 image recognition network \cite{simonyan_very_2015, bangar_vgg-net_2022}}
    \label{fig:vgg16}
\end{figure}

\subsection{ResNet} \label{sec:meth_resnet}
The Residual Networks group (ResNets) represents an advancement over the purely convolutional architecture exemplified by VGG networks. Instead of a strictly convolutional block, they utilize the residual block. It is a block of (usually two) convolutional layers. Still, it contains a concatenate function at the end that joins the output of the preceding block with the original input of the current \cite{he_deep_2015}. Figure \ref{fig:resnet_block} also visualizes the described behavior. This allows the network to learn the "importance" of any given block, as it can leverage whether to use the block output or skip it entirely. It also aims to reduce overfitting and optimize the training process.
The \verb | ResNet152V2 | implementation provided by the Keras framework \cite{chollet_keras_2015} without pre-trained weights is used for training.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.85\linewidth]{graphics/resnet.png}
    \caption{The mode of operation for a residual block in ResNet \cite{ruiz_understanding_2018, he_deep_2015}}
    \label{fig:resnet_block}
\end{figure}

\subsection{ConvNeXt} \label{sec:arch_convnext}
ConvNeXt presents an advancement of the ResNet architecture with many changes inspired by the Vision Transformer (described in Section \ref{sec:method_vit}). One of these changes is to utilize larger kernel sizes in the convolutional layer to gain a wider receptive field. Instead of using ResNet's 3x3 kernel size, ConvNeXt has a kernel size of 7x7, approaching the global attention of the Vision Transformer. ConvNeXt uses the Gaussian error linear unit (GELU) activation function instead of ResNet's Rectifying Linear Unit (ReLU) to improve training performance and model generalization. In addition, instead of using regular convolutional layers, ConvNeXt employs the usage of depthwise convolutional layers, which apply convolution to each of the channels instead of applying one to all of them simultaneously, which allows retaining channel-specific information throughout the model \cite{pandey_depth-wise_2018, singh_convnext_2022} and decrease computational cost.
The architecture of the ConvNeXt network can be seen in Figure \ref{fig:arch_convnext}.
The implementation used for this work is the large variant (ConvNeXtLarge) sourced from the Keras framework \cite{chollet_keras_2015} and was initialized without any pre-training weights applied to it.

\begin{figure}
    \centering
    \includegraphics[width=0.75\linewidth]{graphics/convnext_arch.png}
    \caption{Architecture of a ConvNeXt network \cite{chen_large-scale_2023}}
    \label{fig:arch_convnext}
\end{figure}

\subsection{MobileNet}
MobileNet, as its name suggests, was developed to fit the needs of more efficient Deep Convolutional Neural Networks for mobile devices. For this, depthwise separable convolution is employed instead of standard convolution, which reduces computational cost while achieving similar results. Depthwise separable convolutions combine depthwise convolution (discussed in Section \ref{sec:arch_convnext}) and pointwise convolutions, reducing their computational cost significantly \cite{howard_mobilenets_2017}. The model used in this work is the 3rd iteration of MobileNet, MobileNetV3. It utilizes a specific activation function called \textit{h-swish}, which replaces the computationally expensive swish activation function by omitting the sigmoid in favor of ReLU while producing similar results. Another adaptation made with version 3 is the introduction of the Squeeze-and-Excite (SE) block. This takes the output of a convolutional layer and aims to model the interdependencies between color channels by passing the squeezed output of the convolutional layer through a series of linear layers and multiplying the output with the filter outputs of the convolutional layer, illustrated in Figure \ref{fig:se_block} \cite{howard_searching_2019, erdogan_squeeze-and-excitation_2022}.
The complete architecture of MobileNetV3 is shown in Figure \ref{fig:arch_mobilenetv3}. The specific implementation for MobileNetV3 was sourced from the Keras framework \cite{chollet_keras_2015} and initialized without applying pre-trained weights.

\begin{figure}
    \centering
    \includegraphics[width=0.3\linewidth]{graphics/se_block.png}
    \caption{Illustration of the makeup of a SE block \cite{erdogan_squeeze-and-excitation_2022}}
    \label{fig:se_block}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.5\linewidth]{graphics/arch_mobilenetv3.png}
    \caption{Architecture of the MobileNetV3 architecture \cite{elsayed_abd_elaziz_evolution_2023}}
    \label{fig:arch_mobilenetv3}
\end{figure}

\subsection{InceptionV3} \label{sec:meth_disease_inception}
The InceptionV3 architecture is the second iteration of the original Inception/GoogLeNet architecture proposed by \citeauthor{szegedy_going_2014} \cite{szegedy_going_2014}. The novelty of this architecture is the introduction of Inception blocks, which process input through the application of varying convolutional operations, depending on the input size, as seen in Figure \ref{fig:inception_arch}. This allows the network to capture diverse spatial features simultaneously compared to standard convolution. This parallel application of convolutions also reduces the model's depth and thus prevents overfitting while achieving improved generalizability through multi-scale convolutions. 

\begin{figure}
    \centering
    \includegraphics[width=0.65\linewidth]{graphics/inception_arch.png}
    \caption{The architecture of Inception V3 network \cite{iparraguirre-villanueva_convolutional_2022}}
    \label{fig:inception_arch}
\end{figure}

\subsection{Vision Transformer} \label{sec:method_vit}
The Vision Transformer (ViT) represents a relatively recent advancement in neural network architecture, introduced by \citeauthor{dosovitskiy_image_2021} in 2021. Transformers have garnered considerable attention in the domain of Natural Language Processing (NLP) tasks \cite{vaswani_attention_2023}. 
The ViT approach is designed to apply the principles that a transformer uses for text processing to images. The architecture of the vision transformer is also quite similar. But instead of using Text Embeddings to transform words and sentences into the fitting vector space, the ViT utilizes image embeddings on patches created from a regular grid over the image. These patches reduce the amount of data ingested by the network at once. The aforementioned image embeddings are implemented as convolutional layers whose output is flattened to translate the images into the designated vector space. Otherwise, the architecture of the vision transformer is similar to the original transformer architecture described in \cite{vaswani_attention_2023} \cite{dosovitskiy_image_2021}. The architecture is visualized in Figure \ref{fig:vit_arch}.
The specific implementation of VisionTransformer was sourced from the \verb|vit_keras| library by \citeauthor{fausto_morales_vit_keras_2020} \cite{fausto_morales_vit_keras_2020}, which port the Google \textit{FLAX} implementation \cite{dosovitskiy_image_2021, tolstikhin_mlp-mixer_2021, steiner_how_2021, chen_when_2021, zhuang_surrogate_2022, zhai_lit_2022} to Keras. The model is loaded without pre-trained weights.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.7\linewidth]{graphics/vit_arch.png}
    \caption{Architecture of the Vision Transformer \cite{wolfe_using_2022}}
    \label{fig:vit_arch}
\end{figure}

\chapter{Results} \label{sec:results}
This research aimed to determine the efficacy of various machine learning architectures and models in detecting diseases visually manifested in plants and, more specifically, plant leaves. For this, a pipeline comprised two parts: leaf segmentation and disease detection at the top level. For each of these parts and their respective subdivisions, the results will be presented individually, i.e., the segmentation stage, divided into panoptic and instance segmentation, and the disease detection stage, as well as a complete evaluation that examines the performance of the entire pipeline.

\section{Leaf Segmentation} \label{sec:results_segmentation}
The leaf segmentation stage of the pipeline is responsible for providing the subsequent stage with a set of segmented regions of the image that contain leaves. The proposed approaches to leaf segmentation (see Section \ref{sec:method_segmentation}) were evaluated on both a synthetically generated dataset (described in Section \ref{sec:data_aug}) and a real-world dataset, namely the leaf segmentation dataset \cite{giovi_leaf_2024}.
This dataset was used as it contains leaf segmentation data with images taken in real-world settings with complicated backgrounds similar to the segmentation target. The dataset comprises 351 images with an average of $1.42$ annotations per image. Although this dataset is not a perfect candidate for evaluation as the background often also contains leaf regions not included in the ground-truth segmentation mask, it is the most suitable option given the lack of better alternatives at the time of this study. However, it serves as a good discriminator of a model's performance to segment leaves in front of a complicated background. 
For every model the \textit{true positives (TP)}, \textit{false positives (FP)}, \textit{true negatives (TN)} and \textit{false negatives (FN)} were tracked and from that the metrics \textit{precision}, \textit{recall}, \textit{F1 score}, \textit{mean Intersection over Union (IoU)}, \textit{mean Dice score} and \textit{specificity} were calculated with the formulas in Equation \eqref{eqn:metrics_formulas}:

\begin{equation} \label{eqn:metrics_formulas}
\begin{aligned}
    accuracy_{x} & = \frac{TP_{x} + TN_{x}}{TP_{x} + TN_{x} + FP_{x} + FN_{x}}
    \\
    precision_{x} & = \frac{TP_{x}}{TP_{x} + FP_{x}}
    \\
    recall_{x} & = \frac{TP_{x}}{TP_{x} + FN_{x}}
    \\
    specificity_{x} & = \frac{TN_{x}}{TN_{x} + FP_{x}}
    \\
    F1_{x} & = \frac{2 * (precision_{x} * recall_{x})}{(precision_{x} + recall_{x})}
    \\
    IoU_{x} & = \frac{TP_{x}}{TP_{x} + FN_{x} + FP_{x}}
    \\
    Dice_{x} & = \frac{2 * TP_{x}}{2 * TP_{x} + FP_{x} + FN_{x}}
\end{aligned}
\end{equation}

The mean Dice coefficient (mDC) will be the primary evaluation metric as it shows the general performance of a segmentation model, is more sensitive to region overlap than the IoU score, and is agnostic towards the region area compared to the IoU. The mDC is calculated as an average over all dice coefficients achieved on the dataset $\mathcal{X}$ as in Equation \eqref{eqn:mdc_formula}: 
\begin{equation} \label{eqn:mdc_formula}
    mDC_{\mathcal{X}} = \frac{\sum_{x\in\mathcal{X}} Dice_x}{|\mathcal{X}|} \text{\hspace{1.8em} where } \mathcal{X} \text{ is a dataset}
\end{equation}

\subsection{Total evaluation}
\subsubsection{Synthetic dataset evaluation}
Table \ref{tab:metrics_segmentation_synthetic} shows the evaluation performance of the models presented against the synthetically generated test dataset. 
The combination of ResNet and SAM achieved the highest overall performance and scores across all metrics, with an mDC of 0.8628. This shows that ResNet provides a strong classification model on top of SAM for leaf segmentation.
YOLOv8 performs poorly, with the lowest scores of all models discussed across all metrics, demonstrating its inferiority in segmentation capabilities compared to the other models based on SAM.
On the synthetic dataset, YOLOv8 performed the worst as a leaf classifier in combination with SAM, significantly lagging behind SAM + ResNet and SAM + Inception, attaining mediocre scores across all metrics.
Although the Autoencoder attained a higher mDC than YOLOv8 in combination with SAM, it significantly lacks in the other metrics with a recall of 0.2588 versus YOLOv8 + SAM's 0.3204, showing its shortcomings as a leaf classifier. 
These results generally highlight the superiority of SAM-based models, particularly those that leverage ResNet, while emphasizing the challenges of achieving high precision and recall in segmentation tasks.

\begin{table}[]
    \centering
    \begin{tabular}{lrrrrr}
    \toprule
     & YOLOv8 & ResNet (S) & Inception (S) & AE (S) & YOLOv8 (S) \\
    \midrule
    Precision & 0.0075 & \textbf{0.2543} & 0.1803 & 0.0469 & 0.0892 \\
    Recall & 0.0092 & \textbf{0.5921} & 0.4424 & 0.2588 & 0.3204 \\
    F1-Score & 0.0079 & \textbf{0.3391} & 0.2410 & 0.0758 & 0.1095 \\
    Mean IoU & 0.0131 & \textbf{0.8257} & 0.7475 & 0.6064 & 0.4128 \\
    Mean Dice & 0.0167 & \textbf{0.8628} & 0.7890 & 0.6314 & 0.4341 \\
    \bottomrule
    \end{tabular}
    \caption{Table of precision, recall, F1-score, mean IoU and mean Dice coefficient of the evaluation of the leaf segmentation stage on synthetic test data, (S) indicates SAM-based models}
    \label{tab:metrics_segmentation_synthetic}
\end{table}

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{graphics/metrics_segmentation_synthetic.png}
    \caption{Bar graph of precision, recall, F1-score, mean IoU, and mean Dice coefficient of the evaluation of the leaf segmentation stage on synthetic data}
    \label{fig:metrics_segmentation_synthetic}
\end{figure}

\subsubsection{Real-world dataset evaluation}
In Table \ref{tab:metrics_segmentation}, which compares various model configurations for leaf segmentation evaluated on a real-world dataset, notable trends emerge across the evaluation metrics.
Interestingly, all models except YOLOv8 in combination with SAM demonstrated significantly worse performance than in the synthetic dataset, which could be attributed to the lacking quality of the synthetic dataset and its sub-par transferability to real-world data. SAM + YOLOv8 achieves the highest scores, outperforming other models across all metrics with a mean Dice score of 0.6697. This suggests strong segmentation performance with high overlap accuracy, contradicting the evaluation findings on the synthetic dataset, where YOLOv8, in combination with SAM, only achieved average performance. 

SAM + ResNet is the second-best-performing model in all metrics, with SAM + Inception being a close third-place competitor, both of which achieved mediocre evaluation metrics. In contrast, YOLOv8, as an instance segmentation model, performed poorly, showing values less than 0.01 in every metric, indicating that it cannot generalize synthetic training data to the real-world dataset. The Autoencoder with SAM shows an improved precision of 0.0757 on the real-world data compared to synthetic but still retains a low overall performance with an mDC of 0.3557. This indicates that almost all segmentation approaches struggle significantly with segmenting leaves in real-world data compared to the synthetic dataset. Only YOLOv8 excels on real-world data, significantly exceeding all other models, showing a high generalization and leaf classification performance.
These findings are also illustrated in the bar graph in Figure \ref{fig:metrics_segmentation}.

\begin{table}[]
    \centering
    \begin{tabular}{lrrrrr}
    \toprule
     & YOLOv8 & ResNet (S) & Inception (S) & AE (S) & YOLOv8 (S) \\
    \midrule
    Precision & 0.0050 & 0.1245 & 0.1224 & 0.0757 & \textbf{0.3507} \\
    Recall & 0.0050 & 0.4403 & 0.3655 & 0.3465 & \textbf{0.6719} \\
    F1-score & 0.0044 & 0.1752 & 0.1599 & 0.1078 & \textbf{0.4264} \\
    Mean IoU & 0.0040 & 0.4846 & 0.4136 & 0.3384 & \textbf{0.6407} \\
    Mean Dice & 0.0050 & 0.4967 & 0.4259 & 0.3557 & \textbf{0.6697} \\
    \bottomrule
    \end{tabular}
    \caption{Table of evaluation Metrics of the leaf segmentation stage on real-world data, (S) indicates SAM-based models}
    \label{tab:metrics_segmentation}
\end{table}

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{graphics/metrics_segmentation.png}
    \caption{Bar graph of evaluation metrics of the leaf segmentation stage on real-world data}
    \label{fig:metrics_segmentation}
\end{figure}

\subsection{Panoptic Segmentation} \label{sec:results_segmentation_panoptic}
The panoptic segmentation approach to the first stage of the pipeline includes the Segment Anything model (SAM) \cite{kirillov_segment_2023} generating panoptically segmented image regions, which a classification model subsequently classifies. 
No assessment was performed since SAM is a pre-trained model that is not specifically tailored for this study but a state-of-the-art panoptic segmentation model.
Each classifier used with SAM was additionally evaluated regarding its leaf classification capabilities. The already discussed leaf segmentation dataset \cite{giovi_leaf_2024} was used for this, as it provides various single-leaf images and their corresponding segmentation masks. For each image in the set, the corresponding segmentation mask was applied to closely mirror the output of the SAM. As a negative (non-leaf) class, a random subset spanning 872 samples of Google's Open Images V7 dataset \cite{kuznetsova_open_2020}, which contains various random objects. 
The models' accuracy, precision, recall, and F1 score metrics were collected during the evaluation loop. These results are illustrated in Table \ref{tab:panoptic_classification_metrics} and Figure \ref{fig:panoptic_classification_eval_metrics}. 

ResNet delivers the best performance in all metrics, far outweighing the other models with its F1 score of 0.9579, which shows its ability to generalize reliably.
InceptionV3 closely follows ResNet's performance, with the second-best overall score. Its F1 score of 0.9268 shows its strong ability to generalize and reliably distinguish between leaf and non-leaf regions.
YOLOv8 achieves a decent precision of 0.8028 but exhibits lower scores than ResNet and Inception in recall. This shows the model's inability to correctly identify prevalent plant leaf regions, i.e., a high false negative rate. The Autoencoder falls behind YOLOv8, Inception, and ResNet in precision and F1 score, exhibiting poor performance in classifying non-leaf regions as such, i.e., a high false positive rate. In the confusion matrix in Figure \ref{fig:panoptic_classification_conf_matrix}, you can see that the Autoencoder and YOLOv8 are capable of predicting positive examples mostly correctly but struggle to classify non-leaf examples with similar performance to the random baseline.

Generally, the classifier step evaluation results are accurately aligned with the evaluation of their use in combination with SAM. However, the precision in segmentation is much lower due to misclassifications based on various factors, showing that a classifier's performance significantly decreases in combination with SAM compared to its sole exertion.

\begin{table}[]
    \centering
    \begin{tabular}{lrrr}
    \toprule
    & Precision & Recall & F1-score \\
    \midrule
    Autoencoder & 0.6718 & 0.6556 & 0.6472 \\
    ResNet & \textbf{0.9580} & \textbf{0.9579} & \textbf{0.9579} \\
    YOLOv8 & 0.8028 & 0.7175 & 0.6960 \\
    Inception & 0.9272 & 0.9268 & 0.9268 \\
    \bottomrule
    \end{tabular}
    \caption{Evaluation metrics of the panoptic classifier models}
    \label{tab:panoptic_classification_metrics}
\end{table}

\begin{figure}
    \centering
    \includegraphics[width=0.8\linewidth]{graphics/panoptic_classification_eval_metrics.png}
    \caption{Evaluation metrics of the different classification models in panoptic segmentation}
    \label{fig:panoptic_classification_eval_metrics}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{graphics/confusion_matrix_classifiers.png}
    \caption{Confusion Matrix of the leaf classifier models}
    \label{fig:panoptic_classification_conf_matrix}
\end{figure}

\subsection{Instance Segmentation} \label{sec:results_segmentation_instance}
For instance segmentation, the models presented were trained to segment all regions containing a leaf from a given input image. The Mask R-CNN, RetinaNet, and YOLOv8 architectures have been selected for this task.
For this method, YOLOv8 emerged as the single viable model. Section \ref{sec:results_segmentation} illustrates its performance compared to the SAM-based models.
The selection was made based on the training performance of the different models. The detailed results of the training can be seen in Section \ref{sec:leaf_segmentation_instance_training}. 

A notable pattern was shown during the training of Mask R-CNN and RetinaNet. While Mask R-CNN was able to converge in its training loss and validation mean average precision (mAP) and mean average recall (mAR) of its bounding boxes, RetinaNet did not experience the same behavior but exhibited unstable training and validation performance. This indicates that it is not able to generalize with the presented data. For this reason, RetinaNet will not be evaluated but omitted from this study. 

When comparing Mask R-CNN's and YOLOv8's segmentation performance, it also shows that YOLOv8 is superior to Mask R-CNN in terms of segmentation mask performance. While YOLOv8 reached a maximum mAP of 0.679, the best value of Mask R-CNN for this metric was 0.303. The difference in mAP between YOLOv8's 0.540 and Mask R-CNN's 0.194 further underlines the latter model's inferiority to the former. Due to this poor performance, Mask R-CNN will also not be regarded further in this study.

\section{Disease detection} \label{sec:results_disease}
The disease detection models were evaluated on the combined dataset's test split, also used for training and validation (described in Section \ref{sec:method_disease}). During the evaluation, metrics for precision, binary accuracy, recall, area under curve (AuC), and the F1 score were collected. The evaluation findings are illustrated in Table \ref{tab:eval_metrics_disease_detection_masked} and Figure \ref{fig:confusion_matrix_disease_detection_masked}.

AlexNet emerged as the best-performing model, achieving a precision of 0.8674, a recall of 0.8305, and an F1 score of 0.8261. Its accuracy and AuC also reached 0.8305, indicating its strong generalization and suitability.

The models that showed mediocre performance during training also reflect the difficulty in adapting to disease classification in the evaluation, with VisionTransformer reaching a precision of 0.4805 and an F1 score of 0.3340 and MobileNetV3Large attaining better but still sub-par performance with a precision of 0.6778 and an F1 score of 0.3376. This shows that these models are not viable for this task.

VGG19 and ConvNeXtLarge, which already encountered problems during training (see Section \ref{sec:disease_detection_result_training}), only produced predictions for diseased leaves regardless of the actual input (see Figure \ref{fig:confusion_matrix_disease_detection_masked}), producing identical F1 scores of 0.3333. This uniformity suggests that these models struggled to learn effectively from the dataset, likely due to underfitting or limitations in their architectures for the specific task. These results underscore the varying strengths of the models tested, and AlexNet outperforms its counterparts in this evaluation.


\begin{table}[]
    \centering
    \begin{tabular}{lrrrrr}
    \toprule
     & Precision & Recall & F1-score & Accuracy & AuC \\
    \midrule
    InceptionV3 & 0.8354 & 0.7784 & 0.7685 & 0.7784 & 0.7784 \\
    VisionTransformer & 0.4805 & 0.4999 & 0.3340 & 0.4999 & 0.4999 \\
    AlexNet & \textbf{0.8674} & \textbf{0.8305} & \textbf{0.8261} & \textbf{0.8305} & \textbf{0.8305} \\
    ResNet152V2 & 0.8340 & 0.7786 & 0.7691 & 0.7786 & 0.7786 \\
    MobileNetV3Large & 0.6778 & 0.5017 & 0.3376 & 0.5017 & 0.5017 \\
    VGG19 & 0.2500 & 0.5000 & 0.3333 & 0.5000 & 0.5000 \\
    ConvNeXtLarge & 0.2500 & 0.5000 & 0.3333 & 0.5000 & 0.5000 \\
    \bottomrule
    \end{tabular}
    \caption{Evaluation metrics (precision, recall, F1-score, accuracy and AuC) of the disease detection models}
    \label{tab:eval_metrics_disease_detection_masked}
\end{table}

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{graphics/conf_matrix_disease_detection_2x4.png}
    \caption{Confusion matrix over evaluation data of all discussed disease detection models}
    \label{fig:confusion_matrix_disease_detection_masked}
\end{figure}

The performance of the discussed method and models lags behind that of related literature. For example, in \cite{brahimi_deep_2018}, various models were trained on the discussed PlantVillage dataset \cite{hughes_open_2016}, each using three learning techniques, achieving an accuracy of up to 99.75\%. The best result was achieved by InceptionV3, where the entire network (as opposed to only the fully connected layers) was fine-tuned on ImageNet weights. The other models they discussed reached similar accuracies, for example, AlexNet achieved 99.24\% and ResNet34 99.67\%. The lack of results of this method can be attributed to several factors. Firstly, the research utilized pre-trained models and fine-tuned them instead of training models from scratch, as done in this work. This allows the models to transfer their ``skills`` to a new domain instead of having to learn everything new. Secondly, instead of summarizing all diseased classes into one, \citeauthor{brahimi_deep_2018} classified each disease class separately, allowing models to learn the characteristics that make up a specific diseased leaf for each class individually, which may lead to improved disease detection performance. Third, they introduced a background class into the dataset, which allowed the models to differentiate between relevant information, such as leaf regions, and the background containing no relevant information. Although these results can not be compared directly, it shows that some optimizations can be made to the proposed methodology.

The metrics gathered during the training process are illustrated in Appendix \ref{sec:disease_detection_result_training}.

\section{Complete Pipeline} \label{sec:results_total}
In the last two sections, the performance of each stage was evaluated individually. In this section, evaluations of the compound of both stages were performed. The evaluation was performed with the data provided in the PlantDoc dataset \cite{singh_plantdoc_2020}. This dataset contains 2,922 images, of which 916 are of healthy plants, and 2,006 are infected with various diseases.
In the evaluation loop, a set of constraints for each leaf region $\mathcal{R}$ was imposed, selecting specific areas based on their sharpness $sh(\mathcal{R}) = avg(\sqrt{(\nabla_x \mathcal{R}_{gray})^2 + (\nabla_y \mathcal{R}_{gray})^2})$, size, and leaf probability predicted by the leaf classifiers. These selections were based on thresholding a specific value (leaf probability) or selecting the top N candidates (region area and sharpness). 
For each pipeline configuration, the optimal values for these constraints were determined by an iterative search algorithm, which looped through multiple candidates for each hyperparameter and determined the best-performing combination of a model on the evaluation dataset in terms of its AuC. In addition, the optimal threshold for classifying leaves as healthy or diseased was determined this way. The results of this search algorithm are summarized in Table \ref{tab:results_complete_optimal_search}.

\begin{table}[]
    \centering
    \begin{tabular}{lrrrrr}
    \toprule
    & \makecell[r]{DISEASED\\THRESH-\\OLD} & \makecell[r]{LEAF\\THRESH-\\OLD} & \makecell[r]{N\\SELECT\\AREA} & \makecell[r]{N\\SELECT\\SHARPNESS} \\
    \midrule
    \makecell[l]{Inception (S) + AlexNet} & 0.8000 & 0.9000 & 4 & 2 \\
    \makecell[l]{Inception (S) + InceptionV3} & 0.9375 & 0.8000 & 3 & 2 \\
    \makecell[l]{Inception (S) + ResNet152V2} & 0.9500 & 0.8500 & 2 & 2 \\
    \makecell[l]{ResNet (S) + AlexNet} & 0.8500 & 0.8000 & 3 & 2 \\
    \makecell[l]{ResNet (S) + InceptionV3} & 0.9500 & 0.8000 & 2 & 2 \\
    \makecell[l]{ResNet (S) + ResNet152V2} & 0.9500 & 0.8000 & 2 & 2 \\
    \makecell[l]{YOLOv8 (S) + AlexNet} & 0.8000 & 0.9250 & 4 & 2 \\
    \makecell[l]{YOLOv8 (S) + InceptionV3} & 0.9500 & 0.8000 & 2 & 2 \\
    \makecell[l]{YOLOv8 (S) + ResNet152V2} & 0.9500 & 0.8000 & 2 & 3 \\
    \bottomrule
    \end{tabular}
    \caption{Optimal leaf region selection parameters determined by iterative search by pipeline configuration}
    \label{tab:results_complete_optimal_search}
\end{table}

The evaluation of pipeline configurations, as shown in Tables \ref{tab:results_complete_optimal_search} and \ref{tab:total_eval}, demonstrates the performance diversity between different model combinations, influenced by both classification metrics and the optimal selection of hyperparameters for the constraints of the leaf region. Among the classification results, YOLOv8 (S) + AlexNet emerged as the best-performing configuration, achieving the highest AuC of 0.6971 and an F1 score of 0.6923. 
\iffalse
This configuration's strong performance can be attributed to its effective balance between precision and recall, supported by its optimal hyperparameter settings: a diseased threshold of 0.8000, a leaf probability threshold of 0.9250, and the selection of four regions based on area and two based on sharpness. 
\fi
Similarly, Inception (S) + AlexNet demonstrated competitive classification performance, with an F1-score of 0.6607 and an accuracy of 0.6643, benefiting from similar hyperparameter settings.

The iterative search algorithm used to optimize the constraints in the leaf region identified distinct patterns across the pipeline configurations. Configurations such as YOLOv8 (S) + ResNet152V2 and YOLOv8 (S) + InceptionV3, which achieved moderately lower classification metrics (F1 scores of 0.6012 and 0.6274, respectively), were characterized by stricter hyperparameter settings, with diseased thresholds of 0.9500 and the selection of fewer regions (2 for area and either 2 or 3 for sharpness). Similarly, ResNet-based combinations generally demonstrated lower classification performance, with ResNet (S) + ResNet152V2 achieving the lowest F1-score of 0.5884 and an accuracy of 0.6107. These configurations also consistently required tighter thresholds, such as a diseased threshold of 0.9500, likely reflecting their reduced capacity to generalize in the classification task.

Interestingly, configurations integrating InceptionV3 in the disease detection stage, such as Inception (S) + InceptionV3 and ResNet (S) + InceptionV3, achieved slightly better results in terms of classification performance compared to other setups based on ResNet. However, they achieved similar performance in disease detection evaluation. For example, Inception (S) + InceptionV3 achieved an F1-score of 0.6428 and a recall of 0.6563, while Inception (S) + ResNet152V2 only reached 0.6098 and 0.6254, respectively. This suggests that InceptionV3 contributes to a more nuanced disease detection process, particularly when paired with models like Inception or ResNet.

Overall, the results indicate that including AlexNet in the pipeline consistently enhances performance, particularly when paired with YOLOv8 or Inception. The iterative search for hyperparameters highlights the importance of balancing thresholds for diseased classification and region selection criteria, with the best-performing configurations often requiring more lenient diseased thresholds and a higher number of selected regions. These findings underscore the critical role of model architecture and fine-tuned hyperparameter selection in achieving optimal classification performance. Further research could explore the interaction between these factors to refine pipeline designs for more robust performance across datasets and applications.

\begin{table}[]
    \centering
    \begin{tabular}{lrrrrr}
    \toprule
     & Precision & Recall & F1-Score & AuC \\
    \midrule
    Inception (S) + AlexNet & 0.6715 & 0.6643 & 0.6607 & 0.6643 \\
    YOLOv8 (S) + InceptionV3 & 0.6720 & 0.6430 & 0.6274 & 0.6430 \\
    YOLOv8 (S) + ResNet152V2 & 0.6621 & 0.6244 & 0.6012 & 0.6244 \\
    ResNet (S) + ResNet152V2 & 0.6412 & 0.6107 & 0.5884 & 0.6107 \\
    ResNet (S) + InceptionV3 & 0.6590 & 0.6349 & 0.6205 & 0.6349 \\
    YOLOv8 (S) + AlexNet & \textbf{0.7101} & \textbf{0.6971} & \textbf{0.6923} & \textbf{0.6971} \\
    Inception (S) + InceptionV3 & 0.6842 & 0.6563 & 0.6428 & 0.6563 \\
    Inception (S) + ResNet152V2 & 0.6492 & 0.6254 & 0.6098 & 0.6254 \\
    ResNet (S) + AlexNet & 0.6743 & 0.6633 & 0.6579 & 0.6633 \\
    \bottomrule
    \end{tabular}
    \caption{Classification metrics of the evaluation of the pipeline by pipeline composition}
    \label{tab:total_eval}
\end{table}

\begin{figure}[ht]
    \centering
    \includegraphics[width=1\linewidth]{graphics/total_eval_metrics_selected_new.png}
    \caption{Bar plot of evaluation metrics by pipeline composition (5 best performing (in terms of AuC) configurations)}
    \label{fig:total_top5_eval_metrics}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=1\linewidth]{graphics/confusion_matrix_total_selected_new.png}
    \caption{Confusion matrices of all viable compositions of the total pipeline}
    \label{fig:total_confusion_matrices}
\end{figure}

In Figures \ref{fig:prediction_example_healthy} and \ref{fig:prediction_example_diseased} you can see the visualization of prediction outputs generated by the best performing pipeline ((SAM + YOLOv8) + AlexNet). Each of the displayed regions was selected with a threshold of 80\% leaf certainty produced by the leaf classification stage, and the predicted probability that this region is a diseased leaf is shown by the scalar values inside the region. As you can see, with simple backgrounds as in Figure \ref{fig:prediction_example_healthy} the segmentation and classification is quite accurate. However, with more complicated and realistic backgrounds, as in Figure \ref{fig:prediction_example_diseased}, the segmentation and classification results become more inaccurate, including regions not associated with a leaf. Consequently, the classification results show a higher variance, which underlines the necessity of the region selection proposed in the beginning of this section.


\begin{figure}
    \centering
    \includegraphics[width=0.75\linewidth]{graphics/total_example_healthy.png}
    \caption{Visualization of the prediction by the (SAM + YOLOv8) + AlexNet pipeline for an image of healthy raspberry leaves. The scalars show the prediction that an area with 80\% leaf certainty is diseased}
    \label{fig:prediction_example_healthy}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.75\linewidth]{graphics/total_example_crowded_tomato_yellow_virus.png}
    \caption{Visualization of the prediction by the (SAM + YOLOv8) + AlexNet pipeline for an image of Yellow Virus on a potato plant. The scalars show the prediction that an area with 80\% leaf certainty is diseased}
    \label{fig:prediction_example_diseased}
\end{figure}

\chapter{Discussion} \label{sec:discussion}
This study uses computer vision in real-world scenarios to explore the efficacy of using computer vision to detect plant diseases in leaves. Although earlier studies proposed methods for detecting diseases in plant leaves, they did not address their use outside the laboratory context. They omitted the need to identify and segment individual leaves in a recording taken in real-world applications, where multiple leaves and plants may be present. 

The research identified that employing the proposed methodologies for detecting plant diseases, in conjunction with a segmentation stage, constitutes a practical approach to accurately discerning areas within an image that contain plant leaves and determining whether these leaves exhibit any visible signs of disease.

The proposed methodology generally presents an image pipeline that can most of the times tell apart the presence and location of diseased leaves in an image. This pipeline comprises a segmentation stage, which first attempts to identify all leaf regions in the presented image, and a subsequent disease detection stage. The best-performing configuration of this pipeline consists of SAM-based YOLOv8 in the leaf segmentation stage and AlexNet as the disease detection model, producing an AuC of 69.71\% and an accuracy of 71.01\%.

Comparing these results with the evaluation of the individual stages shows that the performance of the complete pipeline is not solely a ``sum of its parts``, but depends not only on the individual stages' performance but also on their capability of ``working together``. 
Evaluation of the different models responsible for leaf detection (discussed in \ref{sec:results_segmentation_panoptic}), i.e., selecting the relevant regions from SAM's output, shows that ResNet delivers the best performance in discerning images of leaves vs. non-leaves with a precision and recall of 95.8\% and 95.79\%, closely followed by Inception (92.72\% and 92.68\%) and YOLOv8 only ranking as a mediocre third place (80.28\% and 71.75\%). This trend continues when evaluating the entire leaf detection stage (discussed in Section \ref{sec:results_segmentation}), i.e., the leaf classification models in combination with the region-generating model SAM, where SAM + ResNet emerged as the best performer, sporting a mean Dice coefficient (mDC) of 86.28\%, while SAM + YOLOv8 only achieved an mDC of 43.41\%. However, when evaluating on a real-world dataset (discussed in Section \ref{sec:results_total}), in combination with SAM YOLOv8 shows the best performance with an mDC of 66.97\% and ResNet falls behind with only 49.67\%.

Despite attaining satisfactory performance during training with mean average precision scores (mAP) reaching 0.998 for YOLOv8, instance segmentation models did not show adequate performance on previously unseen data, as detailed in Section \ref{sec:results_segmentation_instance}, achieving an inadequate mDC of 1.67\% when evaluated on synthetic data, and a mere 0.5\% on the real-world dataset. 
Two of the three models that did not reach convergence during training and evaluation further underline this method's inferiority. Thus, they were no longer considered viable methods. This additionally shows that training instance segmentation models solely on synthetic datasets does not enable them to generalize and be used in a leaf segmentation application. 

Using a pre-trained model for the complex task of panoptic segmentation, satisfactory performance was achieved in unseen real-world inputs. The model generated coherent segmentation outputs for image regions even in complex backgrounds. 
As the prevalence of extensive datasets containing single-leaf images is much higher than multi-leaf segmentation data, this approach is much better able to harness the data availability by first segmenting an image and subsequently classifying image regions, as the classification step requires solely single-leaf image data. 
With all leaf classification models in combination with this panoptic model far outweighing the instance segmentation approach (seen in Section \ref{sec:results_segmentation}), the panoptic segmentation approach emerged as the  preferred method of segmenting leaves.
\iffalse
Interestingly, the two best-performing leaf detection models (as highlighted in Section \ref{sec:results_segmentation_panoptic}), ResNet and Inception, used with SAM, did not perform as well in evaluating the total pipeline. Of the five best models, two of them used YOLOv8 as their leaf classification model, including the overall best-performing model. However, YOLOv8 exhibited far lower evaluation performance than the other two (recall and precision of 0.9579 and 0.9580 of ResNet versus YOLOv8's 0.7175 and 0.8028, respectively) in the synthetic dataset. This somewhat contradictory result could have multiple reasons. First, as mentioned before, the quality of the dataset used to evaluate the total pipeline could be a factor in this incoherence, further highlighting the necessity of an extensive leaf segmentation dataset. Another option is that specific models just "work better together", i.e., a specific model in disease detection is particularly suited for detecting diseases in leaf segments produced by a certain model in the leaf segmentation stage. The leaf area selection described in Section \ref{sec:results_total} could also play a role in these conflicting results. However, most likely, through its generalizability, given by its more intricate architecture, YOLOv8 emerged as the leaf classification model in the best-performing pipeline configuration, which also aligns with the results on the (incomplete and thus not as significant) real-world dataset seen in Section \ref{sec:results_segmentation}.
\fi
The classification of segmented leaves into healthy and diseased categories was examined using various classification models. Evaluation of these models indicated that AlexNet demonstrated the greatest ability to generalize patterns learned during training to unseen data, achieving an accuracy of 85.68\%. Interestingly, the model with the least computational and architectural complexity can better adapt to the training data than more intricate architectures. ResNet152V2 closely follows all metrics and even exceeds in precision of 87.77\%; in usage in the total pipeline, it falls behind other disease detection models, such as AlexNet and InceptionV3. This finding highlights that for the domain at hand, the complexity of the model and its performance do not necessarily correlate. Further research could investigate the factors at play and further optimize simple models for the classification task of disease detection in leaves.

\section{Limitations}
\subsection{Region selection in total evaluation} \label{sec:lim_selection}
The evaluation of the pipelines presented gives information on the capabilities to detect leaf diseases in conditions outside the laboratory context.
However, these results of the complete pipeline evaluation in Section \ref{sec:results_total} should be interpreted with a grain of salt, as the process of classification of the entire pipeline includes some dataset-specific selection steps. Instead of considering all predicted leaf regions from the leaf detection stage, leading to subpar results, a selection of areas based on multiple factors is being made to improve the performance of the pipeline. The areas generated by the leaf detection stage were selected based on their size, their sharpness, and leaf confidence (i.e., the confidence generated by the leaf detection step), which is described in more detail in Section \ref{sec:results_total}, as well as different thresholds for classifying a region into diseased or healthy classes. The size and sharpness factors utilize key characteristics of the entries in the PlantDoc dataset \cite{singh_plantdoc_2020}. Although the PlantDoc test dataset \cite{singh_plantdoc_2020} contains images of healthy and diseased plants with multiple leaves in different orientations and occlusions present, mirroring real-world conditions, selection based on the region area improves the selection of relevant regions, as the dataset exhibits a narrow focus on single leaves, as shown exemplarily in Figure \ref{fig:plant_doc_example}, where the designated area is focused primarily in the center and occupies the largest portion of the image.
In real-world applications, where footage containing many plants and crops, e.g., an overview of an agricultural field, may be used, leaf regions are much smaller than in the evaluation dataset used for this work. Also, this kind of footage generally does not emphasize single leaves more than others, unlike the dataset at hand. Given these differences between real-world and evaluation data, the presented results may overestimate the robustness and generalizability of the presented approaches. In particular, the segmentation of smaller leaf regions in wide-angle footage may lead to a decline in segmentation capability in the first stage of the presented pipelines. To combat this, future studies could include the use of models such as Semantic-SAM as the panoptic segmentation network, which are capable of segmenting image regions in different granularities, allowing the segmentation of small leaf regions \cite{li_semantic-sam_2023}.
\iffalse
In an attempt to create correct leaf classifications, segmentation regions (described in Section \ref{sec:results_total}) were selected from the results, which are specific to the dataset at hand and cannot be generalized to any situation, reflecting the conditions described above and shown in Figure \ref{fig:plant_doc_example}. However, even with this specific condition, the performance of the best pipeline configuration still classifies about 30\% of the images presented with the wrong label. Although these performances do not accurately reflect the pipelines' performance in real-world scenarios, they present a baseline for comparison of the different presented methods and give a basic understanding of the performance characteristics of each configuration.
\fi

\begin{figure}
    \centering
    \includegraphics[width=0.5\linewidth]{graphics/plantdoc_example1.png}
    \caption{Example of the narrow focus on single leaves in PlantDoc dataset \cite{singh_plantdoc_2020}}
    \label{fig:plant_doc_example}
\end{figure}

\subsection{Lack of comprehensive multi-leaf segmentation data}
The significance of the results of this study is also limited due to a lack of publicly available comprehensive multi-leaf segmentation datasets. We did not find publicly available datasets that reliably contain annotation data for each leaf in a given image with multiple leaves present. The PhenoBench dataset discussed \cite{weyler_phenobench_2023} (see Section \ref{sec:related_work}), although not ideally suited for the generalizability claims of this work, as it contains only images of specific plant species in front of a distinctive background, claims to include such data. However, the actual quality of the data is not in line with the claims the authors make. Upon inspection, it is evident that the annotations for single leaf instances in most cases span multiple leaves and contain multiple completely distinct regions, as seen in Figure \ref{fig:phenobench_example}. 

\begin{figure}
    \centering
    \includegraphics[width=0.75\linewidth]{graphics/phenobench_example.png}
    \caption{Example of an image and its corresponding leaf segmentation masks from the PhenoBench dataset \cite{weyler_phenobench_2023}}
    \label{fig:phenobench_example}
\end{figure}

\subsubsection{Instance segmentation training}
To combat this lack of suitable data, a method for creating synthetic image datasets was proposed (see Section \ref{sec:data_aug}), which presents a scalable approach to obtain data in the necessary fidelity. 
Despite its advantages, the approach of placing leaf images on a background has its shortcomings. The differences in texture, lighting conditions, and background noise between the synthetic dataset and real-world images probably contributed to the limited ability of the instance segmentation models to generalize. As a result, instance segmentation models trained on this dataset exhibited poor performance when applied to real-world scenarios, evident in Sections \ref{sec:results_segmentation}, \ref{sec:results_segmentation_instance}, and \ref{sec:leaf_segmentation_instance_training}.

\subsubsection{Segmentation stage evaluation}
The incompleteness of the real-world dataset and the deviation in results between the synthetic dataset and the total pipeline evaluation pose another limitation to the interpretability of this work's results. 
As described previously, the real-world dataset used for evaluating the segmentation stage only provides incomplete data, as all of the images show a photograph of vegetation with multiple leaves present. Still, the segmentation annotations only offer masks to a single leaf in the picture. This prevalence of false negatives in the ground-truth data poses a substantial limitation on the interpretability of these results. However, the evaluation on this dataset is more in line with the results from the evaluation of the entire pipelines than the synthetic dataset, which further demonstrates the significant inadequacy of this type of data in assessing leaf segmentation performance. 

These limitations demonstrate the need to develop a complete and comprehensive dataset of multi-leaf segmentation data. This dataset should consist of various leaves from vegetation in which multiple leaves are present in a single image captured in a natural environment. This means overlapping and occluded leaves are also present in the image, capturing natural conditions for lighting and occlusion. For these images, an annotation needs to be created, possibly using tools like SAM, to propose panoptic regions, which subsequently need to be filtered manually for leaf regions. The fabrication of such a dataset could mitigate the limitations discussed. Instead of using unrepresentative data for evaluating the segmentation stage and gaining only limited insight into the models' performances, a complete dataset would accurately tell the segmentation performance of a given model and add significance to the results. In addition, training of instance segmentation models would greatly benefit from this data, as the synthetic dataset does not provide the necessary data quality to reach a reliable generalization. With this increased data quality, the instance segmentation models could achieve significantly improved performance on real-world data. 

\iffalse
\textbf{Leaf segmentation dataset scarcity:} The training process of instance segmentation models is highly dependent on the availability of comprehensive and complete datasets. However, the scarcity of such datasets in the domain of leaf images poses a significant challenge in training accurate instance segmentation models. Generally, leaf image datasets that include annotations are quite scarce, and those that exist lack quality. For example, the PhenoBench dataset \cite{weyler_phenobench_2023}, which in and of itself is not optimal for the task at hand, as it contains images of crops in front of a distinctive background, additionally does not contain data in the form claimed by the authors. Instead of presenting mask information for leaf instances in an image, the masking is more semantic, not differentiating between multiple leaves instances. Figure \ref{fig:phenobench_example} clearly shows the described shortcomings of the PhenoBench dataset, showing multiple leaves being grouped into a single leaf region and fully disconnected leaf regions being combined into a single (yellow and green annotation).

\textbf{Synthetic dataset shortcomings:} To combat this lack of suitable data, a synthetic image dataset was created, which presents a scalable approach to obtain data with the necessary fidelity. 
Despite its advantages, the approach of placing leaf images on a background has its shortcomings. The differences in texture, lighting conditions, and background noise between the synthetic dataset and real-world images probably contributed to the limited ability of the instance segmentation models to generalize. As a result, instance segmentation models trained on this dataset exhibited poor performance when applied to real-world scenarios.
To overcome these limitations, developing a real-world dataset with complete and comprehensive annotations for leaf regions is of high importance. Resources like this would enable instance segmentation models to generalize the nuances of real-world data and improve segmentation performance. Although the synthetic dataset used in this study served as an insightful starting point, the limitations of the resulting models underscore the importance of developing more comprehensive datasets.
In addition to improving training performance, such datasets would also enhance the ability to evaluate the approaches presented in this work. The dataset used to assess the segmentation stages \cite{giovi_leaf_2024}, similar to the previously described PlantDoc \cite{singh_plantdoc_2020}, presented a narrow focus on specific leaves in the image and provided a segmentation mask only for them. In contrast, the leaf regions in the background were not annotated. This led to a high rate of false positives in the segmentation stage and limited the significance of the results produced by this evaluation. 
\fi

\subsection{Plant specificity} 
Although the study explores how the prevalence of diseases can be attributed to regions in a recorded image, for use in the real world, it would be very helpful or even necessary to allow the system to attribute plant diseases to specific plant instances in the recorded image. This could be examined in future studies on the subject, developing a system that detects \textbf{plant} instances and from the leaves attributed to this plant can derive its health status using a similar methodology described in this work.

\subsection{Panchromatic detection} 
This study discusses diseases that show features in the panchromatic (human-visible) spectrum; diseases without showing any strain in the human-visible spectrum but possibly exhibiting features visible in hyperspectral wavelengths, such as infrared, are not detectable by the proposed methods. 
The capture of hyperspectral wavelengths could also help predict plant diseases in an earlier stage than possible with panchromatic capture, as \cite{lowe_hyperspectral_2017} and \cite{wan_hyperspectral_2022} suggest. Further research could investigate the possibility of integrating hyperspectral disease detection with the proposed segmentation method and subsequent classification. 

\chapter{Conclusion}
Given the critical role of plant health in ensuring food safety and improving crop efficiency, monitoring the health status of plants in agricultural settings has gained greater importance. Although previous research has made significant advances in identifying leaf diseases, there remains a lack of effective models in real-world applications.

This research introduced a novel methodology for disease detection in practical settings. The two-stage Deep Learning pipeline segments images to isolate leaves and then classifies these segments to assess disease presence. Unlike previous approaches, this method processes multiple leaves within a single image, making it more applicable to actual agricultural environments.

The approach achieved a maximum precision of 71.01x\% and accuracy of 69.71\%, providing a promising baseline for multi-leaf disease detection. While these metrics demonstrate the method's potential, they also underscore the need for further research and development.

Critically, the research highlights the urgent need for a comprehensive, publicly available dataset of leaf segmentation data across various plant species and backgrounds. Such a dataset would be instrumental for researchers and agricultural technology companies, enabling more robust training and evaluation of multi-leaf disease detection methods.

Looking forward, this approach opens promising avenues for scalable, cost-effective plant health monitoring. By leveraging AI to process images of entire plant sections, we move closer to developing precision agriculture technologies that could significantly enhance crop management and global food security.

\appendix
\begin{appendix}
\chapter{Training results}

\section{Leaf segmentation}
The leaf segmentation stage training included a panoptic segmentation approach, which utilizes the SAM by Meta to generate distinct regions in the image and then subsequently classifies each generated region into a leaf or a non-leaf region. The second approach was to segment the leaf regions of a given image directly through instance segmentation, which was trained on a synthetic dataset (described in Section \ref{sec:data_aug}), which inherently encapsulates the classification of a region.

\subsection{Panoptic segmentation} \label{sec:leaf_segmentation_panoptic_training}
The training performance of the autoencoder on the Urban Street dataset \cite{yang_urban_2023} was monitored over 80 epochs, capturing the evolution of various metrics, including training loss and validation accuracy, precision, recall, and F1-score. As shown in Figure \ref{fig:autoencoder_train}, the training loss decreased rapidly in the initial epochs and approached convergence well before reaching the 69th epoch, prompting early stopping to prevent overfitting. This rapid decrease in loss suggests effective optimization and quick adaptation of the model to the training data.

In contrast, the validation metrics — accuracy, precision, recall, and F1-score — remained relatively stable and showed limited improvement throughout the training process. Precision and recall fluctuated around 0.5, indicating that the model struggled to identify relevant patterns in the validation data consistently. The F1-score, a metric that combines precision and recall to provide a balanced view of the model’s performance, remained lower than the other metrics, further indicating the model’s difficulty in achieving reliable performance on the validation set. These stagnating validation metrics imply that while the model successfully minimized training loss, it may be overfitting to the training data or cannot generalize effectively to unseen samples in the Urban Street dataset.

\begin{figure}
    \centering
    \includegraphics[width=0.5\linewidth]{graphics/autoencoder_training_metrics.png}
    \caption{Training metrics of the autoencoder discussed in Section \ref{sec:method_autoencoder}}
    \label{fig:autoencoder_train}
\end{figure}

The training loop of ResNet shows interesting trends. It is evident that the loss, and validation loss, decrease significantly and converge towards the end of training. The validation metrics follow the same values, perfectly overlaying each other in the plot. Also, interestingly, they all start at 0.98 at the beginning of training, showing that either the pre-trained ResNet can extract the relevant features or the sheer length of the training epoch of over 74,000 already leads to a noticeable generalization and learning during the first epoch.

\begin{figure}
    \centering
    \begin{subfigure}{0.48\textwidth}
        \includegraphics[width=\textwidth]{graphics/resnet_training_loss.png}
        \caption{Training and validation loss of ResNet over epochs}
        \label{fig:resnet_training_loss}
    \end{subfigure}
    \begin{subfigure}{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{graphics/resnet_training_val_metrics.png}
        \caption{Validation metrics of ResNet over epochs in the range between 0.98 and 1}
        \label{fig:resnet_training_val_metrics}
        
    \end{subfigure}
    \caption{Training progress of ResNet with average values over one epoch}
    \label{fig:resnet_training}
\end{figure}

InceptionV3, in contrast to ResNet, exhibits a more troublesome training progress. In the first epochs, the validation loss increases significantly to over 100 (clipped in Figure \ref{fig:inception_training}), slowly decreasing to acceptable levels. This might be due to the learning rate scheduling, which reaches its highest learning rate in this stage of the training loop. This is also reflected in the validation metrics, which show a significant dip in this stage of training but quickly recover until they exhibit another dip in epoch 8 before reaching a new best score in all metrics.

\begin{figure}
    \centering
    \begin{subfigure}{0.48\textwidth}
        \includegraphics[width=\textwidth]{graphics/inception_training_loss.png}
        \caption{Training and validation loss of Inception over epochs}
        \label{fig:inception_training_loss}
    \end{subfigure}
    \begin{subfigure}{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{graphics/inception_training_val_metrics.png}
        \caption{Validation metrics of Inception over epochs \newline}
        \label{fig:inception_training_val_metrics}
        
    \end{subfigure}
    \caption{Training progress of Inception with average values over one epoch}
    \label{fig:inception_training}
\end{figure}

\subsection{Instance segmentation} \label{sec:leaf_segmentation_instance_training}

In instance segmentation, the presented models were trained to segment regions containing a leaf from a given input image. The Mask R-CNN, RetinaNet, and YOLOv8 architectures have been selected for this task.
During the training loop described in Section \ref{sec:method_segmentation}, Mask R-CNN and RetinaNet exhibited notable differences in the validation metrics.
In Figure \ref{fig:seg_precision_recall}, you can see the comparison of the two main validation metrics, mean average recall (mAR) and mean average precision (mAP), throughout the progression of training. As you can see, RetinaNet generally achieved a higher mAR with a maximum value of $0.5901$, while Mask R-CNN achieved a maximum of $0.5360$ in this metric. However, the progression of mAR is much smoother in Mask R-CNN than with RetinaNet, with a smoothness of $sm(\text{Mask R-CNN}) = 0.0075$ and $sm(\text{RetinaNet}) = 0.0297$. Where the smoothness $sm$ is defined by the standard deviation ($\sigma$) of the derivative of a function:

\begin{equation} \label{eqn:smoothness}
    sm(f) = \sigma(\frac{df(x)}{dx})
\end{equation}

This behavior is more pronounced when inspecting the mean average precision (mAP) in both networks during training (seen in Figure \ref{fig:seg_precision_recall}). Here, Mask R-CNN achieves an overall higher score of 0.673 and faster convergence compared to RetinaNet, which arguably does not accomplish any convergence. Once again, the smoothness of Mask R-CNN is much higher than RetinaNet's with $sm(\text{Mask R-CNN}) = 0.015$ and $sm(\text{RetinaNet}) = 0.140$ respectively.

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{graphics/segmentation_map_mar.png}
    \caption{Mean average recall (mAR) and mean average precision (mAP) of the two discussed segmentation models}
    \label{fig:seg_precision_recall}
\end{figure}

Due to its inadequate performance, the absence of convergence during training, and the lack of a mask output in RetinaNet, this model is unsuitable for leaf segmentation and will, therefore, not be included in the subsequent stages of this research.

When using YOLOv8 for leaf classification in combination with panoptic segmentation using SAM, that is, a more straightforward task of detecting only a single leaf, YOLOv8 trained on the UrbanStreet dataset \cite{yang_urban_2023}, reached substantially higher training and validation metrics, with a maximum precision and recall of 0.998 and 0.997 respectively. The progress of all metrics throughout the training can be seen in Figure \ref{fig:yolo_training_metrics_us}. Although the convergence of the validation metrics is not as apparent as in the instance segmentation version, the training loss functions converge after the 100 epochs.

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{graphics/yolo_training_result_urban_street.png}
    \caption{Training and validation metrics of the YOLOv8 model on the Urban Street leaves dataset \cite{yang_urban_2023} used for leaf classification}
    \label{fig:yolo_training_metrics_us}
\end{figure}

The training and validation metrics of YOLOv8 as an instance segmentation model on the synthetically generated dataset can be seen in Figure \ref{fig:yolo_training_metrics_synthetic}. The model's performance has converged concerning training loss and validation metrics. The maximum values attained by the instance segmentation model are 0.626 for precision and 0.652 for recall.

\begin{figure}
    \centering
    \includegraphics[width=0.95\linewidth]{graphics/yolo_training_result_synthetic.png}
    \caption{Training and validation metrics of the YOLOv8 model in the synthetically generated dataset from Section \ref{sec:data_aug} used for instance segmentation}
    \label{fig:yolo_training_metrics_synthetic}
\end{figure}

When comparing the validation metrics of the two (remaining) instance segmentation models (seen in Figure \ref{fig:yolo_mrcnn_instance_seg}), Mask R-CNN's scores for bounding box predictions are increasing steadily to values of 0.790 and 0.859 for mAR and mAP respectively. Although the box predictions reach usable scores, it is evident that Mask R-CNN did not manage to generate any meaningful results for the segmentation masks, with its mAP and mAR strongly decreasing at the beginning of training and after that fluctuating heavily and reaching final values of 0.303 and 0.194 respectively. All the while, YOLOv8's mAR and mAP for mask predictions steadily rise throughout the training process and eventually converge to values of around 0.641 and 0.677. Throughout the training, the maximum mAP box score reached by Mask R-CNN was 0.540, whereas YOLOv8 achieved a score of 0.679. As Mask R-CNN proves unviable for this task, it will be omitted in the further progress of this study.


\begin{figure}[]
    \centering
    \begin{subfigure}{0.48\textwidth}
        \includegraphics[width=\textwidth]{graphics/yolo_mrcnn_instance_seg_mar.png}
        \caption{Comparison of mAR between YOLOv8 and Mask R-CNN}
        \label{fig:yolo_mrcnn_instance_seg_mar}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.48\textwidth}
        \includegraphics[width=\textwidth]{graphics/yolo_mrcnn_instance_seg_map.png}
        \caption{Comparison of mAP between YOLOv8 and Mask R-CNN}
        \label{fig:yolo_mrcnn_instance_seg_map}
    \end{subfigure}
    \caption{Comparison of mean average recall (mAR) and mean average precision (mAP) between YOLOv8 and Mask R-CNN}
    \label{fig:yolo_mrcnn_instance_seg}
\end{figure}

\begin{table}[]
    \centering
    \begin{tabular}{lrr}
        \toprule
        & mAP & mAR \\
        \midrule
        RetinaNet & 0.5901 &  0.7120 \\
        Mask R-CNN & 0.5360 &  0.6730 \\
        YOLOv8seg & 0.5403  & 0.6790 \\
        \bottomrule
    \end{tabular}
    \caption{Maximum values for mAP and mAR during training per model}
    \label{tab:instance_segmentation_max_values}
\end{table}


\section{Disease detection} \label{sec:disease_detection_result_training}
For detecting diseases in already segmented leaf regions of the image, seven different models were trained to differentiate between healthy and diseased leaves. Figure \ref{fig:metrics_disease_detection} shows the metrics collected during the training process.

Table \ref{tab:disease_detection_metrics} compares the performance of various models in classifying leaf diseases, using key metrics such as Area under the Curve (AuC), and Recall across both training and validation datasets. AlexNet achieves the highest training metrics, reaching an AuC of 0.9968 and a Recall of 0.9846, underscoring its robust classification capabilities on the training set. While its validation scores are slightly lower, they remain impressive, indicating strong generalization with an AuC of 0.9963, BA, and Recall of 0.9825. InceptionV3 exhibits competitive performance, achieving the same AuC as AlexNet and a slightly lower recall of 0.9841, as well as similar validation metrics with an AuC of 0.9960 and a recall of 0.9826, surpassing the's validation recall of AlexNet.

MobileNetV3Large also demonstrates high performance, particularly in balanced training and validation scores, with minimal drop-offs between datasets. This model offers a reliable balance of efficiency and performance. VisionTransformer and ConvNeXtLarge show moderately high scores, though they fall short of the leading models, particularly in validation performance, indicating they might be more prone to slight overfitting or may have less capacity for capturing subtle disease-related features in this dataset. VGG19, while competitive, shows comparatively lower scores on both training and validation, suggesting potential limitations in its feature extraction for this specific task.

Overall, the analysis highlights AlexNet and InceptionV3 as top-performing models for leaf disease classification. InceptionV3 excels in validation performance, showcasing its effectiveness in achieving reliable generalization. ResNet and MobileNetV3Large also emerge as viable leaf detection models, exhibiting lower but still acceptable performance in their task. 

Figure \ref{fig:metrics_disease_detection_masked} presents the training and validation performance of various models over 30 epochs regarding binary accuracy and loss, providing insights into their convergence behaviors and generalization capabilities for leaf disease classification.

The evaluation of deep learning models, including InceptionV3, MobileNetV3Large, ConvNeXtLarge, ResNet152V2, AlexNet, VGG19, and VisionTransformer, highlights distinct trends across the metrics of AUC, loss, recall, validation AUC, validation loss, and validation recall. In training, you can observe that the models InceptionV3, AlexNet, and ResNet152V2 exhibited the best performance with similar trends in all metrics. They demonstrate loss converging to levels below 0.15 throughout the training process. Their AuC-score and recall also show similar trends, reaching scores of 0.9946, 0.9937, 0.9915 and, 0.9765, 0.9735, 0.9667, respectively. This showcases the models' strong learning performance and generalizing the features of the trained healthy and diseased classes. While MobileNetV3Large shows acceptable performance on par with the previously discussed models, it significantly lags behind in the validation metrics, exhibiting unstable performance with highly fluctuating validation loss, AuC, and recall, showing that this model cannot generalize. While the VisionTransformer exhibits mediocre performance during training, it shows that the loss during the validation cycle sees only minimal improvement throughout 12 epochs, indicating the model is not suited for this task. The other models, namely ConvNeXtLarge and VGG19,, demonstrate that they are also not suited for the classification task at hand, barely showing any improvement of the training and validation metrics during the training process and triggering early stopping quite early during training.

\begin{table}[]
    \centering
    \begin{tabular}{lrrrrrr}
    \toprule
    & min Loss & min Loss (V) \\
    \midrule
    InceptionV3 & 0.0632 & 0.0721 \\
    MobileNetV3Large & 0.1701 & 0.2234 \\
    ConvNeXtLarge & 0.6287 & 0.6281 \\
    ResNet152V2 & 0.0888 & 0.0949 \\
    AlexNet & 0.0715 & 0.0791 \\
    LeNet & 0.6281 & 0.6068 \\
    VGG19 & 0.6280 & 0.6278 \\
    VisionTransformer & 0.4888 & 0.4623 \\
    \bottomrule
    \end{tabular}
    \caption{Training and validation (V) loss of disease detection models}
    \label{tab:disease_detection_losses}
\end{table}

\begin{table}[]
    \centering
    \begin{tabular}{lrrrrrr}
    \toprule
    & max AuC & max Recall & max AuC (V) & max Recall (V) \\
    \midrule
    MobileNetV3Large & 0.9892 & 0.9647 & 0.9874 & 0.9522 \\
    AlexNet & 0.9968 & 0.9852 & 0.9963 & 0.9825 \\
    InceptionV3 & 0.9968 & 0.9841 & 0.9960 & 0.9826 \\
    ResNet152V2 & 0.9957 & 0.9808 & 0.9951 & 0.9789 \\
    VGG19 & 0.6812 & 0.6787 & 0.6789 & 0.6789 \\
    VisionTransformer & 0.8734 & 0.7772 & 0.8816 & 0.7838 \\
    ConvNeXtLarge & 0.7554 & 0.7070 & 0.8144 & 0.7350 \\
    \bottomrule
    \end{tabular}
    \caption{Training and validation (V) metrics of disease detection models}
    \label{tab:disease_detection_metrics}
\end{table}

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{graphics/metrics_classification_masked_new.png}
    \caption{Training and validation metrics of the disease detection classification training loop}
    \label{fig:metrics_disease_detection_masked}
\end{figure}

\end{appendix}


\backmatter

% Use an optional list of figures.
\listoffigures % Starred version, i.e., \listoffigures*, removes the toc entry.

% Use an optional list of tables.
\cleardoublepage % Start list of tables on the next empty right hand page.
\listoftables % Starred version, i.e., \listoftables*, removes the toc entry.

% Use an optional list of alogrithms.
%\listofalgorithms
%\addcontentsline{toc}{chapter}{List of Algorithms}

% Add an index.
\printindex

% Add a glossary.
\printglossaries

% Add a bibliography.
%\bibliographystyle{alpha}
%\bibliography{refs}
\printbibliography

\end{document}
